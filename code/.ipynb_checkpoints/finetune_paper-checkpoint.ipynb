{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ca4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "SEED = 1000\n",
    "setup_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8c02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def rest(t=4000):\n",
    "    import time\n",
    "    [time.sleep(1) for i in range(t)]\n",
    "        \n",
    "        \n",
    "def generate_csv(preds, name):\n",
    "    column_names = ['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']\n",
    "    preds_df = pd.DataFrame(preds, columns=column_names)\n",
    "    preds_df.insert(0, 'ID', [i+1 for i in range(len(preds_df))])\n",
    "    preds_df.to_csv(name, index=False)\n",
    "    \n",
    "    \n",
    "def get_ckpt(path):\n",
    "    return torch.load(path, weights_only=False)\n",
    "\n",
    "\n",
    "def cuda_to_np(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, train_dl, epochs):\n",
    "    total_training_steps = len(train_dl) * epochs\n",
    "    warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "\n",
    "\n",
    "def get_stats(tensor, p=True, r=False, minmax=False):\n",
    "    if minmax:\n",
    "        min, max = tensor.min(), tensor.max()\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Min: {min}, Max: {max} ,Mean: {mean}, Std: {std}\")\n",
    "        if r: return min, max, mean, std\n",
    "    else:\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Mean: {mean}, Std: {std}\")\n",
    "        if r: return mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-8)\n",
    "\n",
    "\n",
    "def reverse_zscore(tensor, mu, sigma):\n",
    "    return (tensor * sigma) + mu\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "    \n",
    "\n",
    "def get_index(iterable):\n",
    "    return random.randint(0, len(iterable) - 1)\n",
    "\n",
    "\n",
    "def get_indices(iterable, n):\n",
    "    return random.sample(range(len(iterable)), n)\n",
    "\n",
    "\n",
    "def split(inputs, targets, seed):\n",
    "    return train_test_split(\n",
    "        inputs,\n",
    "        targets, \n",
    "        test_size=0.2,\n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    ) \n",
    "\n",
    "\n",
    "def show_waves(waves, dpi=100):\n",
    "    \"\"\"\n",
    "    waves: numpy array of shape (3, N)\n",
    "    Creates three separate figures that stretch wide.\n",
    "    \"\"\"\n",
    "    N = waves.shape[1]\n",
    "    t = np.arange(N)\n",
    "\n",
    "    # Wide aspect ratio; height modest so each window fills width\n",
    "    for i in range(waves.shape[0]):\n",
    "        fig = plt.figure(figsize=(14, 4), dpi=dpi)  # wide figure\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(t, waves[i], linewidth=1)\n",
    "        ax.set_title(f\"Wave {i+1}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()  # reduce margins to use width\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def hf_ds_download(hf_token, repo_id):\n",
    "    login(hf_token[1:])\n",
    "    return snapshot_download(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "def get_spectra_features(X, b=False):\n",
    "    \"\"\"Create multi-channel features from spectra: raw, 1st derivative, 2nd derivative.\"\"\"\n",
    "    X_processed = np.zeros_like(X)\n",
    "    # Baseline correction and SNV\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        poly = np.polyfit(np.arange(X.shape[1]), X[i], 3)\n",
    "        baseline = np.polyval(poly, np.arange(X.shape[1]))\n",
    "        corrected_spec = X[i] - baseline\n",
    "        #X_processed[i] = (corrected_spec - corrected_spec.mean()) / (corrected_spec.std() + 1e-8)\n",
    "        X_processed[i] = corrected_spec\n",
    "        \n",
    "    # Calculate derivatives\n",
    "    deriv1 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=1, axis=1)\n",
    "    deriv2 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=2, axis=1)\n",
    "\n",
    "    if b: return np.stack([X_processed, deriv1, deriv2], axis=1)\n",
    "    return np.stack([deriv1, deriv2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfbbe45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'sample_submission.csv'),\n",
       " (1, 'timegate.csv'),\n",
       " (2, 'mettler_toledo.csv'),\n",
       " (3, 'kaiser.csv'),\n",
       " (4, 'anton_532.csv'),\n",
       " (5, 'transfer_plate.csv'),\n",
       " (6, '96_samples.csv'),\n",
       " (7, 'tornado.csv'),\n",
       " (8, 'tec5.csv'),\n",
       " (9, 'metrohm.csv'),\n",
       " (10, 'anton_785.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62074de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    test = pd.read_csv(os.path.join(path, files[6]))\n",
    "\n",
    "    row1 = test.columns[1:].to_numpy().copy()\n",
    "    row1[-1] = \"5611\"\n",
    "    row1 = row1.astype(np.float64)\n",
    "\n",
    "\n",
    "    cols = test.columns[1:]\n",
    "    test = test[cols]\n",
    "    test[\" 5611]\"] = test[\" 5611]\"].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "    test = test.to_numpy()\n",
    "\n",
    "    test = np.insert(test, 0, row1, axis=0)\n",
    "    return test.reshape(-1, 2, 2048).mean(axis=1)\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "    inputs = load_test_data()\n",
    "    \n",
    "    spectra_selection = np.logical_and(\n",
    "        300 <= np.array([float(one) for one in range(2048)]),\n",
    "        np.array([float(one) for one in range(2048)]) <= 1942,\n",
    "    )\n",
    "    \n",
    "    inputs = inputs[:, spectra_selection]\n",
    "\n",
    "    wns = np.array([\n",
    "        float(one) for one in range(2048)\n",
    "    ])[spectra_selection]\n",
    "    wavenumbers = np.arange(300, 1943)\n",
    "\n",
    "    interpolated_data = np.array(\n",
    "        [np.interp(wavenumbers, xp=wns, fp=i) for i in inputs]\n",
    "    )\n",
    "\n",
    "    normed_spectra = interpolated_data / np.max(interpolated_data)\n",
    "    return normed_spectra\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1f5e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 1643), (96, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_transfer_data():\n",
    "    csv_path = os.path.join(path, files[5])\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    input_cols = df.columns[1:2049]\n",
    "    target_cols = df.columns[2050:]\n",
    "\n",
    "    targets  = df[target_cols].dropna().to_numpy()\n",
    "\n",
    "    df = df[input_cols]\n",
    "    df['Unnamed: 1'] = df['Unnamed: 1'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "    df['Unnamed: 2048'] = df['Unnamed: 2048'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "\n",
    "    inputs = df.to_numpy().reshape(-1, 2, 2048)\n",
    "    inputs = inputs.mean(axis=1)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def preprocess_transfer_data():\n",
    "    inputs, targets = load_transfer_data()\n",
    "    \n",
    "    spectra_selection = np.logical_and(\n",
    "        300 <= np.array([float(one) for one in range(2048)]),\n",
    "        np.array([float(one) for one in range(2048)]) <= 1942,\n",
    "    )\n",
    "    \n",
    "    inputs = inputs[:, spectra_selection]\n",
    "    \n",
    "    wns = np.array([\n",
    "        float(one) for one in range(2048)\n",
    "    ])[spectra_selection]\n",
    "    wavenumbers = np.arange(300, 1943)\n",
    "    \n",
    "    interpolated_data = np.array(\n",
    "        [np.interp(wavenumbers, xp=wns, fp=i) for i in inputs]\n",
    "    )\n",
    "    \n",
    "    normed_spectra = interpolated_data / np.max(interpolated_data)\n",
    "    return normed_spectra, targets\n",
    "\n",
    "inputs, targets = preprocess_transfer_data()\n",
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba2c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "np_dtype_from_torch = {\n",
    "    torch.float32: np.float32,\n",
    "    torch.float64: np.float64,\n",
    "}\n",
    "\n",
    "class SpectralDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra,\n",
    "        concentrations,\n",
    "        dtype=None,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=0.0,\n",
    "        baseline_factor_bound=0.0,\n",
    "        baseline_period_lower_bound=100.0,\n",
    "        baseline_period_upper_bound=200.0,\n",
    "        augment_slope_std=0.0,\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=0,\n",
    "        spectrum_rolling_sigma=0.0,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    ):\n",
    "        self.dtype = dtype or torch.float32\n",
    "        self.combine_spectra_range = combine_spectra_range\n",
    "        self.baseline_factor_bound = baseline_factor_bound\n",
    "        self.augment_slope_std = augment_slope_std\n",
    "        self.augment_intercept_std = augment_intersept_std\n",
    "        self.baseline_period_lower_bound = baseline_period_lower_bound\n",
    "        self.baseline_period_upper_bound = baseline_period_upper_bound\n",
    "        self.rolling_bound = rolling_bound\n",
    "        self.spectrum_rolling_sigma = spectrum_rolling_sigma\n",
    "        self.augmentation_weight = torch.tensor(augmentation_weight, dtype=dtype)\n",
    "        self.original_dp_weight = original_datapoint_weight\n",
    "\n",
    "        # normalize spectra\n",
    "        spectra = torch.tensor(spectra, dtype=dtype)\n",
    "\n",
    "        if spectra_mean_std is None:\n",
    "            self.s_mean = torch.mean(spectra)\n",
    "            self.s_std = torch.std(spectra)\n",
    "        else:\n",
    "            self.s_mean, self.s_std = spectra_mean_std\n",
    "\n",
    "        self.spectra = torch.divide(\n",
    "            torch.subtract(spectra, self.s_mean),\n",
    "            self.s_std,\n",
    "        )\n",
    "\n",
    "        self.dummy_wns = np.tile(\n",
    "            np.arange(\n",
    "                0., 1., 1. / self.spectra.shape[2],\n",
    "                dtype=np_dtype_from_torch[self.dtype]\n",
    "            )[None, :self.spectra.shape[2]],\n",
    "            (self.spectra.shape[1], 1),\n",
    "        )\n",
    "\n",
    "        # normalize concentrations\n",
    "        concentrations = torch.tensor(concentrations, dtype=dtype)\n",
    "        if concentration_mean_std is None:\n",
    "            self.concentration_means = torch.nanmean(concentrations, dim=0)\n",
    "\n",
    "            self.concentration_stds = torch.maximum(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        torch.std(col[torch.logical_not(torch.isnan(col))])\n",
    "                        for col in concentrations.T\n",
    "                    ]\n",
    "                ),\n",
    "                torch.tensor([1e-3] * concentrations.shape[1]),\n",
    "            )\n",
    "        else:\n",
    "            self.concentration_means = concentration_mean_std[0]\n",
    "            self.concentration_stds = concentration_mean_std[1]\n",
    "\n",
    "        self.concentrations = torch.divide(\n",
    "            torch.subtract(\n",
    "                concentrations,\n",
    "                self.concentration_means,\n",
    "            ),\n",
    "            self.concentration_stds,\n",
    "        )\n",
    "\n",
    "    def pick_two(self, max_idx=None):\n",
    "        max_idx = max_idx or len(self)\n",
    "        return random.choices(range(max_idx), k=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.concentrations)\n",
    "\n",
    "    def augment_spectra(self, spectra):\n",
    "        if self.augment_slope_std > 0.0:\n",
    "\n",
    "            def spectrum_approximation(x, slope, intercept):\n",
    "                return (slope * x + intercept).reshape(-1, 1)[:, 0]\n",
    "\n",
    "            slope, inter = scipy.optimize.curve_fit(\n",
    "                spectrum_approximation,\n",
    "                self.dummy_wns,\n",
    "                spectra.reshape(-1, 1)[:, 0],\n",
    "                p0=np.random.rand(2),\n",
    "            )[0]\n",
    "\n",
    "            new_slope = slope * (\n",
    "                    np.random.gamma(\n",
    "                        shape=1. / self.augment_slope_std,\n",
    "                        scale=self.augment_slope_std,\n",
    "                        size=1,\n",
    "                    )\n",
    "            )[0]\n",
    "            new_intercept = inter * (\n",
    "                1.0 + np.random.randn(1) * self.augment_intercept_std\n",
    "            )[0]\n",
    "            spectra += torch.tensor(\n",
    "                (new_slope - slope)\n",
    "            ) * self.dummy_wns + new_intercept - inter\n",
    "\n",
    "        factor = self.baseline_factor_bound * torch.rand(size=(1,))\n",
    "        offset = torch.rand(size=(1,)) * 2.0 * torch.pi\n",
    "        period = self.baseline_period_lower_bound + (\n",
    "            self.baseline_period_upper_bound - self.baseline_period_lower_bound\n",
    "        ) * torch.rand(size=(1,))\n",
    "        permutations = factor * torch.cos(\n",
    "            2.0 * torch.pi / period * self.dummy_wns + offset\n",
    "        )\n",
    "        return self.roll_spectrum(\n",
    "            spectra + permutations * spectra,\n",
    "            delta=random.randint(-self.rolling_bound, self.rolling_bound),\n",
    "        )\n",
    "\n",
    "    def roll_spectrum(self, spectra, delta):\n",
    "        num_spectra = spectra.shape[0]\n",
    "        rolled_spectra = np.roll(spectra, delta, axis=1)\n",
    "        if delta > 0:\n",
    "            rolled_spectra[:, :delta] = (\n",
    "                np.random.rand(num_spectra, delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta:(delta + 1)]\n",
    "        elif delta < 0:\n",
    "            rolled_spectra[:, delta:] = (\n",
    "                np.random.rand(num_spectra, -delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta - 1:delta]\n",
    "        return rolled_spectra\n",
    "\n",
    "    def combine_k_items(self, indices, weights):\n",
    "        return (\n",
    "            # spectra\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None, None], self.spectra[indices, :, :]),\n",
    "                dim=0,\n",
    "            ),\n",
    "            # concentrations\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None], self.concentrations[indices, :]),\n",
    "                dim=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.combine_spectra_range < 1e-12:\n",
    "            spectrum = self.spectra[idx]\n",
    "            spectrum = self.augment_spectra(spectrum)\n",
    "            return {\n",
    "                \"spectra\": spectrum,\n",
    "                \"concentrations\": self.concentrations[idx],\n",
    "                \"label_weight\": torch.tensor(1.0, dtype=self.dtype),\n",
    "            }\n",
    "        else:\n",
    "            if random.random() < self.original_dp_weight:\n",
    "                one_weight = 1.\n",
    "                label_weight = torch.tensor(1.0, dtype=self.dtype)\n",
    "            else:\n",
    "                one_weight = random.uniform(0.0, self.combine_spectra_range)\n",
    "                label_weight = self.augmentation_weight\n",
    "            weights = torch.tensor([one_weight, (1 - one_weight)])\n",
    "            # just pick two random indices\n",
    "            indices = random.choices(range(len(self)), k=2)\n",
    "\n",
    "            mixed_spectra, mixed_concentrations = self.combine_k_items(\n",
    "                indices=indices,\n",
    "                weights=weights,\n",
    "            )\n",
    "            mixed_spectra = self.augment_spectra(mixed_spectra)\n",
    "            return mixed_spectra, mixed_concentrations, label_weight\n",
    "\n",
    "\n",
    "config = {\n",
    "    'initial_cnn_channels': 32,\n",
    "    'cnn_channel_factor': 1.279574024454846,\n",
    "    'num_cnn_layers': 8,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'activation_function': 'ELU',\n",
    "    'fc_dropout': 0.10361700399831791,\n",
    "    'lr': 0.001,\n",
    "    'gamma': 0.9649606352621118,\n",
    "    'baseline_factor_bound': 0.748262317340447,\n",
    "    'baseline_period_lower_bound': 0.9703081695287203,\n",
    "    'baseline_period_span': 19.79744237606427,\n",
    "    'original_datapoint_weight': 0.4335003268130408,\n",
    "    'augment_slope_std': 0.08171025264382692,\n",
    "    'batch_size': 32,\n",
    "    'fc_dims': 226,\n",
    "    'rolling_bound': 2,\n",
    "    'num_blocks': 2,\n",
    "}\n",
    "\n",
    "def get_dataset(inputs, targets, config, inputs_mean_std=None, targets_mean_std=None):\n",
    "    return SpectralDataset(\n",
    "        spectra=inputs[:, None, :],\n",
    "        concentrations=targets,\n",
    "        dtype=torch.float32,\n",
    "        spectra_mean_std=inputs_mean_std,\n",
    "        concentration_mean_std=targets_mean_std,\n",
    "        combine_spectra_range=1.0,\n",
    "        baseline_factor_bound=config[\"baseline_factor_bound\"],\n",
    "        baseline_period_lower_bound=config[\"baseline_period_lower_bound\"],\n",
    "        baseline_period_upper_bound=(config[\"baseline_period_lower_bound\"] + config[\"baseline_period_span\"]),\n",
    "        augment_slope_std=config[\"augment_slope_std\"],\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=config[\"rolling_bound\"],\n",
    "        spectrum_rolling_sigma=0.01,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c11034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+5232)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds, train_batch_size, eval_batch_size):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e343398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "        if DROPOUT: neptune_run[\"h_parameters\"] = {\"dropout\": DROPOUT}\n",
    "        if DROP_PATH_RATE: neptune_run[\"h_parameters\"] = {\"drop_path_rate\": DROP_PATH_RATE}\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4227b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    \n",
    "    dim1 = r2_score(targets[:, 0], preds[:, 0])\n",
    "    dim2 = r2_score(targets[:, 1], preds[:, 1])\n",
    "    dim3 = r2_score(targets[:, 2], preds[:, 2])\n",
    "    \n",
    "    return dim1, dim2, dim3, r2_score(targets, logits)\n",
    "\n",
    "\n",
    "class MSEIgnoreNans(_Loss):\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        weights: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        mask = torch.isfinite(target)\n",
    "        mse = torch.mean(\n",
    "            torch.mul(\n",
    "                torch.square(input[mask] - target[mask]),\n",
    "                torch.tile(weights[:, None], dims=(1, target.shape[1]))[mask],\n",
    "            )\n",
    "        )\n",
    "        return torch.where(\n",
    "            torch.isfinite(mse),\n",
    "            mse,\n",
    "            torch.tensor(0.).to(target.device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ccc0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Identity(torch.torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "# this is not a resnet yet\n",
    "class ReZeroBlock(torch.torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        activation_function,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dtype,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(ReZeroBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = torch.torch.nn.BatchNorm1d\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = divmod(kernel_size, 2)[0] if stride == 1 else 0\n",
    "\n",
    "        # does not change spatial dimension\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.bn1 = norm_layer(out_channels, dtype=dtype)\n",
    "        # Both self.conv2 and self.downsample layers\n",
    "        # downsample the input when stride != 1\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            groups=out_channels,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        if stride > 1:\n",
    "            down_conv = torch.nn.Conv1d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "                dtype=dtype,\n",
    "                # groups=out_channels,\n",
    "            )\n",
    "        else:\n",
    "            down_conv = Identity()\n",
    "\n",
    "        self.down_sample = torch.nn.Sequential(\n",
    "            down_conv,\n",
    "            norm_layer(out_channels),\n",
    "        )\n",
    "        self.bn2 = norm_layer(out_channels, dtype=dtype)\n",
    "        # does not change the spatial dimension\n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.bn3 = norm_layer(out_channels, dtype=dtype)\n",
    "        self.activation = activation_function(inplace=True)\n",
    "        self.factor = torch.torch.nn.parameter.Parameter(torch.tensor(0.0, dtype=dtype))\n",
    "\n",
    "    def next_spatial_dim(self, last_spatial_dim):\n",
    "        return math.floor(\n",
    "            (last_spatial_dim + 2 * self.padding - self.kernel_size)\n",
    "            / self.stride + 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        # not really the identity, but kind of\n",
    "        identity = self.down_sample(x)\n",
    "\n",
    "        return self.activation(out * self.factor + identity)\n",
    "\n",
    "\n",
    "class ResNetEncoder(torch.torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectrum_size,\n",
    "        cnn_encoder_channel_dims,\n",
    "        activation_function,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dtype,\n",
    "        num_blocks,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "\n",
    "        self.spatial_dims = [spectrum_size]\n",
    "        layers = []\n",
    "        for in_channels, out_channels in zip(\n",
    "            cnn_encoder_channel_dims[:-1],\n",
    "            cnn_encoder_channel_dims[1:],\n",
    "        ):\n",
    "            block = ReZeroBlock(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                activation_function=activation_function,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            layers.append(block)\n",
    "            self.spatial_dims.append(block.next_spatial_dim(self.spatial_dims[-1]))\n",
    "            for _ in range(num_blocks - 1):\n",
    "                block = ReZeroBlock(\n",
    "                    in_channels=out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    activation_function=activation_function,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    dtype=dtype,\n",
    "                )\n",
    "                layers.append(block)\n",
    "                self.spatial_dims.append(block.next_spatial_dim(self.spatial_dims[-1]))\n",
    "\n",
    "        self.resnet_layers = torch.torch.nn.Sequential(*layers)\n",
    "        if verbose:\n",
    "            print(\"CNN Encoder Channel Dims: %s\" % (cnn_encoder_channel_dims))\n",
    "            print(\"CNN Encoder Spatial Dims: %s\" % (self.spatial_dims))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet_layers(x)\n",
    "\n",
    "\n",
    "class ReZeroNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra_channels,\n",
    "        spectra_size,\n",
    "        initial_cnn_channels,\n",
    "        cnn_channel_factor,\n",
    "        num_cnn_layers,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        activation_function,\n",
    "        fc_dims,\n",
    "        fc_dropout=0.0,\n",
    "        dtype=None,\n",
    "        verbose=False,\n",
    "        fc_output_channels=1,\n",
    "        num_blocks=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc_output_channels = fc_output_channels\n",
    "        self.dtype = dtype or torch.float32\n",
    "\n",
    "        activation_function = getattr(torch.nn, activation_function)\n",
    "\n",
    "        # Setup CNN Encoder\n",
    "        cnn_encoder_channel_dims = [spectra_channels] + [\n",
    "            int(initial_cnn_channels * (cnn_channel_factor**idx))\n",
    "            for idx in range(num_cnn_layers)\n",
    "        ]\n",
    "        self.cnn_encoder = ResNetEncoder(\n",
    "            spectrum_size=spectra_size,\n",
    "            cnn_encoder_channel_dims=cnn_encoder_channel_dims,\n",
    "            activation_function=activation_function,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            num_blocks=num_blocks,\n",
    "            dtype=dtype,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.fc_dims = [\n",
    "            int(\n",
    "                self.cnn_encoder.spatial_dims[-1]\n",
    "            ) * int(cnn_encoder_channel_dims[-1])\n",
    "        ] + fc_dims\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Fc Dims: %s\" % self.fc_dims)\n",
    "        fc_layers = []\n",
    "        for idx, (in_dim, out_dim) in enumerate(\n",
    "                zip(self.fc_dims[:-2], self.fc_dims[1:-1])\n",
    "        ):\n",
    "            fc_layers.append(torch.nn.Linear(in_dim, out_dim))\n",
    "            fc_layers.append(torch.nn.ELU())\n",
    "            fc_layers.append(torch.nn.Dropout(fc_dropout / (2 ** idx)))\n",
    "        fc_layers.append(\n",
    "            torch.nn.Linear(\n",
    "                self.fc_dims[-2],\n",
    "                self.fc_dims[-1] * self.fc_output_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.fc_net = torch.nn.Sequential(*fc_layers)\n",
    "        if verbose:\n",
    "            num_params = sum(p.numel() for p in self.parameters())\n",
    "            print(\"Number of Parameters: %s\" % num_params)\n",
    "\n",
    "    def forward(self, spectra):\n",
    "        embeddings = self.cnn_encoder(spectra)\n",
    "        forecast = self.fc_net(embeddings.view(-1, self.fc_dims[0]))\n",
    "        if self.fc_output_channels > 1:\n",
    "            forecast = forecast.reshape(\n",
    "                -1, self.fc_output_channels, self.fc_dims[-1]\n",
    "            )\n",
    "        return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035dd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "mse_loss_function = MSEIgnoreNans()\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    device,\n",
    "    scaler, \n",
    "    scheduler,\n",
    "    train_dl,\n",
    "    eval_dl,\n",
    "    loss_fn,\n",
    "    epochs,\n",
    "    checkpoint_name,\n",
    "    score=-float(\"inf\"),\n",
    "    neptune_run=None,\n",
    "    p=True,\n",
    "):  \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for inputs, targets, weights in train_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets, weights)\n",
    "                            \n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            if neptune_run is not None:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            total_loss += loss.detach().cpu()\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        all_logits = torch.cat(all_logits)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "        total_loss = total_loss / len(train_dl)\n",
    "        \n",
    "        model.eval()\n",
    "        eval_total_loss = 0.0\n",
    "        eval_all_logits = []\n",
    "        eval_all_targets = []\n",
    "\n",
    "        for inputs, targets, weights in eval_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                #with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets, weights)\n",
    "\n",
    "            eval_total_loss += loss.detach().cpu()\n",
    "            eval_all_logits.append(logits.detach().cpu())\n",
    "            eval_all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        eval_all_logits = torch.cat(eval_all_logits)\n",
    "        eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "        eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "        eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "        \n",
    "        if eval_r2 > score:\n",
    "            score = eval_r2\n",
    "            data = {\"state_dict\": model.state_dict()}\n",
    "            data[\"epoch\"] = epoch \n",
    "            data[\"score\"] = score\n",
    "            torch.save(data, f\"/kaggle/working/{checkpoint_name}\")\n",
    "        \n",
    "        if neptune_run is not None:\n",
    "            neptune_run[\"train/loss\"].append(total_loss)\n",
    "            neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "            neptune_run[\"train/r2\"].append(r2)\n",
    "            neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "            neptune_run[\"train/one\"].append(one)\n",
    "            neptune_run[\"train/two\"].append(two)\n",
    "            neptune_run[\"train/three\"].append(three)\n",
    "            neptune_run[\"eval/one\"].append(eval_one)\n",
    "            neptune_run[\"eval/two\"].append(eval_two)\n",
    "            neptune_run[\"eval/three\"].append(eval_three)\n",
    "            \n",
    "        if p and epoch % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, \"\n",
    "                f\"train/loss: {total_loss:.4f}, \"\n",
    "                f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "                f\"train/r2: {r2:.4f}, \"\n",
    "                f\"eval/r2: {eval_r2:.4f}, \"\n",
    "                f\"train/one: {one:.4f}, \"\n",
    "                f\"train/two: {two:.4f}, \"\n",
    "                f\"train/three: {three:.4f}, \"\n",
    "                f\"eval/one: {eval_one:.4f}, \"\n",
    "                f\"eval/two: {eval_two:.4f}, \"\n",
    "                f\"eval/three: {eval_three:.4f} \"\n",
    "            )\n",
    "            \n",
    "    if neptune_run is not None: neptune_run.stop()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd5d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings#; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "\n",
    "DROPOUT = 0.5\n",
    "DROP_PATH_RATE = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESUME = False\n",
    "\n",
    "config[\"dtype\"] = torch.float32\n",
    "config[\"spectra_size\"] = 1643\n",
    "config[\"spectra_channels\"] = 1\n",
    "config[\"fc_dims\"] = [\n",
    "    config[\"fc_dims\"],\n",
    "    int(config[\"fc_dims\"] / 2),\n",
    "    3,\n",
    "]\n",
    "\n",
    "#mse_loss_function = MSEIgnoreNans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54dd678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734309\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa073c7e4b9e4b0ba0d5e9e47e07af8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0059, eval/loss: 0.9008, train/r2: -0.0144, eval/r2: -0.1498, train/one: -0.0045, train/two: -0.0373, train/three: -0.0015, eval/one: -0.0041, eval/two: -0.0095, eval/three: -0.4356 \n",
      "Epoch: 5, train/loss: 0.9305, eval/loss: 1.0507, train/r2: -0.0238, eval/r2: -0.1839, train/one: -0.0764, train/two: -0.0028, train/three: 0.0077, eval/one: -0.3378, eval/two: -0.1219, eval/three: -0.0919 \n",
      "Epoch: 10, train/loss: 0.8183, eval/loss: 1.0078, train/r2: 0.1088, eval/r2: -0.1464, train/one: -0.0194, train/two: 0.0136, train/three: 0.3320, eval/one: 0.0059, eval/two: -0.2638, eval/three: -0.1813 \n",
      "Epoch: 15, train/loss: 0.7442, eval/loss: 0.8919, train/r2: 0.2582, eval/r2: 0.0749, train/one: 0.0224, train/two: 0.0101, train/three: 0.7422, eval/one: -0.0507, eval/two: -0.0417, eval/three: 0.3170 \n",
      "Epoch: 20, train/loss: 0.6956, eval/loss: 0.7398, train/r2: 0.2649, eval/r2: 0.1603, train/one: -0.0308, train/two: -0.0296, train/three: 0.8549, eval/one: -0.0746, eval/two: -0.1699, eval/three: 0.7253 \n",
      "Epoch: 25, train/loss: 0.6326, eval/loss: 0.7430, train/r2: 0.3289, eval/r2: 0.1856, train/one: 0.0688, train/two: 0.0711, train/three: 0.8468, eval/one: -0.1074, eval/two: -0.2142, eval/three: 0.8785 \n",
      "Epoch: 30, train/loss: 0.5528, eval/loss: 0.4884, train/r2: 0.4065, eval/r2: 0.2314, train/one: 0.2844, train/two: 0.0874, train/three: 0.8477, eval/one: 0.1749, eval/two: -0.1677, eval/three: 0.6870 \n",
      "Epoch: 35, train/loss: 0.4176, eval/loss: 0.3839, train/r2: 0.5775, eval/r2: 0.3519, train/one: 0.6849, train/two: 0.1667, train/three: 0.8807, eval/one: 0.3725, eval/two: -0.1567, eval/three: 0.8399 \n",
      "Epoch: 40, train/loss: 0.3647, eval/loss: 0.3535, train/r2: 0.6596, eval/r2: 0.4894, train/one: 0.7645, train/two: 0.3866, train/three: 0.8276, eval/one: 0.7944, eval/two: -0.2209, eval/three: 0.8945 \n",
      "Epoch: 45, train/loss: 0.3391, eval/loss: 0.3762, train/r2: 0.7014, eval/r2: 0.5894, train/one: 0.7115, train/two: 0.4673, train/three: 0.9254, eval/one: 0.6762, eval/two: 0.2452, eval/three: 0.8469 \n",
      "Epoch: 50, train/loss: 0.3015, eval/loss: 0.3461, train/r2: 0.7235, eval/r2: 0.5418, train/one: 0.7587, train/two: 0.5393, train/three: 0.8727, eval/one: 0.5926, eval/two: 0.1259, eval/three: 0.9069 \n",
      "Epoch: 55, train/loss: 0.2612, eval/loss: 0.2551, train/r2: 0.7509, eval/r2: 0.6625, train/one: 0.7782, train/two: 0.5560, train/three: 0.9186, eval/one: 0.7849, eval/two: 0.4741, eval/three: 0.7284 \n",
      "Epoch: 60, train/loss: 0.1973, eval/loss: 0.2755, train/r2: 0.7608, eval/r2: 0.6861, train/one: 0.8322, train/two: 0.5183, train/three: 0.9320, eval/one: 0.8103, eval/two: 0.4496, eval/three: 0.7986 \n",
      "Epoch: 65, train/loss: 0.1776, eval/loss: 0.1396, train/r2: 0.8291, eval/r2: 0.7401, train/one: 0.8646, train/two: 0.7187, train/three: 0.9039, eval/one: 0.8324, eval/two: 0.7022, eval/three: 0.6857 \n",
      "Epoch: 70, train/loss: 0.1375, eval/loss: 0.1991, train/r2: 0.8589, eval/r2: 0.7089, train/one: 0.8750, train/two: 0.7817, train/three: 0.9200, eval/one: 0.6838, eval/two: 0.6063, eval/three: 0.8366 \n",
      "Epoch: 75, train/loss: 0.1278, eval/loss: 0.2261, train/r2: 0.8720, eval/r2: 0.7326, train/one: 0.8867, train/two: 0.7996, train/three: 0.9297, eval/one: 0.6258, eval/two: 0.6699, eval/three: 0.9021 \n",
      "Epoch: 80, train/loss: 0.1383, eval/loss: 0.1801, train/r2: 0.8672, eval/r2: 0.8128, train/one: 0.8940, train/two: 0.8049, train/three: 0.9027, eval/one: 0.7369, eval/two: 0.7849, eval/three: 0.9166 \n",
      "Epoch: 85, train/loss: 0.1176, eval/loss: 0.1156, train/r2: 0.8698, eval/r2: 0.8261, train/one: 0.8934, train/two: 0.7698, train/three: 0.9462, eval/one: 0.9114, eval/two: 0.7365, eval/three: 0.8305 \n",
      "Epoch: 90, train/loss: 0.0962, eval/loss: 0.1425, train/r2: 0.8911, eval/r2: 0.8368, train/one: 0.8801, train/two: 0.8608, train/three: 0.9325, eval/one: 0.8069, eval/two: 0.7837, eval/three: 0.9200 \n",
      "Epoch: 95, train/loss: 0.1310, eval/loss: 0.1235, train/r2: 0.8886, eval/r2: 0.8366, train/one: 0.8962, train/two: 0.8357, train/three: 0.9338, eval/one: 0.8736, eval/two: 0.7384, eval/three: 0.8980 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 192 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 192 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-157/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b67b1cb7870432fba385af2f00f4bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0546, eval/loss: 0.9470, train/r2: -0.0137, eval/r2: -0.1089, train/one: -0.0175, train/two: -0.0081, train/three: -0.0155, eval/one: -0.1153, eval/two: -0.2104, eval/three: -0.0009 \n",
      "Epoch: 5, train/loss: 0.9741, eval/loss: 0.8028, train/r2: 0.0154, eval/r2: -0.0189, train/one: -0.0084, train/two: 0.0058, train/three: 0.0487, eval/one: -0.0508, eval/two: -0.0053, eval/three: -0.0007 \n",
      "Epoch: 10, train/loss: 0.8773, eval/loss: 0.8115, train/r2: 0.1118, eval/r2: 0.0080, train/one: -0.0623, train/two: 0.0110, train/three: 0.3867, eval/one: -0.0114, eval/two: -0.0099, eval/three: 0.0453 \n",
      "Epoch: 15, train/loss: 0.8215, eval/loss: 0.8129, train/r2: 0.2689, eval/r2: 0.0013, train/one: -0.0051, train/two: -0.0069, train/three: 0.8186, eval/one: 0.0036, eval/two: -0.0664, eval/three: 0.0668 \n",
      "Epoch: 20, train/loss: 0.6623, eval/loss: 1.1136, train/r2: 0.3123, eval/r2: 0.0410, train/one: 0.0899, train/two: -0.0064, train/three: 0.8533, eval/one: -0.1256, eval/two: -0.3461, eval/three: 0.5946 \n",
      "Epoch: 25, train/loss: 0.6717, eval/loss: 0.9340, train/r2: 0.3041, eval/r2: -0.1710, train/one: 0.0653, train/two: 0.0279, train/three: 0.8192, eval/one: -0.4626, eval/two: -0.8190, eval/three: 0.7686 \n",
      "Epoch: 30, train/loss: 0.7086, eval/loss: 0.6578, train/r2: 0.3521, eval/r2: 0.2266, train/one: 0.2358, train/two: -0.0520, train/three: 0.8725, eval/one: 0.1534, eval/two: -0.3166, eval/three: 0.8429 \n",
      "Epoch: 35, train/loss: 0.4571, eval/loss: 0.7350, train/r2: 0.5225, eval/r2: 0.2499, train/one: 0.6352, train/two: 0.0637, train/three: 0.8687, eval/one: 0.3683, eval/two: 0.0760, eval/three: 0.3055 \n",
      "Epoch: 40, train/loss: 0.3404, eval/loss: 0.3182, train/r2: 0.6472, eval/r2: 0.6929, train/one: 0.8224, train/two: 0.2297, train/three: 0.8896, eval/one: 0.8753, eval/two: 0.3466, eval/three: 0.8569 \n",
      "Epoch: 45, train/loss: 0.3024, eval/loss: 0.3347, train/r2: 0.7072, eval/r2: 0.6240, train/one: 0.7726, train/two: 0.4881, train/three: 0.8609, eval/one: 0.6219, eval/two: 0.4078, eval/three: 0.8422 \n",
      "Epoch: 50, train/loss: 0.3294, eval/loss: 0.3378, train/r2: 0.6874, eval/r2: 0.7005, train/one: 0.5861, train/two: 0.5833, train/three: 0.8929, eval/one: 0.8241, eval/two: 0.4096, eval/three: 0.8678 \n",
      "Epoch: 55, train/loss: 0.1859, eval/loss: 0.2625, train/r2: 0.8078, eval/r2: 0.6565, train/one: 0.8311, train/two: 0.6574, train/three: 0.9350, eval/one: 0.7381, eval/two: 0.3937, eval/three: 0.8376 \n",
      "Epoch: 60, train/loss: 0.2040, eval/loss: 0.0954, train/r2: 0.8167, eval/r2: 0.8818, train/one: 0.8768, train/two: 0.6972, train/three: 0.8762, eval/one: 0.8975, eval/two: 0.8752, eval/three: 0.8726 \n",
      "Epoch: 65, train/loss: 0.1449, eval/loss: 0.1138, train/r2: 0.8671, eval/r2: 0.8420, train/one: 0.8875, train/two: 0.8138, train/three: 0.9000, eval/one: 0.8725, eval/two: 0.7732, eval/three: 0.8804 \n",
      "Epoch: 70, train/loss: 0.1127, eval/loss: 0.1304, train/r2: 0.8864, eval/r2: 0.8553, train/one: 0.9188, train/two: 0.8352, train/three: 0.9051, eval/one: 0.9203, eval/two: 0.7688, eval/three: 0.8767 \n",
      "Epoch: 75, train/loss: 0.0994, eval/loss: 0.1430, train/r2: 0.9016, eval/r2: 0.8407, train/one: 0.8919, train/two: 0.9043, train/three: 0.9087, eval/one: 0.8629, eval/two: 0.7980, eval/three: 0.8613 \n",
      "Epoch: 80, train/loss: 0.0951, eval/loss: 0.0773, train/r2: 0.9048, eval/r2: 0.9066, train/one: 0.8968, train/two: 0.9020, train/three: 0.9157, eval/one: 0.9305, eval/two: 0.8757, eval/three: 0.9137 \n",
      "Epoch: 85, train/loss: 0.1216, eval/loss: 0.0824, train/r2: 0.8751, eval/r2: 0.8845, train/one: 0.8747, train/two: 0.8213, train/three: 0.9291, eval/one: 0.8742, eval/two: 0.8862, eval/three: 0.8931 \n",
      "Epoch: 90, train/loss: 0.1043, eval/loss: 0.0676, train/r2: 0.8936, eval/r2: 0.9063, train/one: 0.8970, train/two: 0.8709, train/three: 0.9130, eval/one: 0.8954, eval/two: 0.9626, eval/three: 0.8610 \n",
      "Epoch: 95, train/loss: 0.1012, eval/loss: 0.0837, train/r2: 0.9040, eval/r2: 0.8762, train/one: 0.9199, train/two: 0.8629, train/three: 0.9292, eval/one: 0.8672, eval/two: 0.8997, eval/three: 0.8618 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 117 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 117 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-158/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-159\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85b26a45c974f40921e3e5d00358aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 0.9563, eval/loss: 1.3915, train/r2: -0.0118, eval/r2: -0.0214, train/one: -0.0110, train/two: -0.0007, train/three: -0.0237, eval/one: -0.0297, eval/two: -0.0311, eval/three: -0.0035 \n",
      "Epoch: 5, train/loss: 0.9226, eval/loss: 0.9107, train/r2: 0.0082, eval/r2: -0.1688, train/one: -0.0323, train/two: 0.0014, train/three: 0.0554, eval/one: -0.2021, eval/two: -0.0944, eval/three: -0.2100 \n",
      "Epoch: 10, train/loss: 0.8737, eval/loss: 1.1699, train/r2: 0.0931, eval/r2: -0.1010, train/one: -0.0682, train/two: 0.0135, train/three: 0.3339, eval/one: -0.0001, eval/two: -0.3438, eval/three: 0.0408 \n",
      "Epoch: 15, train/loss: 0.7347, eval/loss: 1.0395, train/r2: 0.2741, eval/r2: -0.2452, train/one: 0.0225, train/two: -0.0097, train/three: 0.8094, eval/one: -0.0146, eval/two: -0.4164, eval/three: -0.3047 \n",
      "Epoch: 20, train/loss: 0.7358, eval/loss: 0.7369, train/r2: 0.2842, eval/r2: 0.2313, train/one: 0.0341, train/two: -0.0056, train/three: 0.8241, eval/one: 0.1000, eval/two: -0.2734, eval/three: 0.8671 \n",
      "Epoch: 25, train/loss: 0.5839, eval/loss: 0.6712, train/r2: 0.3354, eval/r2: 0.2562, train/one: 0.1366, train/two: 0.0004, train/three: 0.8690, eval/one: 0.0056, eval/two: -0.1163, eval/three: 0.8793 \n",
      "Epoch: 30, train/loss: 0.5841, eval/loss: 0.4389, train/r2: 0.4042, eval/r2: 0.3718, train/one: 0.3259, train/two: 0.0573, train/three: 0.8295, eval/one: 0.4576, eval/two: -0.1968, eval/three: 0.8548 \n",
      "Epoch: 35, train/loss: 0.4064, eval/loss: 0.4301, train/r2: 0.5664, eval/r2: 0.4290, train/one: 0.7196, train/two: 0.0967, train/three: 0.8831, eval/one: 0.5376, eval/two: 0.0714, eval/three: 0.6780 \n",
      "Epoch: 40, train/loss: 0.3577, eval/loss: 0.3386, train/r2: 0.6646, eval/r2: 0.5649, train/one: 0.7883, train/two: 0.3289, train/three: 0.8768, eval/one: 0.8392, eval/two: -0.0162, eval/three: 0.8715 \n",
      "Epoch: 45, train/loss: 0.2078, eval/loss: 0.3900, train/r2: 0.7753, eval/r2: 0.5040, train/one: 0.8054, train/two: 0.6250, train/three: 0.8955, eval/one: 0.6213, eval/two: 0.2183, eval/three: 0.6723 \n",
      "Epoch: 50, train/loss: 0.1698, eval/loss: 0.2203, train/r2: 0.8386, eval/r2: 0.7554, train/one: 0.8625, train/two: 0.7660, train/three: 0.8872, eval/one: 0.8854, eval/two: 0.5823, eval/three: 0.7984 \n",
      "Epoch: 55, train/loss: 0.1496, eval/loss: 0.2921, train/r2: 0.8498, eval/r2: 0.6774, train/one: 0.8593, train/two: 0.8005, train/three: 0.8895, eval/one: 0.7079, eval/two: 0.5274, eval/three: 0.7968 \n",
      "Epoch: 60, train/loss: 0.1197, eval/loss: 0.2565, train/r2: 0.8735, eval/r2: 0.6564, train/one: 0.8765, train/two: 0.8531, train/three: 0.8909, eval/one: 0.3069, eval/two: 0.8121, eval/three: 0.8503 \n",
      "Epoch: 65, train/loss: 0.0903, eval/loss: 0.1620, train/r2: 0.9073, eval/r2: 0.8241, train/one: 0.9194, train/two: 0.8902, train/three: 0.9123, eval/one: 0.8239, eval/two: 0.7206, eval/three: 0.9276 \n",
      "Epoch: 70, train/loss: 0.0861, eval/loss: 0.1657, train/r2: 0.9165, eval/r2: 0.7089, train/one: 0.9001, train/two: 0.9050, train/three: 0.9444, eval/one: 0.8490, eval/two: 0.3372, eval/three: 0.9406 \n",
      "Epoch: 75, train/loss: 0.1074, eval/loss: 0.0747, train/r2: 0.9002, eval/r2: 0.9185, train/one: 0.8986, train/two: 0.8969, train/three: 0.9053, eval/one: 0.9585, eval/two: 0.8619, eval/three: 0.9351 \n",
      "Epoch: 80, train/loss: 0.0752, eval/loss: 0.1292, train/r2: 0.9252, eval/r2: 0.8553, train/one: 0.9370, train/two: 0.9009, train/three: 0.9377, eval/one: 0.8638, eval/two: 0.7605, eval/three: 0.9416 \n",
      "Epoch: 85, train/loss: 0.0994, eval/loss: 0.0953, train/r2: 0.8962, eval/r2: 0.9067, train/one: 0.8842, train/two: 0.9053, train/three: 0.8990, eval/one: 0.9406, eval/two: 0.8756, eval/three: 0.9040 \n",
      "Epoch: 90, train/loss: 0.0884, eval/loss: 0.1603, train/r2: 0.9227, eval/r2: 0.7316, train/one: 0.9219, train/two: 0.9169, train/three: 0.9292, eval/one: 0.7990, eval/two: 0.4620, eval/three: 0.9338 \n",
      "Epoch: 95, train/loss: 0.1020, eval/loss: 0.1266, train/r2: 0.9070, eval/r2: 0.8516, train/one: 0.9081, train/two: 0.9037, train/three: 0.9092, eval/one: 0.8130, eval/two: 0.7971, eval/three: 0.9448 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 130 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 130 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-159/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b524f9af8e774acb86045bcba07ac807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0296, eval/loss: 1.5326, train/r2: -0.0092, eval/r2: -0.1685, train/one: -0.0012, train/two: 0.0008, train/three: -0.0271, eval/one: -0.2355, eval/two: -0.0019, eval/three: -0.2680 \n",
      "Epoch: 5, train/loss: 0.8748, eval/loss: 1.2380, train/r2: -0.0292, eval/r2: -0.4328, train/one: -0.0231, train/two: -0.0160, train/three: -0.0484, eval/one: -0.0723, eval/two: -0.4459, eval/three: -0.7801 \n",
      "Epoch: 10, train/loss: 0.9654, eval/loss: 1.4736, train/r2: 0.0963, eval/r2: -0.4774, train/one: -0.0325, train/two: 0.0074, train/three: 0.3140, eval/one: -1.3303, eval/two: -0.1079, eval/three: 0.0061 \n",
      "Epoch: 15, train/loss: 0.7310, eval/loss: 1.2799, train/r2: 0.2629, eval/r2: 0.0546, train/one: -0.0076, train/two: -0.0040, train/three: 0.8004, eval/one: -0.0247, eval/two: -0.0491, eval/three: 0.2375 \n",
      "Epoch: 20, train/loss: 0.6670, eval/loss: 0.8750, train/r2: 0.2721, eval/r2: 0.1177, train/one: -0.0060, train/two: -0.0002, train/three: 0.8224, eval/one: -0.1125, eval/two: -0.0714, eval/three: 0.5371 \n",
      "Epoch: 25, train/loss: 0.7143, eval/loss: 0.8206, train/r2: 0.3007, eval/r2: 0.2337, train/one: 0.0583, train/two: 0.0081, train/three: 0.8357, eval/one: 0.0366, eval/two: -0.1408, eval/three: 0.8054 \n",
      "Epoch: 30, train/loss: 0.6311, eval/loss: 1.0418, train/r2: 0.3786, eval/r2: 0.3585, train/one: 0.2306, train/two: 0.0427, train/three: 0.8625, eval/one: 0.0956, eval/two: 0.0490, eval/three: 0.9308 \n",
      "Epoch: 35, train/loss: 0.4501, eval/loss: 0.6282, train/r2: 0.5643, eval/r2: 0.5605, train/one: 0.6587, train/two: 0.1644, train/three: 0.8699, eval/one: 0.6777, eval/two: 0.0976, eval/three: 0.9062 \n",
      "Epoch: 40, train/loss: 0.4229, eval/loss: 0.5868, train/r2: 0.6405, eval/r2: 0.5625, train/one: 0.7805, train/two: 0.2419, train/three: 0.8991, eval/one: 0.8703, eval/two: 0.1992, eval/three: 0.6180 \n",
      "Epoch: 45, train/loss: 0.2742, eval/loss: 0.5818, train/r2: 0.7303, eval/r2: 0.5121, train/one: 0.8431, train/two: 0.4577, train/three: 0.8901, eval/one: 0.6436, eval/two: -0.0241, eval/three: 0.9169 \n",
      "Epoch: 50, train/loss: 0.2812, eval/loss: 0.5430, train/r2: 0.7216, eval/r2: 0.5688, train/one: 0.7911, train/two: 0.4855, train/three: 0.8881, eval/one: 0.7811, eval/two: 0.0655, eval/three: 0.8597 \n",
      "Epoch: 55, train/loss: 0.2483, eval/loss: 0.4076, train/r2: 0.7492, eval/r2: 0.7025, train/one: 0.7868, train/two: 0.5700, train/three: 0.8909, eval/one: 0.8381, eval/two: 0.3589, eval/three: 0.9106 \n",
      "Epoch: 60, train/loss: 0.1891, eval/loss: 0.3640, train/r2: 0.8198, eval/r2: 0.6542, train/one: 0.8255, train/two: 0.6980, train/three: 0.9360, eval/one: 0.7072, eval/two: 0.4537, eval/three: 0.8016 \n",
      "Epoch: 65, train/loss: 0.2194, eval/loss: 0.3611, train/r2: 0.8037, eval/r2: 0.6899, train/one: 0.8309, train/two: 0.6713, train/three: 0.9090, eval/one: 0.8408, eval/two: 0.5027, eval/three: 0.7264 \n",
      "Epoch: 70, train/loss: 0.1421, eval/loss: 0.3023, train/r2: 0.8499, eval/r2: 0.7760, train/one: 0.8821, train/two: 0.7480, train/three: 0.9195, eval/one: 0.8717, eval/two: 0.5891, eval/three: 0.8672 \n",
      "Epoch: 75, train/loss: 0.1448, eval/loss: 0.3291, train/r2: 0.8704, eval/r2: 0.7248, train/one: 0.8793, train/two: 0.8093, train/three: 0.9226, eval/one: 0.7439, eval/two: 0.5559, eval/three: 0.8746 \n",
      "Epoch: 80, train/loss: 0.1295, eval/loss: 0.2629, train/r2: 0.8627, eval/r2: 0.7908, train/one: 0.8794, train/two: 0.7988, train/three: 0.9099, eval/one: 0.8971, eval/two: 0.6601, eval/three: 0.8152 \n",
      "Epoch: 85, train/loss: 0.1157, eval/loss: 0.1774, train/r2: 0.8803, eval/r2: 0.8081, train/one: 0.8932, train/two: 0.8253, train/three: 0.9223, eval/one: 0.8457, eval/two: 0.7680, eval/three: 0.8106 \n",
      "Epoch: 90, train/loss: 0.1461, eval/loss: 0.2046, train/r2: 0.8804, eval/r2: 0.7840, train/one: 0.8891, train/two: 0.8399, train/three: 0.9124, eval/one: 0.8383, eval/two: 0.7409, eval/three: 0.7727 \n",
      "Epoch: 95, train/loss: 0.0988, eval/loss: 0.2742, train/r2: 0.8939, eval/r2: 0.8062, train/one: 0.8772, train/two: 0.8588, train/three: 0.9456, eval/one: 0.9042, eval/two: 0.6794, eval/three: 0.8351 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 140 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 140 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-160/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf11e77ab474a01962362101348bc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0498, eval/loss: 0.9821, train/r2: -0.0018, eval/r2: -0.0429, train/one: 0.0016, train/two: -0.0047, train/three: -0.0024, eval/one: -0.0023, eval/two: -0.0346, eval/three: -0.0919 \n",
      "Epoch: 5, train/loss: 0.9829, eval/loss: 0.9571, train/r2: 0.0186, eval/r2: -0.2712, train/one: 0.0097, train/two: -0.0246, train/three: 0.0708, eval/one: -0.0059, eval/two: -0.3418, eval/three: -0.4660 \n",
      "Epoch: 10, train/loss: 0.8785, eval/loss: 0.9534, train/r2: 0.1205, eval/r2: -0.1611, train/one: -0.0488, train/two: 0.0275, train/three: 0.3827, eval/one: -0.0233, eval/two: -0.0226, eval/three: -0.4374 \n",
      "Epoch: 15, train/loss: 0.7237, eval/loss: 0.6256, train/r2: 0.2987, eval/r2: 0.1010, train/one: 0.0591, train/two: 0.0479, train/three: 0.7890, eval/one: -0.1396, eval/two: -0.1953, eval/three: 0.6379 \n",
      "Epoch: 20, train/loss: 0.7351, eval/loss: 0.5538, train/r2: 0.2855, eval/r2: 0.2008, train/one: 0.0421, train/two: -0.0101, train/three: 0.8243, eval/one: -0.0621, eval/two: -0.1317, eval/three: 0.7961 \n",
      "Epoch: 25, train/loss: 0.6905, eval/loss: 0.5683, train/r2: 0.3634, eval/r2: 0.2170, train/one: 0.1413, train/two: 0.0840, train/three: 0.8649, eval/one: -0.0395, eval/two: -0.1851, eval/three: 0.8757 \n",
      "Epoch: 30, train/loss: 0.4810, eval/loss: 0.3862, train/r2: 0.4752, eval/r2: 0.4651, train/one: 0.4576, train/two: 0.1015, train/three: 0.8666, eval/one: 0.5148, eval/two: 0.0665, eval/three: 0.8139 \n",
      "Epoch: 35, train/loss: 0.3575, eval/loss: 0.2880, train/r2: 0.5929, eval/r2: 0.4660, train/one: 0.7112, train/two: 0.1885, train/three: 0.8789, eval/one: 0.7906, eval/two: -0.1805, eval/three: 0.7880 \n",
      "Epoch: 40, train/loss: 0.3939, eval/loss: 0.2302, train/r2: 0.6101, eval/r2: 0.6992, train/one: 0.7405, train/two: 0.2010, train/three: 0.8888, eval/one: 0.7845, eval/two: 0.6141, eval/three: 0.6989 \n",
      "Epoch: 45, train/loss: 0.2540, eval/loss: 0.1431, train/r2: 0.7633, eval/r2: 0.7745, train/one: 0.8277, train/two: 0.5617, train/three: 0.9005, eval/one: 0.8343, eval/two: 0.6284, eval/three: 0.8608 \n",
      "Epoch: 50, train/loss: 0.1734, eval/loss: 0.1553, train/r2: 0.8324, eval/r2: 0.7981, train/one: 0.8614, train/two: 0.7307, train/three: 0.9051, eval/one: 0.8826, eval/two: 0.5910, eval/three: 0.9209 \n",
      "Epoch: 55, train/loss: 0.1541, eval/loss: 0.1196, train/r2: 0.8453, eval/r2: 0.8601, train/one: 0.8975, train/two: 0.7231, train/three: 0.9155, eval/one: 0.8320, eval/two: 0.8293, eval/three: 0.9191 \n",
      "Epoch: 60, train/loss: 0.1295, eval/loss: 0.1536, train/r2: 0.8747, eval/r2: 0.7402, train/one: 0.8759, train/two: 0.8554, train/three: 0.8928, eval/one: 0.5670, eval/two: 0.8495, eval/three: 0.8041 \n",
      "Epoch: 65, train/loss: 0.1006, eval/loss: 0.0743, train/r2: 0.8995, eval/r2: 0.9160, train/one: 0.9308, train/two: 0.8579, train/three: 0.9099, eval/one: 0.9067, eval/two: 0.9203, eval/three: 0.9211 \n",
      "Epoch: 70, train/loss: 0.0864, eval/loss: 0.1288, train/r2: 0.9081, eval/r2: 0.8700, train/one: 0.8945, train/two: 0.8884, train/three: 0.9414, eval/one: 0.8541, eval/two: 0.9347, eval/three: 0.8211 \n",
      "Epoch: 75, train/loss: 0.0902, eval/loss: 0.0869, train/r2: 0.9152, eval/r2: 0.8788, train/one: 0.9151, train/two: 0.9078, train/three: 0.9226, eval/one: 0.8237, eval/two: 0.9355, eval/three: 0.8773 \n",
      "Epoch: 80, train/loss: 0.0862, eval/loss: 0.0838, train/r2: 0.9149, eval/r2: 0.8830, train/one: 0.9473, train/two: 0.8940, train/three: 0.9035, eval/one: 0.8857, eval/two: 0.8583, eval/three: 0.9049 \n",
      "Epoch: 85, train/loss: 0.0749, eval/loss: 0.0962, train/r2: 0.9192, eval/r2: 0.8861, train/one: 0.9319, train/two: 0.9110, train/three: 0.9147, eval/one: 0.8691, eval/two: 0.8627, eval/three: 0.9265 \n",
      "Epoch: 90, train/loss: 0.0803, eval/loss: 0.0855, train/r2: 0.9182, eval/r2: 0.8585, train/one: 0.9435, train/two: 0.8992, train/three: 0.9119, eval/one: 0.8633, eval/two: 0.7979, eval/three: 0.9145 \n",
      "Epoch: 95, train/loss: 0.0787, eval/loss: 0.1051, train/r2: 0.9206, eval/r2: 0.8442, train/one: 0.9277, train/two: 0.9292, train/three: 0.9048, eval/one: 0.8707, eval/two: 0.8091, eval/three: 0.8528 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 128 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 128 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-161/metadata\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "inputs_mean_std = []\n",
    "targets_mean_std = []\n",
    "scores = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "splits = kfold.split(inputs)\n",
    "\n",
    "for fold, (train_idx, eval_idx) in enumerate(splits):\n",
    "    MODEL_NAME = f\"resnet.paper.pretrain.avg.fold.{fold}\"\n",
    "    checkpoint_name = f\"paper.pretrain.avg.fold.{fold}.pt\"\n",
    "    \n",
    "    train_inputs = inputs[train_idx]\n",
    "    train_targets = targets[train_idx]\n",
    "    eval_inputs = inputs[eval_idx]\n",
    "    eval_targets = targets[eval_idx]\n",
    "\n",
    "    train_ds = get_dataset(train_inputs, train_targets, config)\n",
    "    \n",
    "    inputs_mean_std.append((fold, train_ds.s_mean, train_ds.s_std))\n",
    "    targets_mean_std.append((fold, train_ds.concentration_means, train_ds.concentration_stds))\n",
    "    \n",
    "    eval_ds = get_dataset(eval_inputs, eval_targets, config, (train_ds.s_mean, train_ds.s_std), (train_ds.concentration_means, train_ds.concentration_stds))\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    train_dl, eval_dl = return_dls(train_ds, eval_ds, BATCH_SIZE, len(eval_ds))\n",
    "    \n",
    "    #model = ResNet(input_channels=1, dropout=DROPOUT).to(device)\n",
    "    model = ReZeroNet(**config).to(device)\n",
    "    if fold == 0: print(get_model_size(model))\n",
    "    \n",
    "    ckpt = get_ckpt(\"/kaggle/working/paper.pretrain.avg.pt\")\n",
    "    model.load_state_dict(ckpt)#[\"state_dict\"])\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    scheduler = get_scheduler(optimizer, train_dl, EPOCHS)\n",
    "    \n",
    "    score = train(\n",
    "            model, \n",
    "            optimizer, \n",
    "            device,\n",
    "            scaler,\n",
    "            scheduler,\n",
    "            train_dl, \n",
    "            eval_dl,\n",
    "            mse_loss_function,\n",
    "            EPOCHS,\n",
    "            checkpoint_name,\n",
    "            neptune_run=setup_neptune(),\n",
    "        )\n",
    "    \n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4925ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralTestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra,\n",
    "        concentrations,\n",
    "        dtype=None,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=0.0,\n",
    "        baseline_factor_bound=0.0,\n",
    "        baseline_period_lower_bound=100.0,\n",
    "        baseline_period_upper_bound=200.0,\n",
    "        augment_slope_std=0.0,\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=0,\n",
    "        spectrum_rolling_sigma=0.0,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    ):\n",
    "        self.dtype = dtype or torch.float32\n",
    "        self.combine_spectra_range = combine_spectra_range\n",
    "        self.baseline_factor_bound = baseline_factor_bound\n",
    "        self.augment_slope_std = augment_slope_std\n",
    "        self.augment_intercept_std = augment_intersept_std\n",
    "        self.baseline_period_lower_bound = baseline_period_lower_bound\n",
    "        self.baseline_period_upper_bound = baseline_period_upper_bound\n",
    "        self.rolling_bound = rolling_bound\n",
    "        self.spectrum_rolling_sigma = spectrum_rolling_sigma\n",
    "        self.augmentation_weight = torch.tensor(augmentation_weight, dtype=dtype)\n",
    "        self.original_dp_weight = original_datapoint_weight\n",
    "\n",
    "        # normalize spectra\n",
    "        spectra = torch.tensor(spectra, dtype=dtype)\n",
    "\n",
    "        if spectra_mean_std is None:\n",
    "            self.s_mean = torch.mean(spectra)\n",
    "            self.s_std = torch.std(spectra)\n",
    "        else:\n",
    "            self.s_mean, self.s_std = spectra_mean_std\n",
    "\n",
    "        self.spectra = torch.divide(\n",
    "            torch.subtract(spectra, self.s_mean),\n",
    "            self.s_std,\n",
    "        )\n",
    "\n",
    "        self.dummy_wns = np.tile(\n",
    "            np.arange(\n",
    "                0., 1., 1. / self.spectra.shape[2],\n",
    "                dtype=np_dtype_from_torch[self.dtype]\n",
    "            )[None, :self.spectra.shape[2]],\n",
    "            (self.spectra.shape[1], 1),\n",
    "        )\n",
    "\n",
    "        if False:\n",
    "            # normalize concentrations\n",
    "            concentrations = torch.tensor(concentrations, dtype=dtype)\n",
    "            if concentration_mean_std is None:\n",
    "                self.concentration_means = torch.nanmean(concentrations, dim=0)\n",
    "\n",
    "                self.concentration_stds = torch.maximum(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            torch.std(col[torch.logical_not(torch.isnan(col))])\n",
    "                            for col in concentrations.T\n",
    "                        ]\n",
    "                    ),\n",
    "                    torch.tensor([1e-3] * concentrations.shape[1]),\n",
    "                )\n",
    "            else:\n",
    "                self.concentration_means = concentration_mean_std[0]\n",
    "                self.concentration_stds = concentration_mean_std[1]\n",
    "\n",
    "            self.concentrations = torch.divide(\n",
    "                torch.subtract(\n",
    "                    concentrations,\n",
    "                    self.concentration_means,\n",
    "                ),\n",
    "                self.concentration_stds,\n",
    "            )\n",
    "\n",
    "    def pick_two(self, max_idx=None):\n",
    "        max_idx = max_idx or len(self)\n",
    "        return random.choices(range(max_idx), k=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 96\n",
    "\n",
    "    def augment_spectra(self, spectra):\n",
    "        if self.augment_slope_std > 0.0:\n",
    "\n",
    "            def spectrum_approximation(x, slope, intercept):\n",
    "                return (slope * x + intercept).reshape(-1, 1)[:, 0]\n",
    "\n",
    "            slope, inter = scipy.optimize.curve_fit(\n",
    "                spectrum_approximation,\n",
    "                self.dummy_wns,\n",
    "                spectra.reshape(-1, 1)[:, 0],\n",
    "                p0=np.random.rand(2),\n",
    "            )[0]\n",
    "\n",
    "            new_slope = slope * (\n",
    "                    np.random.gamma(\n",
    "                        shape=1. / self.augment_slope_std,\n",
    "                        scale=self.augment_slope_std,\n",
    "                        size=1,\n",
    "                    )\n",
    "            )[0]\n",
    "            new_intercept = inter * (\n",
    "                1.0 + np.random.randn(1) * self.augment_intercept_std\n",
    "            )[0]\n",
    "            spectra += torch.tensor(\n",
    "                (new_slope - slope)\n",
    "            ) * self.dummy_wns + new_intercept - inter\n",
    "\n",
    "        factor = self.baseline_factor_bound * torch.rand(size=(1,))\n",
    "        offset = torch.rand(size=(1,)) * 2.0 * torch.pi\n",
    "        period = self.baseline_period_lower_bound + (\n",
    "            self.baseline_period_upper_bound - self.baseline_period_lower_bound\n",
    "        ) * torch.rand(size=(1,))\n",
    "        permutations = factor * torch.cos(\n",
    "            2.0 * torch.pi / period * self.dummy_wns + offset\n",
    "        )\n",
    "        return self.roll_spectrum(\n",
    "            spectra + permutations * spectra,\n",
    "            delta=random.randint(-self.rolling_bound, self.rolling_bound),\n",
    "        )\n",
    "\n",
    "    def roll_spectrum(self, spectra, delta):\n",
    "        num_spectra = spectra.shape[0]\n",
    "        rolled_spectra = np.roll(spectra, delta, axis=1)\n",
    "        if delta > 0:\n",
    "            rolled_spectra[:, :delta] = (\n",
    "                np.random.rand(num_spectra, delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta:(delta + 1)]\n",
    "        elif delta < 0:\n",
    "            rolled_spectra[:, delta:] = (\n",
    "                np.random.rand(num_spectra, -delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta - 1:delta]\n",
    "        return rolled_spectra\n",
    "\n",
    "    def combine_k_items(self, indices, weights):\n",
    "        return (\n",
    "            # spectra\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None, None], self.spectra[indices, :, :]),\n",
    "                dim=0,\n",
    "            ),\n",
    "            # concentrations\n",
    "            #torch.sum(\n",
    "            #    torch.mul(weights[:, None], self.concentrations[indices, :]),\n",
    "            #    dim=0,\n",
    "            #)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if True:#self.combine_spectra_range < 1e-12:\n",
    "            spectrum = self.spectra[idx]\n",
    "            #spectrum = self.augment_spectra(spectrum)\n",
    "            return spectrum\n",
    "        else:\n",
    "            if random.random() < self.original_dp_weight:\n",
    "                one_weight = 1.\n",
    "                label_weight = torch.tensor(1.0, dtype=self.dtype)\n",
    "            else:\n",
    "                one_weight = random.uniform(0.0, self.combine_spectra_range)\n",
    "                label_weight = self.augmentation_weight\n",
    "            weights = torch.tensor([one_weight, (1 - one_weight)])\n",
    "            # just pick two random indices\n",
    "            indices = random.choices(range(len(self)), k=2)\n",
    "\n",
    "            mixed_spectra = self.combine_k_items(\n",
    "                indices=indices,\n",
    "                weights=weights,\n",
    "            )\n",
    "            mixed_spectra = self.augment_spectra(mixed_spectra[0])\n",
    "            return mixed_spectra\n",
    "        \n",
    "  \n",
    "def get_test_dataset(inputs, inputs_mean_std, targets_mean_std):\n",
    "    return SpectralTestDataset(\n",
    "        spectra=inputs[:, None, :],\n",
    "        concentrations=None,\n",
    "        dtype=torch.float32,\n",
    "        spectra_mean_std=inputs_mean_std,\n",
    "        concentration_mean_std=targets_mean_std,\n",
    "        combine_spectra_range=1.0,\n",
    "        baseline_factor_bound=config[\"baseline_factor_bound\"],\n",
    "        baseline_period_lower_bound=config[\"baseline_period_lower_bound\"],\n",
    "        baseline_period_upper_bound=(config[\"baseline_period_lower_bound\"] + config[\"baseline_period_span\"]),\n",
    "        augment_slope_std=config[\"augment_slope_std\"],\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=config[\"rolling_bound\"],\n",
    "        spectrum_rolling_sigma=0.01,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f43bf854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/paper.pretrain.avg.fold.0.pt 93 0.8796180235365756\n",
      "/kaggle/working/paper.pretrain.avg.fold.1.pt 97 0.9127566862313433\n",
      "/kaggle/working/paper.pretrain.avg.fold.2.pt 75 0.9185037460711425\n",
      "/kaggle/working/paper.pretrain.avg.fold.3.pt 94 0.8696868817861656\n",
      "/kaggle/working/paper.pretrain.avg.fold.4.pt 65 0.9160192436386044\n"
     ]
    }
   ],
   "source": [
    "def get_ckpt_paths():\n",
    "    output_dir = \"/kaggle/working\"\n",
    "    output_files = sorted(os.listdir(output_dir))\n",
    "\n",
    "    ckpt_paths = []\n",
    "    for f in output_files:\n",
    "        if \"paper.pretrain.avg.fold\" in f and \"csv\" not in f:\n",
    "            ckpt_path = os.path.join(output_dir, f)\n",
    "            ckpt_paths.append(ckpt_path)\n",
    "            ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "            print(ckpt_path, ckpt[\"epoch\"], ckpt[\"score\"])\n",
    "            \n",
    "    return ckpt_paths\n",
    "\n",
    "ckpt_paths = get_ckpt_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "360479da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "def average_state_dicts(state_dict_list):\n",
    "    \"\"\"\n",
    "    Average the weights of several compatible PyTorch state_dicts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_dict_list : list[dict]\n",
    "        List of state_dicts (e.g. from torch.load).\n",
    "        All must have identical keys and compatible tensor shapes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrderedDict\n",
    "        New state_dict whose tensors are the arithmetic mean\n",
    "        of the corresponding tensors in the input state_dicts.\n",
    "    \"\"\"\n",
    "    if not state_dict_list:\n",
    "        raise ValueError(\"state_dict_list is empty\")\n",
    "\n",
    "    n = len(state_dict_list)\n",
    "    # Ensure we don't modify the originals\n",
    "    avg_sd = OrderedDict()\n",
    "\n",
    "    # Iterate over every parameter/buffer key\n",
    "    for k in state_dict_list[0]:\n",
    "        # sum across models  float32 to avoid overflow on int types\n",
    "        avg = sum(sd[k].float() for sd in state_dict_list) / n\n",
    "        # cast back to original dtype if needed\n",
    "        avg_sd[k] = avg.to(dtype=state_dict_list[0][k].dtype)\n",
    "\n",
    "    return avg_sd\n",
    "\n",
    "\n",
    "state_dicts = []\n",
    "for cp in ckpt_paths:\n",
    "    ckpt = get_ckpt(cp)\n",
    "    state_dicts.append(ckpt[\"state_dict\"])\n",
    "\n",
    "avg_state_dict = average_state_dicts(state_dicts)\n",
    "torch.save(avg_state_dict, \"paper.finetune.avg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb743994",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "for i in range(5):\n",
    "    means.append(targets_mean_std[i][1:][0])\n",
    "    stds.append(targets_mean_std[i][1:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "269d158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = torch.stack(means).mean(dim=0)\n",
    "stds = torch.stack(stds).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91f09c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.8749, 1.1728, 1.5785]), tensor([2.9148, 0.5509, 0.6642]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4786020e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.55467238, 0.97522789, 0.66487008],\n",
       "       [6.00770376, 1.6246631 , 1.7874143 ],\n",
       "       [5.58514936, 0.65663363, 1.16576816],\n",
       "       [4.3727229 , 1.12202299, 0.55961518],\n",
       "       [8.6422731 , 0.77807372, 1.07656401],\n",
       "       [7.22980387, 1.45137016, 1.00378166],\n",
       "       [7.51131046, 0.85546698, 0.42494893],\n",
       "       [7.40632432, 1.60870025, 1.25100468],\n",
       "       [7.42605724, 1.58711335, 1.08990877],\n",
       "       [9.45828925, 0.78054841, 0.37888773],\n",
       "       [8.51004121, 1.26016664, 1.38555842],\n",
       "       [4.09113637, 1.32622821, 1.18116114],\n",
       "       [4.94532741, 1.37489915, 1.39924715],\n",
       "       [5.2825231 , 1.31122966, 1.97904221],\n",
       "       [5.38963794, 1.33641478, 1.44609179],\n",
       "       [7.78276895, 1.35065231, 0.91578834],\n",
       "       [5.97056971, 1.28275809, 1.114799  ],\n",
       "       [6.1743793 , 1.28473473, 1.50536339],\n",
       "       [6.31448413, 1.44891708, 1.32722498],\n",
       "       [5.24208025, 1.27367718, 1.32470422],\n",
       "       [7.07865173, 1.23531133, 0.93651088],\n",
       "       [6.41079666, 1.33530479, 1.33659076],\n",
       "       [3.25496213, 1.19361709, 0.70646297],\n",
       "       [4.98505551, 1.3799636 , 1.66974153],\n",
       "       [5.36433829, 1.30727345, 1.25670861],\n",
       "       [4.64185741, 1.51312172, 1.74242341],\n",
       "       [6.43054352, 1.30528492, 0.99781052],\n",
       "       [5.47947514, 1.38204756, 1.64197307],\n",
       "       [6.0642841 , 1.35283245, 1.26484404],\n",
       "       [6.80669095, 0.89003883, 0.41533585],\n",
       "       [6.55519988, 1.402775  , 1.74446314],\n",
       "       [7.67039528, 0.85851955, 1.04987489],\n",
       "       [6.73274187, 1.51469646, 1.4839139 ],\n",
       "       [7.85485942, 1.3758727 , 0.92801731],\n",
       "       [6.46100123, 1.22304892, 0.9283561 ],\n",
       "       [6.37167129, 0.75307997, 0.88382453],\n",
       "       [7.23475431, 0.87404352, 0.72043091],\n",
       "       [7.31636733, 1.46689754, 0.42440897],\n",
       "       [4.40218562, 1.33888693, 0.66922905],\n",
       "       [8.45282777, 1.51644453, 1.67684228],\n",
       "       [6.09781477, 1.30190186, 1.48180261],\n",
       "       [4.29500737, 1.26892029, 0.57519224],\n",
       "       [7.86742828, 0.56587259, 1.367729  ],\n",
       "       [4.37453428, 0.83046093, 1.15341853],\n",
       "       [2.92934353, 1.19019142, 0.90773633],\n",
       "       [5.81046618, 0.69433104, 1.58571779],\n",
       "       [3.94553843, 0.7699629 , 1.46996931],\n",
       "       [5.24337632, 1.439539  , 1.45529946],\n",
       "       [5.13918338, 1.36706856, 1.74401084],\n",
       "       [7.30564416, 0.76674067, 1.57734153],\n",
       "       [2.80919587, 1.23713244, 1.8780139 ],\n",
       "       [5.28216208, 1.33977125, 1.72442611],\n",
       "       [6.20667739, 1.20303749, 1.24567015],\n",
       "       [4.02597815, 1.33060534, 1.21562154],\n",
       "       [5.5335231 , 1.35901492, 1.43511769],\n",
       "       [4.29805088, 1.23923186, 1.923108  ],\n",
       "       [4.60181224, 1.44246473, 1.63990431],\n",
       "       [4.27350143, 1.13667448, 1.79546701],\n",
       "       [5.98295164, 0.62667395, 1.46786472],\n",
       "       [7.3749777 , 0.83669525, 0.64921607],\n",
       "       [3.56817943, 0.90416476, 1.36925093],\n",
       "       [3.27645539, 1.42474337, 1.60437159],\n",
       "       [5.73932657, 1.38389439, 0.63164068],\n",
       "       [7.64708796, 1.34495694, 2.04346916],\n",
       "       [6.82669252, 1.31812939, 0.61527038],\n",
       "       [6.22882668, 0.81336279, 0.58036028],\n",
       "       [7.7004977 , 1.47519321, 1.68196705],\n",
       "       [5.76418475, 1.35513948, 0.59917078],\n",
       "       [7.10632385, 1.19071928, 0.913178  ],\n",
       "       [7.7054954 , 1.24001115, 1.05085735],\n",
       "       [8.34299344, 1.21240549, 1.33494162],\n",
       "       [7.49291357, 1.46632301, 1.57505492],\n",
       "       [5.07252686, 1.38890254, 1.98367425],\n",
       "       [7.59710985, 0.59026911, 1.15617694],\n",
       "       [5.76190351, 1.37026948, 1.78140092],\n",
       "       [7.00191724, 0.53234432, 1.19513664],\n",
       "       [5.50862704, 1.06513402, 0.43942049],\n",
       "       [5.61546277, 1.31165434, 1.8392345 ],\n",
       "       [3.27442233, 1.25400947, 0.91386855],\n",
       "       [4.45226249, 0.72184591, 1.48781115],\n",
       "       [8.18827574, 0.85224774, 0.62451198],\n",
       "       [3.26688982, 1.17358375, 1.98831219],\n",
       "       [7.4568219 , 0.84999331, 2.1248277 ],\n",
       "       [5.20071643, 1.03517975, 1.43863563],\n",
       "       [4.23223038, 1.00179845, 1.62113803],\n",
       "       [4.78259282, 1.25811966, 1.40817459],\n",
       "       [7.24555903, 1.23322279, 0.72578493],\n",
       "       [5.19440893, 1.26814724, 1.10915678],\n",
       "       [6.81506262, 1.43271627, 1.50578073],\n",
       "       [4.94174219, 1.36587564, 1.45602093],\n",
       "       [6.34137946, 1.34970964, 1.5852628 ],\n",
       "       [5.16899375, 1.35563402, 1.53418741],\n",
       "       [6.31458147, 0.53777751, 1.0126281 ],\n",
       "       [5.53312368, 0.67207566, 0.96958527],\n",
       "       [3.50412913, 1.20283378, 1.06414119],\n",
       "       [4.39558191, 1.4972503 , 1.01471507]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(ckpt_name, i):\n",
    "    ckpt = get_ckpt(ckpt_name)\n",
    "    \n",
    "    test_inputs = get_test_data()\n",
    "    test_ds = get_test_dataset(test_inputs, inputs_mean_std[i][1:], targets_mean_std[i][1:])\n",
    "    test_dl = DataLoader(test_ds, batch_size=32)\n",
    "    \n",
    "    model = ReZeroNet(**config).to(device)\n",
    "    model.load_state_dict(ckpt)#[\"state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    for inputs in test_dl:\n",
    "        with torch.inference_mode():\n",
    "            preds = model(inputs.cuda())\n",
    "            preds = preds.double() \n",
    "            all_preds.append(cuda_to_np(preds))\n",
    "            \n",
    "    preds = np.concatenate(all_preds)\n",
    "    \n",
    "    #mus = targets_mean_std[i][1:][0]\n",
    "    #sigmas = targets_mean_std[i][1:][1]\n",
    "    mus = means\n",
    "    sigmas = stds\n",
    "\n",
    "    for i in range(3):\n",
    "        preds[:, i] = reverse_zscore(preds[:, i], mus[i].numpy(), sigmas[i].numpy())\n",
    "    \n",
    "    return preds\n",
    "\n",
    "preds = inference(\"paper.finetune.avg.pt\", 2)\n",
    "generate_csv(preds, \"paper.finetune.avg.pretrain.avg.finetune.csv\")\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b84e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_inference(ckpt_paths):\n",
    "    test_inputs = get_test_data()\n",
    "    all_preds = []\n",
    "\n",
    "    for i, ckpt_path in enumerate(ckpt_paths):\n",
    "        ckpt = get_ckpt(ckpt_path)\n",
    "        \n",
    "        model = ReZeroNet(**config).to(device)\n",
    "        model.load_state_dict(ckpt[\"state_dict\"])\n",
    "        model.eval()\n",
    "\n",
    "        test_ds = get_test_dataset(test_inputs, inputs_mean_std[i][1:], targets_mean_std[i][1:])\n",
    "        test_dl = DataLoader(test_ds, batch_size=32)\n",
    "        \n",
    "        fold_preds = []\n",
    "        for inputs in test_dl:\n",
    "            with torch.inference_mode():\n",
    "                preds = model(inputs.cuda())\n",
    "                preds = cuda_to_np(preds.double())\n",
    "                fold_preds.append(preds)\n",
    "                \n",
    "        fold_preds = np.concatenate(fold_preds)\n",
    "        \n",
    "        means = targets_mean_std[i][1:][0]\n",
    "        stds = targets_mean_std[i][1:][1]\n",
    "        for i in range(3):\n",
    "            fold_preds[:, i] = reverse_zscore(fold_preds[:, i], means[i].numpy(), stds[i].numpy())\n",
    "            \n",
    "        all_preds.append(fold_preds)\n",
    "\n",
    "    return np.mean(all_preds, axis=0)\n",
    "\n",
    "preds = ensemble_inference(ckpt_paths)\n",
    "generate_csv(preds, \"paper.finetune.avg.pretrain.weights.ensemble.csv\")\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
