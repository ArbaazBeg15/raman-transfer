{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "SEED = 1000\n",
    "setup_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "079da8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_index(iterable):\n",
    "    return random.randint(0, len(iterable) - 1)\n",
    "\n",
    "\n",
    "def get_indices(iterable, n):\n",
    "    return random.sample(range(len(iterable)), n)\n",
    "\n",
    "\n",
    "def get_stats(tensor, p=True, r=False, minmax=False):\n",
    "    if minmax:\n",
    "        min, max = tensor.min(), tensor.max()\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Min: {min}, Max: {max} ,Mean: {mean}, Std: {std}\")\n",
    "        if r: return min, max, mean, std\n",
    "    else:\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Mean: {mean}, Std: {std}\")\n",
    "        if r: return mean, std\n",
    "        \n",
    "        \n",
    "\n",
    "def show_waves(waves, dpi=100):\n",
    "    \"\"\"\n",
    "    waves: numpy array of shape (3, N)\n",
    "    Creates three separate figures that stretch wide.\n",
    "    \"\"\"\n",
    "    N = waves.shape[1]\n",
    "    t = np.arange(N)\n",
    "\n",
    "    # Wide aspect ratio; height modest so each window fills width\n",
    "    for i in range(waves.shape[0]):\n",
    "        fig = plt.figure(figsize=(14, 4), dpi=dpi)  # wide figure\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(t, waves[i], linewidth=1)\n",
    "        ax.set_title(f\"Wave {i+1}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()  # reduce margins to use width\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c619231b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'sample_submission.csv'),\n",
       " (1, 'timegate.csv'),\n",
       " (2, 'mettler_toledo.csv'),\n",
       " (3, 'kaiser.csv'),\n",
       " (4, 'anton_532.csv'),\n",
       " (5, 'transfer_plate.csv'),\n",
       " (6, '96_samples.csv'),\n",
       " (7, 'tornado.csv'),\n",
       " (8, 'tec5.csv'),\n",
       " (9, 'metrohm.csv'),\n",
       " (10, 'anton_785.csv')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a02fcb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_dataset(name, lower, upper, indices):\n",
    "    path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "    df = pd.read_csv(os.path.join(path, name))\n",
    "\n",
    "    spectra_selection = np.logical_and(\n",
    "        lower <= np.array([float(one) for one in df.columns[:-5]]),\n",
    "        np.array([float(one) for one in df.columns[:-5]]) <= upper,\n",
    "    )\n",
    "    \n",
    "    spectra = df.iloc[:, :-5].iloc[:, spectra_selection].values\n",
    "    label = df.iloc[:, -5:-2].values\n",
    "\n",
    "    wavenumbers = np.array([\n",
    "        float(one) for one in df.columns[:-5]\n",
    "    ])[spectra_selection]\n",
    "\n",
    "    #indices = get_indices(spectra, num_samples)                         \n",
    "    return spectra[indices], label[indices], wavenumbers\n",
    "\n",
    "\n",
    "def load_datasets(name, lower, upper, indices):\n",
    "    #datasets = [get_dataset(name, lower, upper, indices) for name in ds_names]\n",
    "    datasets = [get_dataset(name, lower, upper, indices)]\n",
    "    wavenumbers = np.arange(lower, upper + 1)\n",
    "\n",
    "    interpolated_data = [\n",
    "        np.array([\n",
    "            np.interp(\n",
    "                wavenumbers,\n",
    "                xp=wns,\n",
    "                fp=spectrum,\n",
    "            )\n",
    "            for spectrum in spectra\n",
    "        ])\n",
    "        for spectra, _, wns in datasets\n",
    "    ]\n",
    "\n",
    "    normed_spectra = np.concatenate(\n",
    "        [\n",
    "            spectra / np.max(spectra)\n",
    "            for spectra in interpolated_data\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    labels = np.concatenate([ds[1] for ds in datasets])\n",
    "    return normed_spectra, labels\n",
    "\n",
    "ds_names = [\n",
    "    \"timegate.csv\",\n",
    "    \"mettler_toledo.csv\",\n",
    "    \"kaiser.csv\",\n",
    "    \"anton_532.csv\",\n",
    "    #\"transfer_plate.csv\",\n",
    "    \"tornado.csv\",\n",
    "    \"tec5.csv\",\n",
    "    \"metrohm.csv\",\n",
    "    \"anton_785.csv\"\n",
    "]\n",
    "\n",
    "#inputs, targets = load_datasets(ds_names, 300, 1600)\n",
    "#inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e86128ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timegate.csv'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ds_names)):\n",
    "    df = pd.read_csv(os.path.join(path, ds_names[0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ef68faf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200.925569</th>\n",
       "      <th>205.066898</th>\n",
       "      <th>209.205702</th>\n",
       "      <th>213.34198</th>\n",
       "      <th>217.475732</th>\n",
       "      <th>221.606958</th>\n",
       "      <th>225.735655</th>\n",
       "      <th>229.861825</th>\n",
       "      <th>233.985465</th>\n",
       "      <th>238.106575</th>\n",
       "      <th>...</th>\n",
       "      <th>1986.346421</th>\n",
       "      <th>1989.186168</th>\n",
       "      <th>1992.023478</th>\n",
       "      <th>1994.858351</th>\n",
       "      <th>1997.690789</th>\n",
       "      <th>glucose</th>\n",
       "      <th>Na_acetate</th>\n",
       "      <th>Mg_SO4</th>\n",
       "      <th>MSM_present</th>\n",
       "      <th>fold_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.017310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.270740</td>\n",
       "      <td>1.045500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.192280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.257870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004135</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>4.778830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.346020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.767730</td>\n",
       "      <td>3.945410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.004277</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>6.459915</td>\n",
       "      <td>0.109961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.005071</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.004679</td>\n",
       "      <td>0.004443</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>10.161200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.477840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.253690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022609</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows Ã— 516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     200.925569  205.066898  209.205702  213.34198  217.475732  221.606958  \\\n",
       "0      0.002535    0.002427    0.002327   0.002248    0.002108    0.002049   \n",
       "1      0.003631    0.003540    0.003338   0.003195    0.003070    0.002916   \n",
       "2      0.001688    0.001623    0.001571   0.001486    0.001440    0.001344   \n",
       "3      0.001294    0.001263    0.001200   0.001149    0.001110    0.001058   \n",
       "4      0.004135    0.003928    0.003792   0.003595    0.003484    0.003296   \n",
       "..          ...         ...         ...        ...         ...         ...   \n",
       "128    0.002949    0.002867    0.002736   0.002626    0.002482    0.002416   \n",
       "129    0.002331    0.002245    0.002147   0.002080    0.001962    0.001912   \n",
       "130    0.004277    0.004147    0.003946   0.003776    0.003590    0.003524   \n",
       "131    0.005071    0.004859    0.004679   0.004443    0.004292    0.004107   \n",
       "132    0.002622    0.002543    0.002438   0.002353    0.002226    0.002162   \n",
       "\n",
       "     225.735655  229.861825  233.985465  238.106575  ...  1986.346421  \\\n",
       "0      0.001927    0.001877    0.001767    0.001698  ...     0.000087   \n",
       "1      0.002789    0.002665    0.002522    0.002378  ...     0.000150   \n",
       "2      0.001290    0.001239    0.001176    0.001128  ...     0.000079   \n",
       "3      0.001006    0.000959    0.000929    0.000869  ...     0.000060   \n",
       "4      0.003162    0.003022    0.002829    0.002718  ...     0.000167   \n",
       "..          ...         ...         ...         ...  ...          ...   \n",
       "128    0.002288    0.002178    0.002095    0.001957  ...     0.000123   \n",
       "129    0.001822    0.001756    0.001657    0.001576  ...     0.000118   \n",
       "130    0.003312    0.003170    0.002981    0.002887  ...     0.000458   \n",
       "131    0.003913    0.003722    0.003519    0.003336  ...     0.000248   \n",
       "132    0.002041    0.001949    0.001849    0.001758  ...     0.000122   \n",
       "\n",
       "     1989.186168  1992.023478  1994.858351  1997.690789    glucose  \\\n",
       "0       0.000084     0.000102     0.000074     0.000094   0.000000   \n",
       "1       0.000136     0.000146     0.000149     0.000151   0.270740   \n",
       "2       0.000072     0.000083     0.000063     0.000074   0.000000   \n",
       "3       0.000066     0.000066     0.000063     0.000065   0.257870   \n",
       "4       0.000170     0.000156     0.000170     0.000168   4.778830   \n",
       "..           ...          ...          ...          ...        ...   \n",
       "128     0.000128     0.000127     0.000123     0.000127   0.346020   \n",
       "129     0.000115     0.000116     0.000113     0.000109   0.000000   \n",
       "130     0.000457     0.000470     0.000471     0.000469   6.459915   \n",
       "131     0.000244     0.000251     0.000250     0.000258  10.161200   \n",
       "132     0.000129     0.000109     0.000131     0.000119   0.253690   \n",
       "\n",
       "     Na_acetate    Mg_SO4  MSM_present  fold_idx  \n",
       "0      1.017310  0.000000          0.0         0  \n",
       "1      1.045500  0.000000          0.0         0  \n",
       "2      1.192280  0.000000          0.0         0  \n",
       "3      0.000000  0.000000          0.0         0  \n",
       "4      0.000000  0.023272          0.0         0  \n",
       "..          ...       ...          ...       ...  \n",
       "128    0.000000  0.000000          0.0         4  \n",
       "129    0.767730  3.945410          0.0         4  \n",
       "130    0.109961  0.000000          1.0         4  \n",
       "131    0.000000  1.477840          0.0         4  \n",
       "132    0.000000  0.022609          1.0         4  \n",
       "\n",
       "[133 rows x 516 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(path, ds_names[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ac0bbd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 1201), (96, 3))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "np_dtype_from_torch = {\n",
    "    torch.float32: np.float32,\n",
    "    torch.float64: np.float64,\n",
    "}\n",
    "\n",
    "class SpectralDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra,\n",
    "        concentrations,\n",
    "        dtype=None,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=0.0,\n",
    "        baseline_factor_bound=0.0,\n",
    "        baseline_period_lower_bound=100.0,\n",
    "        baseline_period_upper_bound=200.0,\n",
    "        augment_slope_std=0.0,\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=0,\n",
    "        spectrum_rolling_sigma=0.0,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    ):\n",
    "        self.dtype = dtype or torch.float32\n",
    "        self.combine_spectra_range = combine_spectra_range\n",
    "        self.baseline_factor_bound = baseline_factor_bound\n",
    "        self.augment_slope_std = augment_slope_std\n",
    "        self.augment_intercept_std = augment_intersept_std\n",
    "        self.baseline_period_lower_bound = baseline_period_lower_bound\n",
    "        self.baseline_period_upper_bound = baseline_period_upper_bound\n",
    "        self.rolling_bound = rolling_bound\n",
    "        self.spectrum_rolling_sigma = spectrum_rolling_sigma\n",
    "        self.augmentation_weight = torch.tensor(augmentation_weight, dtype=dtype)\n",
    "        self.original_dp_weight = original_datapoint_weight\n",
    "\n",
    "        # normalize spectra\n",
    "        spectra = torch.tensor(spectra, dtype=dtype)\n",
    "\n",
    "        if spectra_mean_std is None:\n",
    "            self.s_mean = torch.mean(spectra)\n",
    "            self.s_std = torch.std(spectra)\n",
    "        else:\n",
    "            self.s_mean, self.s_std = spectra_mean_std\n",
    "\n",
    "        self.spectra = torch.divide(\n",
    "            torch.subtract(spectra, self.s_mean),\n",
    "            self.s_std,\n",
    "        )\n",
    "        print(self.spectra.shape)\n",
    "        self.dummy_wns = np.tile(\n",
    "            np.arange(\n",
    "                0., 1., 1. / self.spectra.shape[2],\n",
    "                dtype=np_dtype_from_torch[self.dtype]\n",
    "            )[None, :self.spectra.shape[2]],\n",
    "            (self.spectra.shape[1], 1),\n",
    "        )\n",
    "\n",
    "        # normalize concentrations\n",
    "        concentrations = torch.tensor(concentrations, dtype=dtype)\n",
    "        if concentration_mean_std is None:\n",
    "            self.concentration_means = torch.nanmean(concentrations, dim=0)\n",
    "\n",
    "            self.concentration_stds = torch.maximum(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        torch.std(col[torch.logical_not(torch.isnan(col))])\n",
    "                        for col in concentrations.T\n",
    "                    ]\n",
    "                ),\n",
    "                torch.tensor([1e-3] * concentrations.shape[1]),\n",
    "            )\n",
    "        else:\n",
    "            self.concentration_means = concentration_mean_std[0]\n",
    "            self.concentration_stds = concentration_mean_std[1]\n",
    "\n",
    "        self.concentrations = torch.divide(\n",
    "            torch.subtract(\n",
    "                concentrations,\n",
    "                self.concentration_means,\n",
    "            ),\n",
    "            self.concentration_stds,\n",
    "        )\n",
    "\n",
    "    def pick_two(self, max_idx=None):\n",
    "        max_idx = max_idx or len(self)\n",
    "        return random.choices(range(max_idx), k=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.concentrations)\n",
    "\n",
    "    def augment_spectra(self, spectra):\n",
    "        if self.augment_slope_std > 0.0:\n",
    "\n",
    "            def spectrum_approximation(x, slope, intercept):\n",
    "                return (slope * x + intercept).reshape(-1, 1)[:, 0]\n",
    "\n",
    "            slope, inter = scipy.optimize.curve_fit(\n",
    "                spectrum_approximation,\n",
    "                self.dummy_wns,\n",
    "                spectra.reshape(-1, 1)[:, 0],\n",
    "                p0=np.random.rand(2),\n",
    "            )[0]\n",
    "\n",
    "            new_slope = slope * (\n",
    "                    np.random.gamma(\n",
    "                        shape=1. / self.augment_slope_std,\n",
    "                        scale=self.augment_slope_std,\n",
    "                        size=1,\n",
    "                    )\n",
    "            )[0]\n",
    "            new_intercept = inter * (\n",
    "                1.0 + np.random.randn(1) * self.augment_intercept_std\n",
    "            )[0]\n",
    "            spectra += torch.tensor(\n",
    "                (new_slope - slope)\n",
    "            ) * self.dummy_wns + new_intercept - inter\n",
    "\n",
    "        factor = self.baseline_factor_bound * torch.rand(size=(1,))\n",
    "        offset = torch.rand(size=(1,)) * 2.0 * torch.pi\n",
    "        period = self.baseline_period_lower_bound + (\n",
    "            self.baseline_period_upper_bound - self.baseline_period_lower_bound\n",
    "        ) * torch.rand(size=(1,))\n",
    "        permutations = factor * torch.cos(\n",
    "            2.0 * torch.pi / period * self.dummy_wns + offset\n",
    "        )\n",
    "        return self.roll_spectrum(\n",
    "            spectra + permutations * spectra,\n",
    "            delta=random.randint(-self.rolling_bound, self.rolling_bound),\n",
    "        )\n",
    "\n",
    "    def roll_spectrum(self, spectra, delta):\n",
    "        num_spectra = spectra.shape[0]\n",
    "        rolled_spectra = np.roll(spectra, delta, axis=1)\n",
    "        if delta > 0:\n",
    "            rolled_spectra[:, :delta] = (\n",
    "                np.random.rand(num_spectra, delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta:(delta + 1)]\n",
    "        elif delta < 0:\n",
    "            rolled_spectra[:, delta:] = (\n",
    "                np.random.rand(num_spectra, -delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta - 1:delta]\n",
    "        return rolled_spectra\n",
    "\n",
    "    def combine_k_items(self, indices, weights):\n",
    "        return (\n",
    "            # spectra\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None, None], self.spectra[indices, :, :]),\n",
    "                dim=0,\n",
    "            ),\n",
    "            # concentrations\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None], self.concentrations[indices, :]),\n",
    "                dim=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.combine_spectra_range < 1e-12:\n",
    "            spectrum = self.spectra[idx]\n",
    "            spectrum = self.augment_spectra(spectrum)\n",
    "            return {\n",
    "                \"spectra\": spectrum,\n",
    "                \"concentrations\": self.concentrations[idx],\n",
    "                \"label_weight\": torch.tensor(1.0, dtype=self.dtype),\n",
    "            }\n",
    "        else:\n",
    "            if random.random() < self.original_dp_weight:\n",
    "                one_weight = 1.\n",
    "                label_weight = torch.tensor(1.0, dtype=self.dtype)\n",
    "            else:\n",
    "                one_weight = random.uniform(0.0, self.combine_spectra_range)\n",
    "                label_weight = self.augmentation_weight\n",
    "            weights = torch.tensor([one_weight, (1 - one_weight)])\n",
    "            # just pick two random indices\n",
    "            indices = random.choices(range(len(self)), k=2)\n",
    "\n",
    "            mixed_spectra, mixed_concentrations = self.combine_k_items(\n",
    "                indices=indices,\n",
    "                weights=weights,\n",
    "            )\n",
    "            mixed_spectra = self.augment_spectra(mixed_spectra)\n",
    "            return {\n",
    "                \"spectra\": mixed_spectra,\n",
    "                \"concentrations\": mixed_concentrations,\n",
    "                \"label_weight\": label_weight,\n",
    "            }\n",
    "\n",
    "\n",
    "config = {\n",
    "    'initial_cnn_channels': 32,\n",
    "    'cnn_channel_factor': 1.279574024454846,\n",
    "    'num_cnn_layers': 8,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'activation_function': 'ELU',\n",
    "    'fc_dropout': 0.10361700399831791,\n",
    "    'lr': 0.001,\n",
    "    'gamma': 0.9649606352621118,\n",
    "    'baseline_factor_bound': 0.748262317340447,\n",
    "    'baseline_period_lower_bound': 0.9703081695287203,\n",
    "    'baseline_period_span': 19.79744237606427,\n",
    "    'original_datapoint_weight': 0.4335003268130408,\n",
    "    'augment_slope_std': 0.08171025264382692,\n",
    "    'batch_size': 32,\n",
    "    'fc_dims': 226,\n",
    "    'rolling_bound': 2,\n",
    "    'num_blocks': 2,\n",
    "}\n",
    "\n",
    "def get_dataset(inputs, targets, config):\n",
    "    return SpectralDataset(\n",
    "        spectra=inputs[:, None, :],\n",
    "        concentrations=targets,\n",
    "        dtype=torch.float32,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=1.0,\n",
    "        baseline_factor_bound=config[\"baseline_factor_bound\"],\n",
    "        baseline_period_lower_bound=config[\"baseline_period_lower_bound\"],\n",
    "        baseline_period_upper_bound=(config[\"baseline_period_lower_bound\"] + config[\"baseline_period_span\"]),\n",
    "        augment_slope_std=config[\"augment_slope_std\"],\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=config[\"rolling_bound\"],\n",
    "        spectrum_rolling_sigma=0.01,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eb8f5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+5232)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds, train_batch_size, eval_batch_size):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1e76b4ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neptune'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_557/823722401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mneptune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msetup_neptune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mRESUME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neptune'"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "        if DROPOUT: neptune_run[\"h_parameters\"] = {\"dropout\": DROPOUT}\n",
    "        if DROP_PATH_RATE: neptune_run[\"h_parameters\"] = {\"drop_path_rate\": DROP_PATH_RATE}\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    \n",
    "    dim1 = r2_score(targets[:, 0], preds[:, 0])\n",
    "    dim2 = r2_score(targets[:, 1], preds[:, 1])\n",
    "    dim3 = r2_score(targets[:, 2], preds[:, 2])\n",
    "    \n",
    "    mean_r2 = (dim1 + dim2 + dim3) / 3\n",
    "    \n",
    "    return dim1, dim2, dim3, mean_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d23c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A residual block with two 1D convolutional layers.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.elu = nn.ELU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.elu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.elu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"A deeper ResNet-style 1D CNN for Raman spectra.\"\"\"\n",
    "    def __init__(self, dropout, input_channels=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.elu = nn.GELU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout), # Increased dropout for better regularization\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, stride=s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    device,\n",
    "    scaler, \n",
    "    scheduler,\n",
    "    train_dl,\n",
    "    eval_dl,\n",
    "    epochs,\n",
    "    checkpoint_name,\n",
    "    score=-float(\"inf\"),\n",
    "    neptune_run=None,\n",
    "    p=True,\n",
    "):  \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for inputs, targets in train_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            if neptune_run is not None:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            total_loss += loss.detach().cpu()\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        all_logits = torch.cat(all_logits)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "        total_loss = total_loss / len(train_dl)\n",
    "        \n",
    "        model.eval()\n",
    "        eval_total_loss = 0.0\n",
    "        eval_all_logits = []\n",
    "        eval_all_targets = []\n",
    "\n",
    "        for inputs, targets in eval_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                #with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "\n",
    "            eval_total_loss += loss.detach().cpu()\n",
    "            eval_all_logits.append(logits.detach().cpu())\n",
    "            eval_all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        eval_all_logits = torch.cat(eval_all_logits)\n",
    "        eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "        eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "        eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "        \n",
    "        if eval_r2 > score:\n",
    "            score = eval_r2\n",
    "            data = {\"state_dict\": model.state_dict()}\n",
    "            data[\"epoch\"] = epoch \n",
    "            data[\"score\"] = score\n",
    "            torch.save(data, f\"/kaggle/working/{checkpoint_name}\")\n",
    "        \n",
    "        if neptune_run is not None:\n",
    "            neptune_run[\"train/loss\"].append(total_loss)\n",
    "            neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "            neptune_run[\"train/r2\"].append(r2)\n",
    "            neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "            neptune_run[\"train/one\"].append(one)\n",
    "            neptune_run[\"train/two\"].append(two)\n",
    "            neptune_run[\"train/three\"].append(three)\n",
    "            neptune_run[\"eval/one\"].append(eval_one)\n",
    "            neptune_run[\"eval/two\"].append(eval_two)\n",
    "            neptune_run[\"eval/three\"].append(eval_three)\n",
    "            \n",
    "        if p and epoch % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, \"\n",
    "                f\"train/loss: {total_loss:.4f}, \"\n",
    "                f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "                f\"train/r2: {r2:.4f}, \"\n",
    "                f\"eval/r2: {eval_r2:.4f}, \"\n",
    "                f\"train/one: {one:.4f}, \"\n",
    "                f\"train/two: {two:.4f}, \"\n",
    "                f\"train/three: {three:.4f}, \"\n",
    "                f\"eval/one: {eval_one:.4f}, \"\n",
    "                f\"eval/two: {eval_two:.4f}, \"\n",
    "                f\"eval/three: {eval_three:.4f} \"\n",
    "            )\n",
    "            \n",
    "    if neptune_run is not None: neptune_run.stop()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings#; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "EPOCHS = 500\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "\n",
    "DROPOUT = 0.5\n",
    "DROP_PATH_RATE = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "mus, sigmas = [], []\n",
    "scores = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "splits = kfold.split(range(96))\n",
    "\n",
    "for fold, (train_idx, eval_idx) in enumerate(splits):\n",
    "    MODEL_NAME = f\"resnet.pretrain.fold.{fold}\"\n",
    "    checkpoint_name = f\"pretrain.fold.{fold}.pt\"\n",
    "    \n",
    "    \n",
    "\n",
    "    mu, sigma = get_stats(train_inputs, p=False, r=True)\n",
    "    train_inputs = zscore(train_inputs, mu, sigma)\n",
    "    eval_inputs = zscore(eval_inputs, mu, sigma)\n",
    "    mus.append(mu)\n",
    "    sigmas.append(sigma)\n",
    "    \n",
    "    train_ds = TensorDataset(train_inputs.float(), train_targets.float())\n",
    "    eval_ds = TensorDataset(eval_inputs.float(), eval_targets.float())\n",
    "    \n",
    "    BATCH_SIZE = len(train_ds)\n",
    "    train_dl, eval_dl = return_dls(train_ds, eval_ds, BATCH_SIZE, len(eval_ds))\n",
    "    \n",
    "    model = ResNet(input_channels=2, dropout=DROPOUT).to(device)\n",
    "    if fold == 0: print(get_model_size(model))\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    scheduler = get_scheduler(optimizer, train_dl, EPOCHS)\n",
    "    \n",
    "    score = train(\n",
    "            model, \n",
    "            optimizer, \n",
    "            device,\n",
    "            scaler,\n",
    "            scheduler,\n",
    "            train_dl, \n",
    "            eval_dl,\n",
    "            EPOCHS,\n",
    "            checkpoint_name,\n",
    "            neptune_run=setup_neptune(),\n",
    "        )\n",
    "    \n",
    "    scores.append(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
