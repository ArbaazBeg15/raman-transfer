{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = 1000\n",
    "\n",
    "def setup_reproducibility():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "setup_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:03.359753Z",
     "iopub.status.busy": "2025-08-13T05:38:03.359267Z",
     "iopub.status.idle": "2025-08-13T05:38:03.370622Z",
     "shell.execute_reply": "2025-08-13T05:38:03.369430Z",
     "shell.execute_reply.started": "2025-08-13T05:38:03.359680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:03.372207Z",
     "iopub.status.busy": "2025-08-13T05:38:03.371818Z",
     "iopub.status.idle": "2025-08-13T05:38:04.027661Z",
     "shell.execute_reply": "2025-08-13T05:38:04.026458Z",
     "shell.execute_reply.started": "2025-08-13T05:38:03.372171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(os.path.join(path, files[5]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:07.239423Z",
     "iopub.status.busy": "2025-08-13T05:38:07.238588Z",
     "iopub.status.idle": "2025-08-13T05:38:07.244976Z",
     "shell.execute_reply": "2025-08-13T05:38:07.243069Z",
     "shell.execute_reply.started": "2025-08-13T05:38:07.239387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_cols = df.columns[1:2049]\n",
    "target_cols = df.columns[2050:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:08.714355Z",
     "iopub.status.busy": "2025-08-13T05:38:08.713966Z",
     "iopub.status.idle": "2025-08-13T05:38:08.725976Z",
     "shell.execute_reply": "2025-08-13T05:38:08.724822Z",
     "shell.execute_reply.started": "2025-08-13T05:38:08.714324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "targets  = df[target_cols].dropna().to_numpy()\n",
    "targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:10.239867Z",
     "iopub.status.busy": "2025-08-13T05:38:10.239386Z",
     "iopub.status.idle": "2025-08-13T05:38:10.260483Z",
     "shell.execute_reply": "2025-08-13T05:38:10.259169Z",
     "shell.execute_reply.started": "2025-08-13T05:38:10.239832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df[input_cols]\n",
    "df['Unnamed: 1'] = df['Unnamed: 1'].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "df['Unnamed: 2048'] = df['Unnamed: 2048'].str.replace('[\\[\\]]', '', regex=True).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:10.799794Z",
     "iopub.status.busy": "2025-08-13T05:38:10.799353Z",
     "iopub.status.idle": "2025-08-13T05:38:11.002877Z",
     "shell.execute_reply": "2025-08-13T05:38:11.001763Z",
     "shell.execute_reply.started": "2025-08-13T05:38:10.799761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = []\n",
    "for i in range(0, len(df), 2):\n",
    "    row1 = df.iloc[i].to_numpy()\n",
    "    row2 = df.iloc[i+1].to_numpy()\n",
    "    i = np.concatenate([row1, row2])\n",
    "    inputs.append(i)\n",
    "    \n",
    "inputs = np.stack(inputs)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:13.410577Z",
     "iopub.status.busy": "2025-08-13T05:38:13.409546Z",
     "iopub.status.idle": "2025-08-13T05:38:14.157506Z",
     "shell.execute_reply": "2025-08-13T05:38:14.156575Z",
     "shell.execute_reply.started": "2025-08-13T05:38:13.410534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_inputs, eval_inputs, train_targets, eval_targets = train_test_split(\n",
    "        inputs,\n",
    "        targets,                      \n",
    "        test_size=0.2,\n",
    "        random_state=1000,\n",
    "        shuffle=True\n",
    ")\n",
    "\n",
    "train_inputs.shape, eval_inputs.shape, train_targets.shape, eval_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:15.602590Z",
     "iopub.status.busy": "2025-08-13T05:38:15.601992Z",
     "iopub.status.idle": "2025-08-13T05:38:15.631888Z",
     "shell.execute_reply": "2025-08-13T05:38:15.630554Z",
     "shell.execute_reply.started": "2025-08-13T05:38:15.602557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "eval_inputs = scaler.transform(eval_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs).float()\n",
    "train_targets = torch.tensor(train_targets).float()\n",
    "eval_inputs = torch.tensor(eval_inputs).float()\n",
    "eval_targets = torch.tensor(eval_targets).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(train_inputs, train_targets)\n",
    "eval_ds = TensorDataset(eval_inputs, eval_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+1)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=20,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    one = r2_score(targets[:, 0], preds[:, 0])\n",
    "    two = r2_score(targets[:, 1], preds[:, 1])\n",
    "    three = r2_score(targets[:, 2], preds[:, 2])\n",
    "    mean_r2 = (one + two + three) / 3\n",
    "    return one, two, three, mean_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP optimized for Raman spectroscopy concentration prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=4096, hidden_sizes=[128], \n",
    "                 output_size=3, dropout_rate=0.3, use_batch_norm=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        # Build the network layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            \n",
    "            # Batch normalization\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(nn.GELU())\n",
    "            \n",
    "            # Dropout (not on the last hidden layer)\n",
    "            if i < len(hidden_sizes) - 1:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        self.d = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier/Glorot initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.d(x)\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "MODEL_NAME = \"MLP.Baseline.AdamW\"\n",
    "EPOCHS = 4000\n",
    "BATCH_SIZE = 16\n",
    "WD = 1e-4\n",
    "LR = 1e-4\n",
    "DROPOUT = 0.3\n",
    "SCORE = float('-inf')\n",
    "LOG = False\n",
    "RESUME = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "model = Model(dropout_rate=DROPOUT).to(device)\n",
    "#model = nn.DataParallel(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "scaler = torch.GradScaler(device)\n",
    "train_dl, eval_dl = return_dls(train_ds, eval_ds)\n",
    "\n",
    "total_training_steps = len(train_dl) * EPOCHS\n",
    "warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "if LOG:\n",
    "    neptune_run = setup_neptune()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for inputs, targets in train_dl:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        \n",
    "        noise = torch.randn_like(inputs) * 0.1  # std=0.1\n",
    "        inputs = inputs + noise\n",
    "\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, targets)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.detach().cpu()\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "    \n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "\n",
    "    one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "    total_loss = total_loss / len(train_dl)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    eval_total_loss = 0.0\n",
    "    eval_all_logits = []\n",
    "    eval_all_targets = []\n",
    "\n",
    "    for inputs, targets in eval_dl:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "\n",
    "        eval_total_loss += loss.detach().cpu()\n",
    "        eval_all_logits.append(logits.detach().cpu())\n",
    "        eval_all_targets.append(targets.detach().cpu())\n",
    "    \n",
    "    eval_all_logits = torch.cat(eval_all_logits)\n",
    "    eval_all_targets = torch.cat(eval_all_targets)\n",
    "   \n",
    "    eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "    eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "    \n",
    "    if r2 > SCORE:\n",
    "        SCORE = r2\n",
    "        data = {\"state_dict\": model.state_dict()}\n",
    "        data[\"epoch\"] = epoch \n",
    "        data[\"score\"] = SCORE\n",
    "        torch.save(data, \"/kaggle/working/ckpt.pt\")\n",
    "    \n",
    "    if LOG:\n",
    "        neptune_run[\"train/loss\"].append(LOSS)\n",
    "        neptune_run[\"eval/loss\"].append(EVAL_LOSS)\n",
    "        neptune_run[\"train/auroc\"].append(auroc)\n",
    "        neptune_run[\"eval/auroc\"].append(eval_auroc)\n",
    "        \n",
    "    if True:\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"train/loss: {total_loss:.4f}, \"\n",
    "            f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "            f\"train/r2: {r2:.4f}, \"\n",
    "            f\"eval/r2: {eval_r2:.4f}, \"\n",
    "            f\"train/one: {one:.4f}, \"\n",
    "            f\"train/two: {two:.4f}, \"\n",
    "            f\"train/three: {three:.4f}, \"\n",
    "            f\"eval/one: {eval_one:.4f}, \"\n",
    "            f\"eval/two: {eval_two:.4f}, \"\n",
    "            f\"eval/three: {eval_three:.4f}, \"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:54.374610Z",
     "iopub.status.busy": "2025-08-13T05:38:54.374187Z",
     "iopub.status.idle": "2025-08-13T05:38:54.522345Z",
     "shell.execute_reply": "2025-08-13T05:38:54.520846Z",
     "shell.execute_reply.started": "2025-08-13T05:38:54.374578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#def load_test():    train/r2: 0.8097    eval/r2: -0.7274, t\n",
    "test = pd.read_csv(os.path.join(path, files[6]))\n",
    "\n",
    "row1 = test.columns[1:].to_numpy().copy()\n",
    "row1[-1] = \"5611\"\n",
    "row1 = row1.astype(np.float64)\n",
    "\n",
    "\n",
    "cols = test.columns[1:]\n",
    "test = test[cols]\n",
    "test[\" 5611]\"] = test[\" 5611]\"].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "test = test.to_numpy()\n",
    "\n",
    "test = np.insert(test, 0, row1, axis=0)\n",
    "test = test.reshape(-1, 2, 2048).reshape(96, 2*2048)\n",
    "test = test.astype(np.float32)\n",
    "test.shape, test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:56.788721Z",
     "iopub.status.busy": "2025-08-13T05:38:56.788348Z",
     "iopub.status.idle": "2025-08-13T05:38:56.808415Z",
     "shell.execute_reply": "2025-08-13T05:38:56.807319Z",
     "shell.execute_reply.started": "2025-08-13T05:38:56.788679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#def scale():\n",
    "test = scaler.transform(test)\n",
    "preds = xgb.predict(test)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:58.909362Z",
     "iopub.status.busy": "2025-08-13T05:38:58.908930Z",
     "iopub.status.idle": "2025-08-13T05:38:58.926823Z",
     "shell.execute_reply": "2025-08-13T05:38:58.925789Z",
     "shell.execute_reply.started": "2025-08-13T05:38:58.909334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#def prepare_test():\n",
    "column_names = ['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']\n",
    "preds_df = pd.DataFrame(preds, columns=column_names)\n",
    "preds_df.insert(0, 'ID', [i+1 for i in range(len(preds_df))])\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:39:07.934713Z",
     "iopub.status.busy": "2025-08-13T05:39:07.934305Z",
     "iopub.status.idle": "2025-08-13T05:39:07.955465Z",
     "shell.execute_reply": "2025-08-13T05:39:07.954454Z",
     "shell.execute_reply.started": "2025-08-13T05:39:07.934670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#def save_test():\n",
    "preds_df.to_csv(\"baseline.csv\", index=False)\n",
    "f = pd.read_csv(\"/kaggle/working/xgb_baseline.csv\")\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12807347,
     "sourceId": 105802,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
