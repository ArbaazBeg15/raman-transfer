{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -qU neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = 1000\n",
    "\n",
    "def setup_reproducibility():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "setup_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_stats(tensor, p=True, r=False):\n",
    "    mean, std = tensor.mean(), tensor.std()\n",
    "    min, max =  tensor.min(), tensor.max()\n",
    "    \n",
    "    if p: print(f\"Min: {min}, Max: {max}, Mean: {mean}, Std: {std}\")\n",
    "    \n",
    "    if r: return min, max, mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-6)\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:03.359753Z",
     "iopub.status.busy": "2025-08-13T05:38:03.359267Z",
     "iopub.status.idle": "2025-08-13T05:38:03.370622Z",
     "shell.execute_reply": "2025-08-13T05:38:03.369430Z",
     "shell.execute_reply.started": "2025-08-13T05:38:03.359680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = []\n",
    "train_targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:03.372207Z",
     "iopub.status.busy": "2025-08-13T05:38:03.371818Z",
     "iopub.status.idle": "2025-08-13T05:38:04.027661Z",
     "shell.execute_reply": "2025-08-13T05:38:04.026458Z",
     "shell.execute_reply.started": "2025-08-13T05:38:03.372171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "timegate = pd.read_csv(os.path.join(path, files[1]))\n",
    "\n",
    "timegate.drop(columns=\"fold_idx\", inplace=True)\n",
    "timegate.drop(columns=\"MSM_present\", inplace=True)\n",
    "timegate_inputs = timegate[timegate.columns[:-3]].to_numpy()\n",
    "timegate_targets = timegate[timegate.columns[-3:]].to_numpy()\n",
    "\n",
    "train_inputs.append(timegate_inputs)\n",
    "train_targets.append(timegate_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mettler_toledo = pd.read_csv(os.path.join(path, files[2]))\n",
    "\n",
    "mettler_toledo.drop(columns=\"fold_idx\", inplace=True)\n",
    "mettler_toledo.drop(columns=\"MSM_present\", inplace=True)\n",
    "mettler_toledo_inputs = mettler_toledo[mettler_toledo.columns[:-3]].to_numpy()\n",
    "mettler_toledo_targets = mettler_toledo[mettler_toledo.columns[-3:]].to_numpy()\n",
    "\n",
    "train_inputs.append(mettler_toledo_inputs)\n",
    "train_targets.append(mettler_toledo_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaiser = pd.read_csv(os.path.join(path, files[3]))\n",
    "\n",
    "kaiser.drop(columns=\"fold_idx\", inplace=True)\n",
    "kaiser.drop(columns=\"MSM_present\", inplace=True)\n",
    "kaiser_inputs = kaiser[kaiser.columns[:-3]].to_numpy()\n",
    "kaiser_targets = kaiser[kaiser.columns[-3:]].to_numpy()\n",
    "\n",
    "train_inputs.append(kaiser_inputs)\n",
    "train_targets.append(kaiser_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anton = pd.read_csv(os.path.join(path, files[4]))\n",
    "\n",
    "anton.drop(columns=\"fold_idx\", inplace=True)\n",
    "anton.drop(columns=\"MSM_present\", inplace=True)\n",
    "anton_inputs = anton[anton.columns[:-3]].to_numpy()\n",
    "anton_targets = anton[anton.columns[-3:]].to_numpy()\n",
    "\n",
    "train_inputs.append(anton_inputs)\n",
    "train_targets.append(anton_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tornado = pd.read_csv(os.path.join(path, files[7]))\n",
    "\n",
    "tornado.drop(columns=\"fold_idx\", inplace=True)\n",
    "tornado.drop(columns=\"MSM_present\", inplace=True)\n",
    "tornado_inputs = tornado[tornado.columns[:-3]].to_numpy()\n",
    "tornado_targets = tornado[tornado.columns[-3:]].to_numpy()\n",
    "\n",
    "train_inputs.append(tornado_inputs)\n",
    "train_targets.append(tornado_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(path, files[8])\n",
    "print(csv_path)\n",
    "tec5 = pd.read_csv(csv_path)\n",
    "\n",
    "tec5.drop(columns=\"fold_idx\", inplace=True)\n",
    "tec5.drop(columns=\"MSM_present\", inplace=True)\n",
    "tec5_inputs = tec5[tec5.columns[:-3]].to_numpy()\n",
    "tec5_targets = tec5[tec5.columns[-3:]].to_numpy()\n",
    "\n",
    "train_inputs.append(tec5_inputs)\n",
    "train_targets.append(tec5_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(path, files[9])\n",
    "print(csv_path)\n",
    "metrohm = pd.read_csv(csv_path)\n",
    "\n",
    "metrohm.drop(columns=\"fold_idx\", inplace=True)\n",
    "metrohm.drop(columns=\"MSM_present\", inplace=True)\n",
    "metrohm_inputs = metrohm[metrohm.columns[:-3]].to_numpy()\n",
    "metrohm_targets = metrohm[metrohm.columns[-3:]].to_numpy()\n",
    "\n",
    "train_inputs.append(metrohm_inputs)\n",
    "train_targets.append(metrohm_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(path, files[10])\n",
    "print(csv_path)\n",
    "anton785 = pd.read_csv(csv_path)\n",
    "\n",
    "anton785.drop(columns=\"fold_idx\", inplace=True)\n",
    "anton785.drop(columns=\"MSM_present\", inplace=True)\n",
    "anton785_inputs = anton785[anton785.columns[:-3]].to_numpy()\n",
    "anton785_targets = anton785[anton785.columns[-3:]].to_numpy()\n",
    "\n",
    "train_inputs.append(anton785_inputs)\n",
    "train_targets.append(anton785_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "for i in range(len(train_inputs)):\n",
    "    x = train_inputs[i]\n",
    "    x = torch.tensor(x).unsqueeze(0)\n",
    "    x = F.interpolate(x, size=2048, mode=\"nearest-exact\")\n",
    "    train_inputs[i] = x.squeeze()\n",
    "    \n",
    "train_inputs = torch.cat(train_inputs)\n",
    "train_targets = [torch.tensor(t) for t in train_targets]\n",
    "train_targets = torch.cat(train_targets)\n",
    "\n",
    "train_inputs.shape, train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = torch.quantile(train_inputs, 0.02, interpolation=\"linear\")\n",
    "high = torch.quantile(train_inputs, 0.98, interpolation=\"linear\")\n",
    "train_inputs = train_inputs.clamp(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, mean, std = return_stats(train_inputs, r=True)\n",
    "train_inputs = zscore(train_inputs, mean, std)\n",
    "return_stats(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:07.239423Z",
     "iopub.status.busy": "2025-08-13T05:38:07.238588Z",
     "iopub.status.idle": "2025-08-13T05:38:07.244976Z",
     "shell.execute_reply": "2025-08-13T05:38:07.243069Z",
     "shell.execute_reply.started": "2025-08-13T05:38:07.239387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "csv_path = os.path.join(path, files[5])\n",
    "print(csv_path)\n",
    "eval_set = pd.read_csv(csv_path)\n",
    "\n",
    "eval_input_cols = eval_set.columns[1:2049]\n",
    "eval_target_cols = eval_set.columns[2050:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:08.714355Z",
     "iopub.status.busy": "2025-08-13T05:38:08.713966Z",
     "iopub.status.idle": "2025-08-13T05:38:08.725976Z",
     "shell.execute_reply": "2025-08-13T05:38:08.724822Z",
     "shell.execute_reply.started": "2025-08-13T05:38:08.714324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_targets  = eval_set[eval_target_cols].dropna().to_numpy()\n",
    "eval_targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:10.239867Z",
     "iopub.status.busy": "2025-08-13T05:38:10.239386Z",
     "iopub.status.idle": "2025-08-13T05:38:10.260483Z",
     "shell.execute_reply": "2025-08-13T05:38:10.259169Z",
     "shell.execute_reply.started": "2025-08-13T05:38:10.239832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_set = eval_set[eval_input_cols]\n",
    "eval_set['Unnamed: 1'] = eval_set['Unnamed: 1'].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "eval_set['Unnamed: 2048'] = eval_set['Unnamed: 2048'].str.replace('[\\[\\]]', '', regex=True).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_inputs = eval_set.to_numpy().reshape(-1, 2, 2048)\n",
    "eval_inputs = eval_inputs.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_inputs = torch.tensor(eval_inputs)\n",
    "eval_targets = torch.tensor(eval_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_stats(eval_inputs)\n",
    "eval_inputs = eval_inputs.clamp(low, high)\n",
    "return_stats(eval_inputs)\n",
    "eval_inputs = zscore(eval_inputs, mean, std)\n",
    "return_stats(eval_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:13.410577Z",
     "iopub.status.busy": "2025-08-13T05:38:13.409546Z",
     "iopub.status.idle": "2025-08-13T05:38:14.157506Z",
     "shell.execute_reply": "2025-08-13T05:38:14.156575Z",
     "shell.execute_reply.started": "2025-08-13T05:38:13.410534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split():\n",
    "        train_inputs, eval_inputs, train_targets, eval_targets = train_test_split(\n",
    "                inputs,\n",
    "                targets,                      \n",
    "                test_size=0.2,\n",
    "                random_state=1000,\n",
    "                shuffle=True\n",
    "        )\n",
    "\n",
    "        train_inputs.shape, eval_inputs.shape, train_targets.shape, eval_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_inputs.float().unsqueeze(1)\n",
    "train_targets = train_targets.float()\n",
    "eval_inputs = eval_inputs.float().unsqueeze(1)\n",
    "eval_targets = eval_targets.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(train_inputs, train_targets)\n",
    "eval_ds = TensorDataset(eval_inputs, eval_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+1)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=96,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "resnet for 1-d signal data, pytorch version\n",
    "Shenda Hong, Oct 2019\n",
    "https://github.com/hsd1503/resnet1d\n",
    "\n",
    "Modified by Hiroshi Yoshihara\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MyConv1dPadSame(nn.Module):\n",
    "    \"\"\"\n",
    "    extend nn.Conv1d to support SAME padding\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1):\n",
    "        super(MyConv1dPadSame, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.conv = torch.nn.Conv1d(\n",
    "            in_channels=self.in_channels, \n",
    "            out_channels=self.out_channels, \n",
    "            kernel_size=self.kernel_size, \n",
    "            stride=self.stride, \n",
    "            groups=self.groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        net = x\n",
    "        \n",
    "        # compute pad shape\n",
    "        in_dim = net.shape[-1]\n",
    "        out_dim = (in_dim + self.stride - 1) // self.stride\n",
    "        p = max(0, (out_dim - 1) * self.stride + self.kernel_size - in_dim)\n",
    "        pad_left = p // 2\n",
    "        pad_right = p - pad_left\n",
    "        net = F.pad(net, (pad_left, pad_right), \"constant\", 0)\n",
    "        \n",
    "        net = self.conv(net)\n",
    "\n",
    "        return net\n",
    "\n",
    "      \n",
    "class MyMaxPool1dPadSame(nn.Module):\n",
    "    \"\"\"\n",
    "    extend nn.MaxPool1d to support SAME padding\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(MyMaxPool1dPadSame, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = 1\n",
    "        self.max_pool = torch.nn.MaxPool1d(kernel_size=self.kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        net = x\n",
    "        \n",
    "        # compute pad shape\n",
    "        in_dim = net.shape[-1]\n",
    "        out_dim = (in_dim + self.stride - 1) // self.stride\n",
    "        p = max(0, (out_dim - 1) * self.stride + self.kernel_size - in_dim)\n",
    "        pad_left = p // 2\n",
    "        pad_right = p - pad_left\n",
    "        net = F.pad(net, (pad_left, pad_right), \"constant\", 0)\n",
    "        \n",
    "        net = self.max_pool(net)\n",
    "        \n",
    "        return net\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Basic Block\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size, \n",
    "                 stride, \n",
    "                 groups, \n",
    "                 downsample, \n",
    "                 use_bn, \n",
    "                 dropout=0.2, \n",
    "                 activation=nn.ReLU,\n",
    "                 is_first_block=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.downsample = downsample\n",
    "        if self.downsample:\n",
    "            self.stride = stride\n",
    "        else:\n",
    "            self.stride = 1\n",
    "        self.is_first_block = is_first_block\n",
    "        self.use_bn = use_bn\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "\n",
    "        # the first conv\n",
    "        self.bn1 = nn.BatchNorm1d(in_channels)\n",
    "        self.relu1 = self.activation(inplace=True)\n",
    "        self.do1 = nn.Dropout(p=self.dropout)\n",
    "        self.conv1 = MyConv1dPadSame(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=self.stride,\n",
    "            groups=self.groups)\n",
    "\n",
    "        # the second conv\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu2 = self.activation(inplace=True)\n",
    "        self.do2 = nn.Dropout(p=self.dropout)\n",
    "        self.conv2 = MyConv1dPadSame(\n",
    "            in_channels=out_channels, \n",
    "            out_channels=out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=1,\n",
    "            groups=self.groups)\n",
    "                \n",
    "        self.max_pool = MyMaxPool1dPadSame(kernel_size=self.stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        # the first conv\n",
    "        out = x\n",
    "        if not self.is_first_block:\n",
    "            if self.use_bn:\n",
    "                out = self.bn1(out)\n",
    "            out = self.relu1(out)\n",
    "            out = self.do1(out)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        # the second conv\n",
    "        if self.use_bn:\n",
    "            out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.do2(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        # if downsample, also downsample identity\n",
    "        if self.downsample:\n",
    "            identity = self.max_pool(identity)\n",
    "            \n",
    "        # if expand channel, also pad zeros to identity\n",
    "        if self.out_channels != self.in_channels:\n",
    "            identity = identity.transpose(-1,-2)\n",
    "            ch1 = (self.out_channels-self.in_channels)//2\n",
    "            ch2 = self.out_channels-self.in_channels-ch1\n",
    "            identity = F.pad(identity, (ch1, ch2), \"constant\", 0)\n",
    "            identity = identity.transpose(-1,-2)\n",
    "        \n",
    "        # shortcut\n",
    "        out += identity\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet1d(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "        X: (n_samples, n_channel, n_length)\n",
    "        Y: (n_samples)\n",
    "        \n",
    "    Output:\n",
    "        out: (n_samples)\n",
    "        \n",
    "    Pararmetes:\n",
    "        in_channels: dim of input, the same as n_channel\n",
    "        base_filters: number of filters in the first several Conv layer, it will double at every 4 layers\n",
    "        kernel_size: width of kernel\n",
    "        stride: stride of kernel moving\n",
    "        groups: set larget to 1 as ResNeXt\n",
    "        n_block: number of blocks\n",
    "        n_classes: number of classes\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 base_filters, \n",
    "                 kernel_size, \n",
    "                 stride=2, \n",
    "                 groups=32, \n",
    "                 n_block=16, \n",
    "                 n_classes=1, \n",
    "                 downsample_gap=None, \n",
    "                 increasefilter_gap=None, \n",
    "                 use_bn=True, \n",
    "                 dropout=0.2, \n",
    "                 activation=nn.ReLU,\n",
    "                 final_act=True,\n",
    "                 reinit=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_block = n_block\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.use_bn = use_bn\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.final_act = final_act\n",
    "\n",
    "        if downsample_gap is None:\n",
    "            downsample_gap = self.n_block // 8\n",
    "        self.downsample_gap = downsample_gap # 2 for base model\n",
    "        if increasefilter_gap is None:\n",
    "            increasefilter_gap = self.n_block // 4\n",
    "        self.increasefilter_gap = increasefilter_gap # 4 for base model\n",
    "\n",
    "        # first block\n",
    "        self.first_block_conv = MyConv1dPadSame(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=base_filters, \n",
    "            kernel_size=self.kernel_size, \n",
    "            stride=1)\n",
    "        self.first_block_bn = nn.BatchNorm1d(base_filters)\n",
    "        self.first_block_relu = self.activation(inplace=True)\n",
    "        out_channels = base_filters\n",
    "                \n",
    "        # residual blocks\n",
    "        self.basicblock_list = nn.ModuleList()\n",
    "        for i_block in range(self.n_block):\n",
    "            # is_first_block\n",
    "            if i_block == 0:\n",
    "                is_first_block = True\n",
    "            else:\n",
    "                is_first_block = False\n",
    "            # downsample at every self.downsample_gap blocks\n",
    "            if i_block % self.downsample_gap == 1:\n",
    "                downsample = True\n",
    "            else:\n",
    "                downsample = False\n",
    "            # in_channels and out_channels\n",
    "            if is_first_block:\n",
    "                in_channels = base_filters\n",
    "                out_channels = in_channels\n",
    "            else:\n",
    "                # increase filters at every self.increasefilter_gap blocks\n",
    "                in_channels = int(base_filters*2**((i_block-1)//self.increasefilter_gap))\n",
    "                if (i_block % self.increasefilter_gap == 0) and (i_block != 0):\n",
    "                    out_channels = in_channels * 2\n",
    "                else:\n",
    "                    out_channels = in_channels\n",
    "            \n",
    "            tmp_block = BasicBlock(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                kernel_size=self.kernel_size, \n",
    "                stride = self.stride, \n",
    "                groups = self.groups, \n",
    "                downsample=downsample, \n",
    "                use_bn = self.use_bn,  \n",
    "                dropout = self.dropout, \n",
    "                activation = self.activation,\n",
    "                is_first_block=is_first_block)\n",
    "            self.basicblock_list.append(tmp_block)\n",
    "\n",
    "        # final prediction\n",
    "        self.final_bn = nn.BatchNorm1d(out_channels)\n",
    "        self.final_relu = self.activation(inplace=True)\n",
    "        self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dense = nn.Linear(out_channels, n_classes)\n",
    "\n",
    "        # init \n",
    "        if reinit:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv1d):\n",
    "                    nn.init.kaiming_normal_(m.weight)\n",
    "                elif isinstance(m, nn.BatchNorm1d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        out = self.first_block_conv(x)\n",
    "        if self.use_bn:\n",
    "            out = self.first_block_bn(out)\n",
    "        out = self.first_block_relu(out)\n",
    "        \n",
    "        # residual blocks, every block has two conv\n",
    "        for i_block in range(self.n_block):\n",
    "            net = self.basicblock_list[i_block]\n",
    "            out = net(out)\n",
    "        \n",
    "        # final prediction\n",
    "        if self.final_act:\n",
    "            if self.use_bn:\n",
    "                out = self.final_bn(out)\n",
    "            out = self.final_relu(out)\n",
    "\n",
    "        # out = out.mean(-1)\n",
    "        out = self.final_pool(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.forward_features(x)\n",
    "        out = out.squeeze(-1)\n",
    "        out = self.dense(out)\n",
    "        return out    \n",
    "\n",
    "    def reset_classifier(self, **kwargs):\n",
    "        self.dense = nn.Identity()\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    one = r2_score(targets[:, 0], preds[:, 0])\n",
    "    two = r2_score(targets[:, 1], preds[:, 1])\n",
    "    three = r2_score(targets[:, 2], preds[:, 2])\n",
    "    mean_r2 = (one + two + three) / 3\n",
    "    return one, two, three, mean_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP optimized for Raman spectroscopy concentration prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=train_inputs.shape[1], hidden_sizes=[128], \n",
    "                 output_size=3, dropout_rate=0.3, use_batch_norm=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        # Build the network layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            \n",
    "            # Batch normalization\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(nn.GELU())\n",
    "            \n",
    "            # Dropout (not on the last hidden layer)\n",
    "            if i < len(hidden_sizes) - 1:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        self.d = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier/Glorot initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.d(x)\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "MODEL_NAME = \"ResNet.Kernel3\"\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 17\n",
    "WD = 1e-4\n",
    "LR = 1e-4\n",
    "DROPOUT = 0.3\n",
    "SCORE = float('-inf')\n",
    "LOG = True\n",
    "RESUME = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "#model = Model(hidden_sizes=[512, 512, 512, 512, 512, 512], dropout_rate=DROPOUT).to(device)\n",
    "model = ResNet1d(1, base_filters=64, kernel_size=3, n_classes=3).to(device)\n",
    "get_model_size(model)\n",
    "#model = nn.DataParallel(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "scaler = torch.amp.GradScaler(device)\n",
    "train_dl, eval_dl = return_dls(train_ds, eval_ds)\n",
    "\n",
    "total_training_steps = len(train_dl) * EPOCHS\n",
    "warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "if LOG:\n",
    "    neptune_run = setup_neptune()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for inputs, targets in train_dl:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        \n",
    "        #if random.random() < 0.7:\n",
    "        #    noise = torch.randn_like(inputs) * 0.1  # std=0.1\n",
    "        #    inputs = inputs + noise\n",
    "\n",
    "        with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, targets)\n",
    "        \n",
    "        if LOG:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "        scheduler.step()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.detach().cpu()\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "    \n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "\n",
    "    one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "    total_loss = total_loss / len(train_dl)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    eval_total_loss = 0.0\n",
    "    eval_all_logits = []\n",
    "    eval_all_targets = []\n",
    "\n",
    "    for inputs, targets in eval_dl:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "\n",
    "        eval_total_loss += loss.detach().cpu()\n",
    "        eval_all_logits.append(logits.detach().cpu())\n",
    "        eval_all_targets.append(targets.detach().cpu())\n",
    "    \n",
    "    eval_all_logits = torch.cat(eval_all_logits)\n",
    "    eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "    eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "    eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "    \n",
    "    if eval_r2 > SCORE:\n",
    "        SCORE = eval_r2\n",
    "        data = {\"state_dict\": model.state_dict()}\n",
    "        data[\"epoch\"] = epoch \n",
    "        data[\"score\"] = SCORE\n",
    "        torch.save(data, \"/kaggle/working/ckpt.pt\")\n",
    "    \n",
    "    if LOG:\n",
    "        neptune_run[\"train/loss\"].append(total_loss)\n",
    "        neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "        neptune_run[\"train/r2\"].append(r2)\n",
    "        neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "        neptune_run[\"train/one\"].append(one)\n",
    "        neptune_run[\"train/two\"].append(two)\n",
    "        neptune_run[\"train/three\"].append(three)\n",
    "        neptune_run[\"eval/one\"].append(eval_one)\n",
    "        neptune_run[\"eval/two\"].append(eval_two)\n",
    "        neptune_run[\"eval/three\"].append(eval_three)\n",
    "        \n",
    "    if True:\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"train/loss: {total_loss:.4f}, \"\n",
    "            f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "            f\"train/r2: {r2:.4f}, \"\n",
    "            f\"eval/r2: {eval_r2:.4f}, \"\n",
    "            f\"train/one: {one:.4f}, \"\n",
    "            f\"train/two: {two:.4f}, \"\n",
    "            f\"train/three: {three:.4f}, \"\n",
    "            f\"eval/one: {eval_one:.4f}, \"\n",
    "            f\"eval/two: {eval_two:.4f}, \"\n",
    "            f\"eval/three: {eval_three:.4f} \"\n",
    "        )\n",
    "        \n",
    "if LOG:\n",
    "    neptune_run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt_path = \"/kaggle/working/ckpt.pt\"\n",
    "ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "ckpt[\"epoch\"], ckpt[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:54.374610Z",
     "iopub.status.busy": "2025-08-13T05:38:54.374187Z",
     "iopub.status.idle": "2025-08-13T05:38:54.522345Z",
     "shell.execute_reply": "2025-08-13T05:38:54.520846Z",
     "shell.execute_reply.started": "2025-08-13T05:38:54.374578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#def load_test():    \n",
    "test = pd.read_csv(os.path.join(path, files[6]))\n",
    "\n",
    "row1 = test.columns[1:].to_numpy().copy()\n",
    "row1[-1] = \"5611\"\n",
    "row1 = row1.astype(np.float64)\n",
    "\n",
    "\n",
    "cols = test.columns[1:]\n",
    "test = test[cols]\n",
    "test[\" 5611]\"] = test[\" 5611]\"].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "test = test.to_numpy()\n",
    "\n",
    "test = np.insert(test, 0, row1, axis=0)\n",
    "test = test.reshape(-1, 2, 2048).mean(axis=1)\n",
    "\n",
    "#test_features = []\n",
    "#for i in test:\n",
    "#    f = extract_features(i)\n",
    "#    \n",
    "#    values = []\n",
    "#    for v in f.values():\n",
    "#        values.append(v)\n",
    "# \n",
    "#    values = np.stack(values).ravel()\n",
    " #   test_features.append(values)\n",
    "\n",
    "#test_features = np.stack(test_features)\n",
    "#print(test_features.shape)\n",
    "\n",
    "#test = test_features\n",
    "return_stats(test)\n",
    "test = torch.tensor(test)\n",
    "test = test.clamp(low, high)\n",
    "return_stats(test)\n",
    "test = zscore(test, mean, std).float()\n",
    "test.shape, test.dtype, return_stats(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.unsqueeze(1)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = ResNet1d(1, base_filters=64, kernel_size=3, n_classes=3).to(device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = model(test.cuda())\n",
    "\n",
    "preds = preds.cpu().detach().numpy()\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:58.909362Z",
     "iopub.status.busy": "2025-08-13T05:38:58.908930Z",
     "iopub.status.idle": "2025-08-13T05:38:58.926823Z",
     "shell.execute_reply": "2025-08-13T05:38:58.925789Z",
     "shell.execute_reply.started": "2025-08-13T05:38:58.909334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#def prepare_test():\n",
    "column_names = ['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']\n",
    "preds_df = pd.DataFrame(preds, columns=column_names)\n",
    "preds_df.insert(0, 'ID', [i+1 for i in range(len(preds_df))])\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:39:07.934713Z",
     "iopub.status.busy": "2025-08-13T05:39:07.934305Z",
     "iopub.status.idle": "2025-08-13T05:39:07.955465Z",
     "shell.execute_reply": "2025-08-13T05:39:07.954454Z",
     "shell.execute_reply.started": "2025-08-13T05:39:07.934670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#def save_test():\n",
    "name = \"ResNet.-0.1588.csv\"\n",
    "preds_df.to_csv(name, index=False)\n",
    "f = pd.read_csv(f\"/kaggle/working/{name}\")\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12807347,
     "sourceId": 105802,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
