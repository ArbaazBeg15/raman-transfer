{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -qU neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = 1000\n",
    "\n",
    "def setup_reproducibility():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "setup_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_stats(tensor, p=True, r=False):\n",
    "    mean, std = tensor.mean(), tensor.std()\n",
    "    min, max =  tensor.min(), tensor.max()\n",
    "    \n",
    "    if p: print(f\"Min: {min}, Max: {max}, Mean: {mean}, Std: {std}\")\n",
    "    \n",
    "    if r: return min, max, mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-6)\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:03.359753Z",
     "iopub.status.busy": "2025-08-13T05:38:03.359267Z",
     "iopub.status.idle": "2025-08-13T05:38:03.370622Z",
     "shell.execute_reply": "2025-08-13T05:38:03.369430Z",
     "shell.execute_reply.started": "2025-08-13T05:38:03.359680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_transfer_data():\n",
    "    csv_path = os.path.join(path, files[10])\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    input_cols = df.columns[1:2049]\n",
    "    target_cols = df.columns[2050:]\n",
    "\n",
    "    targets  = df[target_cols].dropna().to_numpy()\n",
    "\n",
    "    df = df[input_cols]\n",
    "    df['Unnamed: 1'] = df['Unnamed: 1'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "    df['Unnamed: 2048'] = df['Unnamed: 2048'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "\n",
    "    inputs = df.to_numpy().reshape(-1, 2, 2048)\n",
    "    inputs = inputs.mean(axis=1)\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_all_datasets():\n",
    "    train_inputs = []\n",
    "    train_targets = []\n",
    "    \n",
    "    timegate = pd.read_csv(os.path.join(path, files[1]))\n",
    "\n",
    "    timegate.drop(columns=\"fold_idx\", inplace=True)\n",
    "    timegate.drop(columns=\"MSM_present\", inplace=True)\n",
    "    timegate_inputs = timegate[timegate.columns[:-3]].to_numpy()\n",
    "    timegate_targets = timegate[timegate.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(timegate_inputs)\n",
    "    train_targets.append(timegate_targets)\n",
    "    \n",
    "    mettler_toledo = pd.read_csv(os.path.join(path, files[2]))\n",
    "\n",
    "    mettler_toledo.drop(columns=\"fold_idx\", inplace=True)\n",
    "    mettler_toledo.drop(columns=\"MSM_present\", inplace=True)\n",
    "    mettler_toledo_inputs = mettler_toledo[mettler_toledo.columns[:-3]].to_numpy()\n",
    "    mettler_toledo_targets = mettler_toledo[mettler_toledo.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(mettler_toledo_inputs)\n",
    "    train_targets.append(mettler_toledo_targets)\n",
    "    \n",
    "    kaiser = pd.read_csv(os.path.join(path, files[3]))\n",
    "\n",
    "    kaiser.drop(columns=\"fold_idx\", inplace=True)\n",
    "    kaiser.drop(columns=\"MSM_present\", inplace=True)\n",
    "    kaiser_inputs = kaiser[kaiser.columns[:-3]].to_numpy()\n",
    "    kaiser_targets = kaiser[kaiser.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(kaiser_inputs)\n",
    "    train_targets.append(kaiser_targets)\n",
    "    \n",
    "    anton = pd.read_csv(os.path.join(path, files[4]))\n",
    "\n",
    "    anton.drop(columns=\"fold_idx\", inplace=True)\n",
    "    anton.drop(columns=\"MSM_present\", inplace=True)\n",
    "    anton_inputs = anton[anton.columns[:-3]].to_numpy()\n",
    "    anton_targets = anton[anton.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(anton_inputs)\n",
    "    train_targets.append(anton_targets)\n",
    "    \n",
    "    tornado = pd.read_csv(os.path.join(path, files[7]))\n",
    "\n",
    "    tornado.drop(columns=\"fold_idx\", inplace=True)\n",
    "    tornado.drop(columns=\"MSM_present\", inplace=True)\n",
    "    tornado_inputs = tornado[tornado.columns[:-3]].to_numpy()\n",
    "    tornado_targets = tornado[tornado.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(tornado_inputs)\n",
    "    train_targets.append(tornado_targets)\n",
    "    \n",
    "    csv_path = os.path.join(path, files[8])\n",
    "    print(csv_path)\n",
    "    tec5 = pd.read_csv(csv_path)\n",
    "\n",
    "    tec5.drop(columns=\"fold_idx\", inplace=True)\n",
    "    tec5.drop(columns=\"MSM_present\", inplace=True)\n",
    "    tec5_inputs = tec5[tec5.columns[:-3]].to_numpy()\n",
    "    tec5_targets = tec5[tec5.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(tec5_inputs)\n",
    "    train_targets.append(tec5_targets)\n",
    "    \n",
    "    csv_path = os.path.join(path, files[9])\n",
    "    print(csv_path)\n",
    "    metrohm = pd.read_csv(csv_path)\n",
    "\n",
    "    metrohm.drop(columns=\"fold_idx\", inplace=True)\n",
    "    metrohm.drop(columns=\"MSM_present\", inplace=True)\n",
    "    metrohm_inputs = metrohm[metrohm.columns[:-3]].to_numpy()\n",
    "    metrohm_targets = metrohm[metrohm.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(metrohm_inputs)\n",
    "    train_targets.append(metrohm_targets)\n",
    "    \n",
    "    csv_path = os.path.join(path, files[10])\n",
    "    print(csv_path)\n",
    "    anton785 = pd.read_csv(csv_path)\n",
    "\n",
    "    anton785.drop(columns=\"fold_idx\", inplace=True)\n",
    "    anton785.drop(columns=\"MSM_present\", inplace=True)\n",
    "    anton785_inputs = anton785[anton785.columns[:-3]].to_numpy()\n",
    "    anton785_targets = anton785[anton785.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(anton785_inputs)\n",
    "    train_targets.append(anton785_targets)\n",
    "    \n",
    "    return train_inputs, train_targets\n",
    "\n",
    "train_inputs, train_targets = load_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def foo(train_inputs, train_targets):\n",
    "    for i in range(len(train_inputs)):\n",
    "        x = train_inputs[i]\n",
    "        x = torch.tensor(x).unsqueeze(0)\n",
    "        x = F.interpolate(x, size=2048, mode=\"nearest-exact\")\n",
    "        train_inputs[i] = x.squeeze()\n",
    "        \n",
    "    train_inputs = torch.cat(train_inputs)\n",
    "    train_targets = [torch.tensor(t) for t in train_targets]\n",
    "    train_targets = torch.cat(train_targets)\n",
    "\n",
    "    return train_inputs, train_targets\n",
    "    \n",
    "train_inputs, train_targets = foo(train_inputs, train_targets)\n",
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def get_advanced_spectra_features(X):\n",
    "    \"\"\"Create multi-channel features from spectra: raw, 1st derivative, 2nd derivative.\"\"\"\n",
    "    X_processed = np.zeros_like(X)\n",
    "    # Baseline correction and SNV\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        poly = np.polyfit(np.arange(X.shape[1]), X[i], 3)\n",
    "        baseline = np.polyval(poly, np.arange(X.shape[1]))\n",
    "        corrected_spec = X[i] - baseline\n",
    "        X_processed[i] = (corrected_spec - corrected_spec.mean()) / (corrected_spec.std() + 1e-8)\n",
    "\n",
    "    # Calculate derivatives\n",
    "    deriv1 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=1, axis=1)\n",
    "    deriv2 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=2, axis=1)\n",
    "\n",
    "    # Stack as channels\n",
    "    return np.stack([X_processed, deriv1, deriv2], axis=1)\n",
    "\n",
    "#train_inputs = get_advanced_spectra_features(train_inputs.numpy())\n",
    "#train_inputs = torch.tensor(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low = torch.quantile(train_inputs, 0.02, interpolation=\"linear\")\n",
    "#high = torch.quantile(train_inputs, 0.98, interpolation=\"linear\")\n",
    "#train_inputs = train_inputs.clamp(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_inputs, eval_inputs, train_targets, eval_targets = train_test_split(\n",
    "    train_inputs,\n",
    "    train_targets,\n",
    "    shuffle=True,\n",
    "    test_size=0.2,\n",
    "    random_state=1000\n",
    ")\n",
    "\n",
    "train_inputs.shape, eval_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, mean, std = return_stats(train_inputs, r=True)\n",
    "train_inputs = zscore(train_inputs, mean, std)\n",
    "return_stats(train_inputs), train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_stats(eval_inputs)\n",
    "eval_inputs = zscore(eval_inputs, mean, std)\n",
    "return_stats(eval_inputs), eval_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:10.239867Z",
     "iopub.status.busy": "2025-08-13T05:38:10.239386Z",
     "iopub.status.idle": "2025-08-13T05:38:10.260483Z",
     "shell.execute_reply": "2025-08-13T05:38:10.259169Z",
     "shell.execute_reply.started": "2025-08-13T05:38:10.239832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "csv_path = os.path.join(path, files[5])\n",
    "print(csv_path)\n",
    "eval_set = pd.read_csv(csv_path)\n",
    "\n",
    "eval_input_cols = eval_set.columns[1:2049]\n",
    "eval_target_cols = eval_set.columns[2050:]\n",
    "\n",
    "eval_targets  = eval_set[eval_target_cols].dropna().to_numpy()\n",
    "eval_targets[:5]\n",
    "\n",
    "eval_set = eval_set[eval_input_cols]\n",
    "eval_set['Unnamed: 1'] = eval_set['Unnamed: 1'].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "eval_set['Unnamed: 2048'] = eval_set['Unnamed: 2048'].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "\n",
    "eval_inputs = eval_set.to_numpy().reshape(-1, 2, 2048)\n",
    "eval_inputs = eval_inputs.mean(axis=1)\n",
    "eval_inputs = get_advanced_spectra_features(eval_inputs)\n",
    "eval_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_inputs, eval_inputs, train_targets, eval_targets = train_test_split(\n",
    "    eval_inputs,\n",
    "    eval_targets,\n",
    "    shuffle=True,\n",
    "    test_size=0.2,\n",
    "    random_state=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs).float()\n",
    "train_targets = torch.tensor(train_targets).float()\n",
    "eval_inputs = torch.tensor(eval_inputs).float()\n",
    "eval_targets = torch.tensor(eval_targets).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, mean, std = return_stats(train_inputs, r=True)\n",
    "train_inputs = zscore(train_inputs, mean, std)\n",
    "return_stats(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_stats(eval_inputs)\n",
    "eval_inputs = zscore(eval_inputs, mean, std)\n",
    "return_stats(eval_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs =  train_inputs.reshape(-1, 3 * 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_inputs = eval_inputs.reshape(-1, 3 * 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_inputs.float().numpy()\n",
    "train_targets = train_targets.float().numpy()\n",
    "eval_inputs = eval_inputs.float().numpy()\n",
    "eval_targets = eval_targets.float().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "xgb_base = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=150,      # fewer rounds because no early stopping; try 200-1000 depending on overfit\n",
    "    learning_rate=0.06,\n",
    "    max_depth=3,           # shallower trees for small data\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    #reg_alpha=0.000001,        # L1 regularization\n",
    "    reg_lambda=0.00001,        # L2 regularization\n",
    "    random_state=SEED,\n",
    "    verbosity=0            # quiet\n",
    ")\n",
    "\n",
    "# Wrap the XGB regressor so sklearn treats multi-targets automatically\n",
    "multi_xgb = MultiOutputRegressor(xgb_base, n_jobs=-1)\n",
    "multi_xgb.fit(train_inputs.astype(np.float32), train_targets.astype(np.float32))\n",
    "\n",
    "preds = multi_xgb.predict(train_inputs)\n",
    "\n",
    "scores = []\n",
    "for i in range(3):\n",
    "    scores.append(r2_score(train_targets[:, i], preds[:, i]))\n",
    "print(scores)\n",
    "print(np.stack(scores).mean())\n",
    "\n",
    "preds = multi_xgb.predict(eval_inputs)\n",
    "\n",
    "scores = []\n",
    "for i in range(3):\n",
    "    scores.append(r2_score(eval_targets[:, i], preds[:, i]))\n",
    "print(scores)\n",
    "np.stack(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9017590299366272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "preds = multi_xgb.predict(eval_inputs)\n",
    "\n",
    "scores = []\n",
    "for i in range(3):\n",
    "    scores.append(r2_score(eval_targets[:, i], preds[:, i]))\n",
    "print(scores)\n",
    "np.stack(scores).mean()\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "preds = multi_xgb.predict(eval_inputs)\n",
    "\n",
    "scores = []\n",
    "for i in range(3):\n",
    "    scores.append(r2_score(eval_targets[:, i], preds[:, i]))\n",
    "print(scores)\n",
    "np.stack(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(train_inputs.float(), train_targets.float())\n",
    "eval_ds = TensorDataset(eval_inputs.float(), eval_targets.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+1)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=20,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "        if DROP_PATH_RATE: neptune_run[\"h_parameters\"] = {\"drop_path_rate\": DROP_PATH_RATE}\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    one = r2_score(targets[:, 0], preds[:, 0])\n",
    "    two = r2_score(targets[:, 1], preds[:, 1])\n",
    "    three = r2_score(targets[:, 2], preds[:, 2])\n",
    "    mean_r2 = (one + two + three) / 3\n",
    "    return one, two, three, mean_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A residual block with two 1D convolutional layers.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.elu = nn.ELU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.elu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.elu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"A deeper ResNet-style 1D CNN for Raman spectra.\"\"\"\n",
    "    def __init__(self, input_channels=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.elu = nn.GELU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5), # Increased dropout for better regularization\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, stride=s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "MODEL_NAME = \"ResNet.Finetune\"\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 19\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "DROPOUT = 0.5\n",
    "DROP_PATH_RATE = 0.0\n",
    "SCORE = float('-inf')\n",
    "LOG = False\n",
    "RESUME = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint_name = \"finetune.pt\"\n",
    "\n",
    "\n",
    "model = ResNet().to(device)\n",
    "get_model_size(model)\n",
    "\n",
    "ckpt_path = \"/kaggle/working/pretrain.pt\"\n",
    "ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "scaler = torch.amp.GradScaler(device)\n",
    "train_dl, eval_dl = return_dls(train_ds, eval_ds)\n",
    "\n",
    "total_training_steps = len(train_dl) * EPOCHS\n",
    "warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "if LOG:\n",
    "    neptune_run = setup_neptune()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for inputs, targets in train_dl:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        \n",
    "        #if random.random() < 0.7:\n",
    "        #    noise = torch.randn_like(inputs) * 0.1  # std=0.1\n",
    "        #    inputs = inputs + noise\n",
    "\n",
    "        with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, targets)\n",
    "        \n",
    "        if LOG:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "        scheduler.step()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.detach().cpu()\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "    \n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "\n",
    "    one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "    total_loss = total_loss / len(train_dl)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    eval_total_loss = 0.0\n",
    "    eval_all_logits = []\n",
    "    eval_all_targets = []\n",
    "\n",
    "    for inputs, targets in eval_dl:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "\n",
    "        eval_total_loss += loss.detach().cpu()\n",
    "        eval_all_logits.append(logits.detach().cpu())\n",
    "        eval_all_targets.append(targets.detach().cpu())\n",
    "    \n",
    "    eval_all_logits = torch.cat(eval_all_logits)\n",
    "    eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "    eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "    eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "    \n",
    "    if eval_r2 > SCORE:\n",
    "        SCORE = eval_r2\n",
    "        data = {\"state_dict\": model.state_dict()}\n",
    "        data[\"epoch\"] = epoch \n",
    "        data[\"score\"] = SCORE\n",
    "        torch.save(data, f\"/kaggle/working/{checkpoint_name}\")\n",
    "    \n",
    "    if LOG:\n",
    "        neptune_run[\"train/loss\"].append(total_loss)\n",
    "        neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "        neptune_run[\"train/r2\"].append(r2)\n",
    "        neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "        neptune_run[\"train/one\"].append(one)\n",
    "        neptune_run[\"train/two\"].append(two)\n",
    "        neptune_run[\"train/three\"].append(three)\n",
    "        neptune_run[\"eval/one\"].append(eval_one)\n",
    "        neptune_run[\"eval/two\"].append(eval_two)\n",
    "        neptune_run[\"eval/three\"].append(eval_three)\n",
    "        \n",
    "    if True:\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"train/loss: {total_loss:.4f}, \"\n",
    "            f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "            f\"train/r2: {r2:.4f}, \"\n",
    "            f\"eval/r2: {eval_r2:.4f}, \"\n",
    "            f\"train/one: {one:.4f}, \"\n",
    "            f\"train/two: {two:.4f}, \"\n",
    "            f\"train/three: {three:.4f}, \"\n",
    "            f\"eval/one: {eval_one:.4f}, \"\n",
    "            f\"eval/two: {eval_two:.4f}, \"\n",
    "            f\"eval/three: {eval_three:.4f} \"\n",
    "        )\n",
    "        \n",
    "if LOG:\n",
    "    neptune_run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "ckpt_path = f\"/kaggle/working/{checkpoint_name}\"\n",
    "ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "ckpt[\"epoch\"], ckpt[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:54.374610Z",
     "iopub.status.busy": "2025-08-13T05:38:54.374187Z",
     "iopub.status.idle": "2025-08-13T05:38:54.522345Z",
     "shell.execute_reply": "2025-08-13T05:38:54.520846Z",
     "shell.execute_reply.started": "2025-08-13T05:38:54.374578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#def load_test():    \n",
    "test = pd.read_csv(os.path.join(path, files[6]))\n",
    "\n",
    "row1 = test.columns[1:].to_numpy().copy()\n",
    "row1[-1] = \"5611\"\n",
    "row1 = row1.astype(np.float64)\n",
    "\n",
    "\n",
    "cols = test.columns[1:]\n",
    "test = test[cols]\n",
    "test[\" 5611]\"] = test[\" 5611]\"].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "test = test.to_numpy()\n",
    "\n",
    "test = np.insert(test, 0, row1, axis=0)\n",
    "test = test.reshape(-1, 2, 2048).mean(axis=1)\n",
    "\n",
    "return_stats(test)\n",
    "test = get_advanced_spectra_features(test)\n",
    "test = torch.tensor(test)\n",
    "return_stats(test)\n",
    "test = zscore(test, mean, std).float()\n",
    "test.shape, test.dtype, return_stats(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.numpy()\n",
    "test = test.reshape(-1, 3 * 2048)\n",
    "preds = multi_xgb.predict(test)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = ResNet().to(device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = model(test.cuda())\n",
    "\n",
    "preds = preds.cpu().detach().numpy()\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:38:58.909362Z",
     "iopub.status.busy": "2025-08-13T05:38:58.908930Z",
     "iopub.status.idle": "2025-08-13T05:38:58.926823Z",
     "shell.execute_reply": "2025-08-13T05:38:58.925789Z",
     "shell.execute_reply.started": "2025-08-13T05:38:58.909334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#def prepare_test():\n",
    "column_names = ['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']\n",
    "preds_df = pd.DataFrame(preds, columns=column_names)\n",
    "preds_df.insert(0, 'ID', [i+1 for i in range(len(preds_df))])\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:39:07.934713Z",
     "iopub.status.busy": "2025-08-13T05:39:07.934305Z",
     "iopub.status.idle": "2025-08-13T05:39:07.955465Z",
     "shell.execute_reply": "2025-08-13T05:39:07.954454Z",
     "shell.execute_reply.started": "2025-08-13T05:39:07.934670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#def save_test():\n",
    "#name = f\"{MODEL_NAME}.{ckpt['score']:.4f}.csv\"\n",
    "name = \"xgboost.concat.9017.csv\"\n",
    "preds_df.to_csv(name, index=False)\n",
    "f = pd.read_csv(f\"/kaggle/working/{name}\")\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12807347,
     "sourceId": 105802,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
