{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75833e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efb1268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12cf169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280887296"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_memory_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "SEED = 1000\n",
    "setup_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c29561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721289216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.device_memory_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8039a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def average_state_dicts(state_dict_list):\n",
    "    n = len(state_dict_list)\n",
    "    # Ensure we don't modify the originals\n",
    "    avg_sd = OrderedDict()\n",
    "\n",
    "    # Iterate over every parameter/buffer key\n",
    "    for k in state_dict_list[0]:\n",
    "        # sum across models â†’ float32 to avoid overflow on int types\n",
    "        avg = sum(sd[k].float() for sd in state_dict_list) / n\n",
    "        # cast back to original dtype if needed\n",
    "        avg_sd[k] = avg.to(dtype=state_dict_list[0][k].dtype)\n",
    "\n",
    "    return avg_sd\n",
    "\n",
    "\n",
    "def get_ckpt_paths(output_dir, keyword):\n",
    "    output_files = sorted(os.listdir(output_dir))\n",
    "\n",
    "    ckpt_paths = []\n",
    "    for f in output_files:\n",
    "        if keyword in f and \"csv\" not in f:\n",
    "            ckpt_path = os.path.join(output_dir, f)\n",
    "            ckpt_paths.append(ckpt_path)\n",
    "            ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "            print(ckpt_path, ckpt[\"epoch\"], ckpt[\"score\"])\n",
    "            \n",
    "    return ckpt_paths\n",
    "\n",
    "\n",
    "def rest(t=4000):\n",
    "    import time\n",
    "    [time.sleep(1) for i in range(t)]\n",
    "        \n",
    "        \n",
    "def generate_csv(preds, name):\n",
    "    column_names = ['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']\n",
    "    preds_df = pd.DataFrame(preds, columns=column_names)\n",
    "    preds_df.insert(0, 'ID', [i+1 for i in range(len(preds_df))])\n",
    "    preds_df.to_csv(name, index=False)\n",
    "    \n",
    "    \n",
    "def get_ckpt(path):\n",
    "    return torch.load(path, weights_only=False)\n",
    "\n",
    "\n",
    "def cuda_to_np(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, train_dl, epochs):\n",
    "    total_training_steps = len(train_dl) * epochs\n",
    "    warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "\n",
    "\n",
    "def get_stats(tensor, p=True, r=False, minmax=False):\n",
    "    if minmax:\n",
    "        min, max = tensor.min(), tensor.max()\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Min: {min}, Max: {max} ,Mean: {mean}, Std: {std}\")\n",
    "        if r: return min, max, mean, std\n",
    "    else:\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Mean: {mean}, Std: {std}\")\n",
    "        if r: return mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-8)\n",
    "\n",
    "\n",
    "def reverse_zscore(tensor, mu, sigma):\n",
    "    return (tensor * sigma) + mu\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "    \n",
    "\n",
    "def get_index(iterable):\n",
    "    return random.randint(0, len(iterable) - 1)\n",
    "\n",
    "\n",
    "def get_indices(iterable, n):\n",
    "    return random.sample(range(len(iterable)), n)\n",
    "\n",
    "\n",
    "def split(inputs, targets, seed):\n",
    "    return train_test_split(\n",
    "        inputs,\n",
    "        targets, \n",
    "        test_size=0.2,\n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    ) \n",
    "\n",
    "\n",
    "def show_waves(waves, dpi=100):\n",
    "    \"\"\"\n",
    "    waves: numpy array of shape (3, N)\n",
    "    Creates three separate figures that stretch wide.\n",
    "    \"\"\"\n",
    "    N = waves.shape[1]\n",
    "    t = np.arange(N)\n",
    "\n",
    "    # Wide aspect ratio; height modest so each window fills width\n",
    "    for i in range(waves.shape[0]):\n",
    "        fig = plt.figure(figsize=(14, 4), dpi=dpi)  # wide figure\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(t, waves[i], linewidth=1)\n",
    "        ax.set_title(f\"Wave {i+1}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()  # reduce margins to use width\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def hf_ds_download(hf_token, repo_id):\n",
    "    login(hf_token[1:])\n",
    "    return snapshot_download(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "def get_spectra_features(X, b=False):\n",
    "    \"\"\"Create multi-channel features from spectra: raw, 1st derivative, 2nd derivative.\"\"\"\n",
    "    X_processed = np.zeros_like(X)\n",
    "    # Baseline correction and SNV\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        poly = np.polyfit(np.arange(X.shape[1]), X[i], 3)\n",
    "        baseline = np.polyval(poly, np.arange(X.shape[1]))\n",
    "        corrected_spec = X[i] - baseline\n",
    "        #X_processed[i] = (corrected_spec - corrected_spec.mean()) / (corrected_spec.std() + 1e-8)\n",
    "        X_processed[i] = corrected_spec\n",
    "        \n",
    "    # Calculate derivatives\n",
    "    deriv1 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=1, axis=1)\n",
    "    deriv2 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=2, axis=1)\n",
    "\n",
    "    if b: return np.stack([X_processed, deriv1, deriv2], axis=1)\n",
    "    return np.stack([deriv1, deriv2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57252b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'sample_submission.csv')\n",
      "(1, 'timegate.csv')\n",
      "(2, 'mettler_toledo.csv')\n",
      "(3, 'kaiser.csv')\n",
      "(4, 'anton_532.csv')\n",
      "(5, 'transfer_plate.csv')\n",
      "(6, '96_samples.csv')\n",
      "(7, 'tornado.csv')\n",
      "(8, 'tec5.csv')\n",
      "(9, 'metrohm.csv')\n",
      "(10, 'anton_785.csv')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if True:\n",
    "    path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "    files = os.listdir(path)\n",
    "    [print((i, files[i])) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c33b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    hf_token = \"xhf_XURkoNhwOIPtEdHfNeRpVkjEwKSkhtigFi\"\n",
    "    path = hf_ds_download(hf_token, \"ArbaazBeg/kaggle-spectogram\")\n",
    "    files = os.listdir(path)\n",
    "    [(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae341b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    test = pd.read_csv(os.path.join(path, files[6]))\n",
    "\n",
    "    row1 = test.columns[1:].to_numpy().copy()\n",
    "    row1[-1] = \"5611\"\n",
    "    row1 = row1.astype(np.float64)\n",
    "\n",
    "\n",
    "    cols = test.columns[1:]\n",
    "    test = test[cols]\n",
    "    test[\" 5611]\"] = test[\" 5611]\"].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "    test = test.to_numpy()\n",
    "\n",
    "    test = np.insert(test, 0, row1, axis=0)\n",
    "    return test.reshape(-1, 2, 2048).mean(axis=1)\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "    inputs = load_test_data()\n",
    "    \n",
    "    spectra_selection = np.logical_and(\n",
    "        300 <= np.array([float(one) for one in range(2048)]),\n",
    "        np.array([float(one) for one in range(2048)]) <= 1942,\n",
    "    )\n",
    "    \n",
    "    inputs = inputs[:, spectra_selection]\n",
    "\n",
    "    wns = np.array([\n",
    "        float(one) for one in range(2048)\n",
    "    ])[spectra_selection]\n",
    "    wavenumbers = np.arange(300, 1943)\n",
    "\n",
    "    interpolated_data = np.array(\n",
    "        [np.interp(wavenumbers, xp=wns, fp=i) for i in inputs]\n",
    "    )\n",
    "\n",
    "    normed_spectra = interpolated_data / np.max(interpolated_data)\n",
    "    return normed_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ddc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 1643), (96, 3))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_transfer_data():\n",
    "    csv_path = os.path.join(path, files[5])\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    input_cols = df.columns[1:2049]\n",
    "    target_cols = df.columns[2050:]\n",
    "\n",
    "    targets  = df[target_cols].dropna().to_numpy()\n",
    "\n",
    "    df = df[input_cols]\n",
    "    df['Unnamed: 1'] = df['Unnamed: 1'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "    df['Unnamed: 2048'] = df['Unnamed: 2048'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "\n",
    "    inputs = df.to_numpy().reshape(-1, 2, 2048)\n",
    "    inputs = inputs.mean(axis=1)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def preprocess_transfer_data():\n",
    "    inputs, targets = load_transfer_data()\n",
    "    \n",
    "    spectra_selection = np.logical_and(\n",
    "        300 <= np.array([float(one) for one in range(2048)]),\n",
    "        np.array([float(one) for one in range(2048)]) <= 1942,\n",
    "    )\n",
    "    \n",
    "    inputs = inputs[:, spectra_selection]\n",
    "    \n",
    "    wns = np.array([\n",
    "        float(one) for one in range(2048)\n",
    "    ])[spectra_selection]\n",
    "    wavenumbers = np.arange(300, 1943)\n",
    "    \n",
    "    interpolated_data = np.array(\n",
    "        [np.interp(wavenumbers, xp=wns, fp=i) for i in inputs]\n",
    "    )\n",
    "    \n",
    "    normed_spectra = interpolated_data / np.max(interpolated_data)\n",
    "    return normed_spectra, targets\n",
    "\n",
    "inputs, targets = preprocess_transfer_data()\n",
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "np_dtype_from_torch = {\n",
    "    torch.float32: np.float32,\n",
    "    torch.float64: np.float64,\n",
    "}\n",
    "\n",
    "class SpectralDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra,\n",
    "        concentrations,\n",
    "        dtype=None,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=0.0,\n",
    "        baseline_factor_bound=0.0,\n",
    "        baseline_period_lower_bound=100.0,\n",
    "        baseline_period_upper_bound=200.0,\n",
    "        augment_slope_std=0.0,\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=0,\n",
    "        spectrum_rolling_sigma=0.0,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    ):\n",
    "        self.dtype = dtype or torch.float32\n",
    "        self.combine_spectra_range = combine_spectra_range\n",
    "        self.baseline_factor_bound = baseline_factor_bound\n",
    "        self.augment_slope_std = augment_slope_std\n",
    "        self.augment_intercept_std = augment_intersept_std\n",
    "        self.baseline_period_lower_bound = baseline_period_lower_bound\n",
    "        self.baseline_period_upper_bound = baseline_period_upper_bound\n",
    "        self.rolling_bound = rolling_bound\n",
    "        self.spectrum_rolling_sigma = spectrum_rolling_sigma\n",
    "        self.augmentation_weight = torch.tensor(augmentation_weight, dtype=dtype)\n",
    "        self.original_dp_weight = original_datapoint_weight\n",
    "\n",
    "        # normalize spectra\n",
    "        spectra = torch.tensor(spectra, dtype=dtype)\n",
    "\n",
    "        if spectra_mean_std is None:\n",
    "            self.s_mean = torch.mean(spectra)\n",
    "            self.s_std = torch.std(spectra)\n",
    "        else:\n",
    "            self.s_mean, self.s_std = spectra_mean_std\n",
    "\n",
    "        self.spectra = torch.divide(\n",
    "            torch.subtract(spectra, self.s_mean),\n",
    "            self.s_std,\n",
    "        )\n",
    "\n",
    "        self.dummy_wns = np.tile(\n",
    "            np.arange(\n",
    "                0., 1., 1. / self.spectra.shape[2],\n",
    "                dtype=np_dtype_from_torch[self.dtype]\n",
    "            )[None, :self.spectra.shape[2]],\n",
    "            (self.spectra.shape[1], 1),\n",
    "        )\n",
    "\n",
    "        # normalize concentrations\n",
    "        concentrations = torch.tensor(concentrations, dtype=dtype)\n",
    "        if concentration_mean_std is None:\n",
    "            self.concentration_means = torch.nanmean(concentrations, dim=0)\n",
    "\n",
    "            self.concentration_stds = torch.maximum(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        torch.std(col[torch.logical_not(torch.isnan(col))])\n",
    "                        for col in concentrations.T\n",
    "                    ]\n",
    "                ),\n",
    "                torch.tensor([1e-3] * concentrations.shape[1]),\n",
    "            )\n",
    "        else:\n",
    "            self.concentration_means = concentration_mean_std[0]\n",
    "            self.concentration_stds = concentration_mean_std[1]\n",
    "\n",
    "        self.concentrations = torch.divide(\n",
    "            torch.subtract(\n",
    "                concentrations,\n",
    "                self.concentration_means,\n",
    "            ),\n",
    "            self.concentration_stds,\n",
    "        )\n",
    "\n",
    "    def pick_two(self, max_idx=None):\n",
    "        max_idx = max_idx or len(self)\n",
    "        return random.choices(range(max_idx), k=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.concentrations)\n",
    "\n",
    "    def augment_spectra(self, spectra):\n",
    "        if self.augment_slope_std > 0.0:\n",
    "\n",
    "            def spectrum_approximation(x, slope, intercept):\n",
    "                return (slope * x + intercept).reshape(-1, 1)[:, 0]\n",
    "\n",
    "            slope, inter = scipy.optimize.curve_fit(\n",
    "                spectrum_approximation,\n",
    "                self.dummy_wns,\n",
    "                spectra.reshape(-1, 1)[:, 0],\n",
    "                p0=np.random.rand(2),\n",
    "            )[0]\n",
    "\n",
    "            new_slope = slope * (\n",
    "                    np.random.gamma(\n",
    "                        shape=1. / self.augment_slope_std,\n",
    "                        scale=self.augment_slope_std,\n",
    "                        size=1,\n",
    "                    )\n",
    "            )[0]\n",
    "            new_intercept = inter * (\n",
    "                1.0 + np.random.randn(1) * self.augment_intercept_std\n",
    "            )[0]\n",
    "            spectra += torch.tensor(\n",
    "                (new_slope - slope)\n",
    "            ) * self.dummy_wns + new_intercept - inter\n",
    "\n",
    "        factor = self.baseline_factor_bound * torch.rand(size=(1,))\n",
    "        offset = torch.rand(size=(1,)) * 2.0 * torch.pi\n",
    "        period = self.baseline_period_lower_bound + (\n",
    "            self.baseline_period_upper_bound - self.baseline_period_lower_bound\n",
    "        ) * torch.rand(size=(1,))\n",
    "        permutations = factor * torch.cos(\n",
    "            2.0 * torch.pi / period * self.dummy_wns + offset\n",
    "        )\n",
    "        return self.roll_spectrum(\n",
    "            spectra + permutations * spectra,\n",
    "            delta=random.randint(-self.rolling_bound, self.rolling_bound),\n",
    "        )\n",
    "\n",
    "    def roll_spectrum(self, spectra, delta):\n",
    "        num_spectra = spectra.shape[0]\n",
    "        rolled_spectra = np.roll(spectra, delta, axis=1)\n",
    "        if delta > 0:\n",
    "            rolled_spectra[:, :delta] = (\n",
    "                np.random.rand(num_spectra, delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta:(delta + 1)]\n",
    "        elif delta < 0:\n",
    "            rolled_spectra[:, delta:] = (\n",
    "                np.random.rand(num_spectra, -delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta - 1:delta]\n",
    "        return rolled_spectra\n",
    "\n",
    "    def combine_k_items(self, indices, weights):\n",
    "        return (\n",
    "            # spectra\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None, None], self.spectra[indices, :, :]),\n",
    "                dim=0,\n",
    "            ),\n",
    "            # concentrations\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None], self.concentrations[indices, :]),\n",
    "                dim=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.combine_spectra_range < 1e-12:\n",
    "            spectrum = self.spectra[idx]\n",
    "            spectrum = self.augment_spectra(spectrum)\n",
    "            return {\n",
    "                \"spectra\": spectrum,\n",
    "                \"concentrations\": self.concentrations[idx],\n",
    "                \"label_weight\": torch.tensor(1.0, dtype=self.dtype),\n",
    "            }\n",
    "        else:\n",
    "            if random.random() < self.original_dp_weight:\n",
    "                one_weight = 1.\n",
    "                label_weight = torch.tensor(1.0, dtype=self.dtype)\n",
    "            else:\n",
    "                one_weight = random.uniform(0.0, self.combine_spectra_range)\n",
    "                label_weight = self.augmentation_weight\n",
    "            weights = torch.tensor([one_weight, (1 - one_weight)])\n",
    "            # just pick two random indices\n",
    "            indices = random.choices(range(len(self)), k=2)\n",
    "\n",
    "            mixed_spectra, mixed_concentrations = self.combine_k_items(\n",
    "                indices=indices,\n",
    "                weights=weights,\n",
    "            )\n",
    "            mixed_spectra = self.augment_spectra(mixed_spectra)\n",
    "            return mixed_spectra, mixed_concentrations, label_weight\n",
    "\n",
    "\n",
    "config = {\n",
    "    'initial_cnn_channels': 32,\n",
    "    'cnn_channel_factor': 1.279574024454846,\n",
    "    'num_cnn_layers': 8,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'activation_function': 'ELU',\n",
    "    'fc_dropout': 0.10361700399831791,\n",
    "    'lr': 0.001,\n",
    "    'gamma': 0.9649606352621118,\n",
    "    'baseline_factor_bound': 0.748262317340447,\n",
    "    'baseline_period_lower_bound': 0.9703081695287203,\n",
    "    'baseline_period_span': 19.79744237606427,\n",
    "    'original_datapoint_weight': 0.4335003268130408,\n",
    "    'augment_slope_std': 0.08171025264382692,\n",
    "    'batch_size': 32,\n",
    "    'fc_dims': 226,\n",
    "    'rolling_bound': 2,\n",
    "    'num_blocks': 2,\n",
    "}\n",
    "\n",
    "def get_dataset(inputs, targets, config, inputs_mean_std=None, targets_mean_std=None):\n",
    "    return SpectralDataset(\n",
    "        spectra=inputs[:, None, :],\n",
    "        concentrations=targets,\n",
    "        dtype=torch.float32,\n",
    "        spectra_mean_std=inputs_mean_std,\n",
    "        concentration_mean_std=targets_mean_std,\n",
    "        combine_spectra_range=1.0,\n",
    "        baseline_factor_bound=config[\"baseline_factor_bound\"],\n",
    "        baseline_period_lower_bound=config[\"baseline_period_lower_bound\"],\n",
    "        baseline_period_upper_bound=(config[\"baseline_period_lower_bound\"] + config[\"baseline_period_span\"]),\n",
    "        augment_slope_std=config[\"augment_slope_std\"],\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=config[\"rolling_bound\"],\n",
    "        spectrum_rolling_sigma=0.01,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+5232)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds, train_batch_size, eval_batch_size):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "        if DROPOUT: neptune_run[\"h_parameters\"] = {\"dropout\": DROPOUT}\n",
    "        if DROP_PATH_RATE: neptune_run[\"h_parameters\"] = {\"drop_path_rate\": DROP_PATH_RATE}\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().float().numpy()\n",
    "    targets = targets.cpu().detach().float().numpy()\n",
    "    \n",
    "    dim1 = r2_score(targets[:, 0], preds[:, 0])\n",
    "    dim2 = r2_score(targets[:, 1], preds[:, 1])\n",
    "    dim3 = r2_score(targets[:, 2], preds[:, 2])\n",
    "    \n",
    "    return dim1, dim2, dim3, r2_score(targets, preds)\n",
    "\n",
    "\n",
    "class MSEIgnoreNans(_Loss):\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        weights: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        mask = torch.isfinite(target)\n",
    "        mse = torch.mean(\n",
    "            torch.mul(\n",
    "                torch.square(input[mask] - target[mask]),\n",
    "                torch.tile(weights[:, None], dims=(1, target.shape[1]))[mask],\n",
    "            )\n",
    "        )\n",
    "        return torch.where(\n",
    "            torch.isfinite(mse),\n",
    "            mse,\n",
    "            torch.tensor(0.).to(target.device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Identity(torch.torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "# this is not a resnet yet\n",
    "class ReZeroBlock(torch.torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        activation_function,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dtype,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(ReZeroBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = torch.torch.nn.BatchNorm1d\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = divmod(kernel_size, 2)[0] if stride == 1 else 0\n",
    "\n",
    "        # does not change spatial dimension\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.bn1 = norm_layer(out_channels, dtype=dtype)\n",
    "        # Both self.conv2 and self.downsample layers\n",
    "        # downsample the input when stride != 1\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            groups=out_channels,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        if stride > 1:\n",
    "            down_conv = torch.nn.Conv1d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "                dtype=dtype,\n",
    "                # groups=out_channels,\n",
    "            )\n",
    "        else:\n",
    "            down_conv = Identity()\n",
    "\n",
    "        self.down_sample = torch.nn.Sequential(\n",
    "            down_conv,\n",
    "            norm_layer(out_channels),\n",
    "        )\n",
    "        self.bn2 = norm_layer(out_channels, dtype=dtype)\n",
    "        # does not change the spatial dimension\n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.bn3 = norm_layer(out_channels, dtype=dtype)\n",
    "        self.activation = activation_function(inplace=True)\n",
    "        self.factor = torch.torch.nn.parameter.Parameter(torch.tensor(0.0, dtype=dtype))\n",
    "\n",
    "    def next_spatial_dim(self, last_spatial_dim):\n",
    "        return math.floor(\n",
    "            (last_spatial_dim + 2 * self.padding - self.kernel_size)\n",
    "            / self.stride + 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        # not really the identity, but kind of\n",
    "        identity = self.down_sample(x)\n",
    "\n",
    "        return self.activation(out * self.factor + identity)\n",
    "\n",
    "\n",
    "class ResNetEncoder(torch.torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectrum_size,\n",
    "        cnn_encoder_channel_dims,\n",
    "        activation_function,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dtype,\n",
    "        num_blocks,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "\n",
    "        self.spatial_dims = [spectrum_size]\n",
    "        layers = []\n",
    "        for in_channels, out_channels in zip(\n",
    "            cnn_encoder_channel_dims[:-1],\n",
    "            cnn_encoder_channel_dims[1:],\n",
    "        ):\n",
    "            block = ReZeroBlock(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                activation_function=activation_function,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            layers.append(block)\n",
    "            self.spatial_dims.append(block.next_spatial_dim(self.spatial_dims[-1]))\n",
    "            for _ in range(num_blocks - 1):\n",
    "                block = ReZeroBlock(\n",
    "                    in_channels=out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    activation_function=activation_function,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    dtype=dtype,\n",
    "                )\n",
    "                layers.append(block)\n",
    "                self.spatial_dims.append(block.next_spatial_dim(self.spatial_dims[-1]))\n",
    "\n",
    "        self.resnet_layers = torch.torch.nn.Sequential(*layers)\n",
    "        if verbose:\n",
    "            print(\"CNN Encoder Channel Dims: %s\" % (cnn_encoder_channel_dims))\n",
    "            print(\"CNN Encoder Spatial Dims: %s\" % (self.spatial_dims))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet_layers(x)\n",
    "\n",
    "\n",
    "class ReZeroNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra_channels,\n",
    "        spectra_size,\n",
    "        initial_cnn_channels,\n",
    "        cnn_channel_factor,\n",
    "        num_cnn_layers,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        activation_function,\n",
    "        fc_dims,\n",
    "        fc_dropout=0.0,\n",
    "        dtype=None,\n",
    "        verbose=False,\n",
    "        fc_output_channels=1,\n",
    "        num_blocks=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc_output_channels = fc_output_channels\n",
    "        self.dtype = dtype or torch.float32\n",
    "\n",
    "        activation_function = getattr(torch.nn, activation_function)\n",
    "\n",
    "        # Setup CNN Encoder\n",
    "        cnn_encoder_channel_dims = [spectra_channels] + [\n",
    "            int(initial_cnn_channels * (cnn_channel_factor**idx))\n",
    "            for idx in range(num_cnn_layers)\n",
    "        ]\n",
    "        self.cnn_encoder = ResNetEncoder(\n",
    "            spectrum_size=spectra_size,\n",
    "            cnn_encoder_channel_dims=cnn_encoder_channel_dims,\n",
    "            activation_function=activation_function,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            num_blocks=num_blocks,\n",
    "            dtype=dtype,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.fc_dims = [\n",
    "            int(\n",
    "                self.cnn_encoder.spatial_dims[-1]\n",
    "            ) * int(cnn_encoder_channel_dims[-1])\n",
    "        ] + fc_dims\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Fc Dims: %s\" % self.fc_dims)\n",
    "        fc_layers = []\n",
    "        for idx, (in_dim, out_dim) in enumerate(\n",
    "                zip(self.fc_dims[:-2], self.fc_dims[1:-1])\n",
    "        ):\n",
    "            fc_layers.append(torch.nn.Linear(in_dim, out_dim))\n",
    "            fc_layers.append(torch.nn.ELU())\n",
    "            fc_layers.append(torch.nn.Dropout(fc_dropout / (2 ** idx)))\n",
    "        fc_layers.append(\n",
    "            torch.nn.Linear(\n",
    "                self.fc_dims[-2],\n",
    "                self.fc_dims[-1] * self.fc_output_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.fc_net = torch.nn.Sequential(*fc_layers)\n",
    "        if verbose:\n",
    "            num_params = sum(p.numel() for p in self.parameters())\n",
    "            print(\"Number of Parameters: %s\" % num_params)\n",
    "\n",
    "    def forward(self, spectra):\n",
    "        embeddings = self.cnn_encoder(spectra)\n",
    "        forecast = self.fc_net(embeddings.view(-1, self.fc_dims[0]))\n",
    "        if self.fc_output_channels > 1:\n",
    "            forecast = forecast.reshape(\n",
    "                -1, self.fc_output_channels, self.fc_dims[-1]\n",
    "            )\n",
    "        return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    device,\n",
    "    amp_dtype,\n",
    "    scheduler,\n",
    "    train_dl,\n",
    "    eval_dl,\n",
    "    loss_fn,\n",
    "    epochs,\n",
    "    checkpoint_name,\n",
    "    score=-float(\"inf\"),\n",
    "    neptune_run=None,\n",
    "    p=True,\n",
    "):  \n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for inputs, targets, weights in train_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(device_type=device, dtype=amp_dtype, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets, weights)\n",
    "                  \n",
    "            if amp_dtype == torch.bfloat16:                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            scheduler.step()\n",
    "            if neptune_run is not None:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            total_loss += loss.detach().cpu()\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        all_logits = torch.cat(all_logits)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "        total_loss = total_loss / len(train_dl)\n",
    "        \n",
    "        model.eval()\n",
    "        eval_total_loss = 0.0\n",
    "        eval_all_logits = []\n",
    "        eval_all_targets = []\n",
    "\n",
    "        for inputs, targets, weights in eval_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                with torch.amp.autocast(device_type=device, dtype=amp_dtype, cache_enabled=True):\n",
    "                    logits = model(inputs)\n",
    "                    loss = loss_fn(logits, targets, weights)\n",
    "\n",
    "            eval_total_loss += loss.detach().cpu()\n",
    "            eval_all_logits.append(logits.detach().cpu())\n",
    "            eval_all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        eval_all_logits = torch.cat(eval_all_logits)\n",
    "        eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "        eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "        eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "        \n",
    "        if eval_r2 > score:\n",
    "            score = eval_r2\n",
    "            data = {\"state_dict\": model.state_dict()}\n",
    "            data[\"epoch\"] = epoch \n",
    "            data[\"score\"] = score\n",
    "            torch.save(data, f\"/kaggle/working/{checkpoint_name}\")\n",
    "        \n",
    "        if neptune_run is not None:\n",
    "            neptune_run[\"train/loss\"].append(total_loss)\n",
    "            neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "            neptune_run[\"train/r2\"].append(r2)\n",
    "            neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "            neptune_run[\"train/one\"].append(one)\n",
    "            neptune_run[\"train/two\"].append(two)\n",
    "            neptune_run[\"train/three\"].append(three)\n",
    "            neptune_run[\"eval/one\"].append(eval_one)\n",
    "            neptune_run[\"eval/two\"].append(eval_two)\n",
    "            neptune_run[\"eval/three\"].append(eval_three)\n",
    "            \n",
    "        if p and epoch % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, \"\n",
    "                f\"train/loss: {total_loss:.4f}, \"\n",
    "                f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "                f\"train/r2: {r2:.4f}, \"\n",
    "                f\"eval/r2: {eval_r2:.4f}, \"\n",
    "                f\"train/one: {one:.4f}, \"\n",
    "                f\"train/two: {two:.4f}, \"\n",
    "                f\"train/three: {three:.4f}, \"\n",
    "                f\"eval/one: {eval_one:.4f}, \"\n",
    "                f\"eval/two: {eval_two:.4f}, \"\n",
    "                f\"eval/three: {eval_three:.4f} \"\n",
    "            )\n",
    "            \n",
    "    if neptune_run is not None: neptune_run.stop()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "\n",
    "DROPOUT = 0.5\n",
    "DROP_PATH_RATE = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESUME = False\n",
    "\n",
    "config[\"dtype\"] = torch.float32\n",
    "config[\"spectra_size\"] = 1643\n",
    "config[\"spectra_channels\"] = 1\n",
    "config[\"fc_dims\"] = [\n",
    "    config[\"fc_dims\"],\n",
    "    int(config[\"fc_dims\"] / 2),\n",
    "    3,\n",
    "]\n",
    "\n",
    "mse_loss_function = MSEIgnoreNans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e18593",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_mean = torch.tensor(0.23636309802532196)\n",
    "inputs_std = torch.tensor(0.17638377845287323)\n",
    "targets_means = torch.tensor([3.0586953163146973, 0.6427047848701477, 1.0438220500946045])\n",
    "targets_stds = torch.tensor([3.8679287433624268, 0.5630283951759338, 1.3215022087097168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f796300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734309\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f077948aa17447288764dc7b0c0a8aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.9970, eval/loss: 2.6166, train/r2: -2.2397, eval/r2: -4.7254, train/one: -1.0089, train/two: -3.2348, train/three: -2.4755, eval/one: -2.4350, eval/two: -6.8863, eval/three: -4.8549 \n",
      "Epoch: 5, train/loss: 0.8850, eval/loss: 0.6925, train/r2: -0.5596, eval/r2: -0.2821, train/one: -0.2956, train/two: -0.7051, train/three: -0.6781, eval/one: -0.0013, eval/two: -0.3100, eval/three: -0.5351 \n",
      "Epoch: 10, train/loss: 0.5507, eval/loss: 0.5673, train/r2: 0.0021, eval/r2: -0.1482, train/one: 0.0845, train/two: 0.0424, train/three: -0.1206, eval/one: -0.3849, eval/two: 0.0568, eval/three: -0.1165 \n",
      "Epoch: 15, train/loss: 0.4953, eval/loss: 0.5463, train/r2: 0.1194, eval/r2: 0.1203, train/one: 0.4893, train/two: 0.0221, train/three: -0.1533, eval/one: 0.3176, eval/two: -0.0363, eval/three: 0.0796 \n",
      "Epoch: 20, train/loss: 0.3497, eval/loss: 0.4333, train/r2: 0.3338, eval/r2: -0.0056, train/one: 0.6322, train/two: 0.2597, train/three: 0.1095, eval/one: 0.3983, eval/two: 0.1314, eval/three: -0.5466 \n",
      "Epoch: 25, train/loss: 0.3234, eval/loss: 0.5571, train/r2: 0.3847, eval/r2: 0.1737, train/one: 0.6611, train/two: 0.2809, train/three: 0.2122, eval/one: 0.3506, eval/two: -0.1658, eval/three: 0.3363 \n",
      "Epoch: 30, train/loss: 0.3102, eval/loss: 0.2474, train/r2: 0.4253, eval/r2: 0.3345, train/one: 0.6310, train/two: 0.3600, train/three: 0.2850, eval/one: 0.4097, eval/two: 0.4062, eval/three: 0.1876 \n",
      "Epoch: 35, train/loss: 0.3396, eval/loss: 0.3224, train/r2: 0.4567, eval/r2: 0.1826, train/one: 0.7462, train/two: 0.2943, train/three: 0.3296, eval/one: 0.4358, eval/two: -0.1794, eval/three: 0.2914 \n",
      "Epoch: 40, train/loss: 0.2573, eval/loss: 0.2811, train/r2: 0.6156, eval/r2: 0.4108, train/one: 0.7757, train/two: 0.4930, train/three: 0.5782, eval/one: 0.6044, eval/two: 0.2588, eval/three: 0.3691 \n",
      "Epoch: 45, train/loss: 0.2794, eval/loss: 0.4230, train/r2: 0.6187, eval/r2: 0.3286, train/one: 0.8040, train/two: 0.4702, train/three: 0.5819, eval/one: 0.4397, eval/two: 0.0070, eval/three: 0.5390 \n",
      "Epoch: 50, train/loss: 0.1996, eval/loss: 0.3001, train/r2: 0.6945, eval/r2: 0.4634, train/one: 0.8289, train/two: 0.6173, train/three: 0.6374, eval/one: 0.8111, eval/two: 0.0065, eval/three: 0.5727 \n",
      "Epoch: 55, train/loss: 0.1378, eval/loss: 0.3235, train/r2: 0.7797, eval/r2: 0.1940, train/one: 0.8774, train/two: 0.6848, train/three: 0.7768, eval/one: 0.7141, eval/two: 0.1834, eval/three: -0.3156 \n",
      "Epoch: 60, train/loss: 0.1997, eval/loss: 0.2397, train/r2: 0.6635, eval/r2: 0.6317, train/one: 0.7996, train/two: 0.5180, train/three: 0.6728, eval/one: 0.7735, eval/two: 0.4032, eval/three: 0.7185 \n",
      "Epoch: 65, train/loss: 0.1347, eval/loss: 0.2329, train/r2: 0.7914, eval/r2: 0.2912, train/one: 0.8942, train/two: 0.7199, train/three: 0.7602, eval/one: 0.7440, eval/two: 0.1889, eval/three: -0.0592 \n",
      "Epoch: 70, train/loss: 0.1764, eval/loss: 0.2005, train/r2: 0.7242, eval/r2: 0.6124, train/one: 0.7362, train/two: 0.6287, train/three: 0.8077, eval/one: 0.6982, eval/two: 0.4321, eval/three: 0.7071 \n",
      "Epoch: 75, train/loss: 0.1313, eval/loss: 0.3090, train/r2: 0.8095, eval/r2: 0.5108, train/one: 0.8517, train/two: 0.7417, train/three: 0.8351, eval/one: 0.4864, eval/two: 0.3615, eval/three: 0.6845 \n",
      "Epoch: 80, train/loss: 0.1655, eval/loss: 0.3111, train/r2: 0.7647, eval/r2: 0.6231, train/one: 0.8192, train/two: 0.6670, train/three: 0.8079, eval/one: 0.6314, eval/two: 0.4203, eval/three: 0.8177 \n",
      "Epoch: 85, train/loss: 0.1511, eval/loss: 0.1955, train/r2: 0.7736, eval/r2: 0.5275, train/one: 0.8236, train/two: 0.6538, train/three: 0.8432, eval/one: 0.7778, eval/two: 0.2299, eval/three: 0.5749 \n",
      "Epoch: 90, train/loss: 0.1284, eval/loss: 0.3387, train/r2: 0.7715, eval/r2: 0.5281, train/one: 0.8107, train/two: 0.7152, train/three: 0.7886, eval/one: 0.5697, eval/two: 0.2291, eval/three: 0.7857 \n",
      "Epoch: 95, train/loss: 0.1353, eval/loss: 0.1751, train/r2: 0.8162, eval/r2: 0.6529, train/one: 0.8669, train/two: 0.7946, train/three: 0.7871, eval/one: 0.7175, eval/two: 0.4918, eval/three: 0.7494 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 180 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 180 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-225/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e855e2bd7fe6430c9655f99dba0a18cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 2.1258, eval/loss: 1.6231, train/r2: -2.5868, eval/r2: -2.4165, train/one: -0.5898, train/two: -4.1007, train/three: -3.0698, eval/one: -1.4210, eval/two: -1.9473, eval/three: -3.8812 \n",
      "Epoch: 5, train/loss: 0.8193, eval/loss: 0.7305, train/r2: -0.3754, eval/r2: -0.6917, train/one: -0.4760, train/two: -0.1939, train/three: -0.4563, eval/one: -0.2188, eval/two: -0.3041, eval/three: -1.5523 \n",
      "Epoch: 10, train/loss: 0.5886, eval/loss: 0.5118, train/r2: 0.0814, eval/r2: -0.1378, train/one: 0.2906, train/two: -0.1108, train/three: 0.0646, eval/one: 0.1916, eval/two: 0.0705, eval/three: -0.6754 \n",
      "Epoch: 15, train/loss: 0.5285, eval/loss: 0.4027, train/r2: 0.2488, eval/r2: 0.2468, train/one: 0.5437, train/two: 0.1179, train/three: 0.0848, eval/one: 0.4695, eval/two: 0.1966, eval/three: 0.0743 \n",
      "Epoch: 20, train/loss: 0.4173, eval/loss: 0.5969, train/r2: 0.3215, eval/r2: 0.0826, train/one: 0.5488, train/two: 0.1442, train/three: 0.2716, eval/one: 0.6093, eval/two: -0.1264, eval/three: -0.2350 \n",
      "Epoch: 25, train/loss: 0.3648, eval/loss: 0.4906, train/r2: 0.3643, eval/r2: 0.0293, train/one: 0.6689, train/two: 0.2254, train/three: 0.1986, eval/one: 0.2448, eval/two: -0.1212, eval/three: -0.0358 \n",
      "Epoch: 30, train/loss: 0.3456, eval/loss: 0.3452, train/r2: 0.4633, eval/r2: 0.3246, train/one: 0.6877, train/two: 0.2598, train/three: 0.4423, eval/one: 0.5593, eval/two: 0.1226, eval/three: 0.2919 \n",
      "Epoch: 35, train/loss: 0.3118, eval/loss: 0.4182, train/r2: 0.4979, eval/r2: 0.1975, train/one: 0.6719, train/two: 0.3879, train/three: 0.4340, eval/one: 0.5998, eval/two: 0.3350, eval/three: -0.3423 \n",
      "Epoch: 40, train/loss: 0.2356, eval/loss: 0.2876, train/r2: 0.6082, eval/r2: 0.4821, train/one: 0.7811, train/two: 0.4879, train/three: 0.5557, eval/one: 0.6593, eval/two: 0.4550, eval/three: 0.3321 \n",
      "Epoch: 45, train/loss: 0.2248, eval/loss: 0.2734, train/r2: 0.6114, eval/r2: 0.5194, train/one: 0.7778, train/two: 0.5540, train/three: 0.5024, eval/one: 0.7628, eval/two: 0.4001, eval/three: 0.3952 \n",
      "Epoch: 50, train/loss: 0.1962, eval/loss: 0.3058, train/r2: 0.7084, eval/r2: 0.5181, train/one: 0.6934, train/two: 0.6892, train/three: 0.7427, eval/one: 0.6152, eval/two: 0.4797, eval/three: 0.4595 \n",
      "Epoch: 55, train/loss: 0.1681, eval/loss: 0.2824, train/r2: 0.7633, eval/r2: 0.3996, train/one: 0.8057, train/two: 0.6793, train/three: 0.8048, eval/one: 0.8141, eval/two: 0.0934, eval/three: 0.2913 \n",
      "Epoch: 60, train/loss: 0.1550, eval/loss: 0.1723, train/r2: 0.7533, eval/r2: 0.6445, train/one: 0.7936, train/two: 0.7348, train/three: 0.7315, eval/one: 0.8561, eval/two: 0.5682, eval/three: 0.5091 \n",
      "Epoch: 65, train/loss: 0.1578, eval/loss: 0.1985, train/r2: 0.7586, eval/r2: 0.5662, train/one: 0.8217, train/two: 0.6887, train/three: 0.7655, eval/one: 0.5062, eval/two: 0.4866, eval/three: 0.7057 \n",
      "Epoch: 70, train/loss: 0.1406, eval/loss: 0.1962, train/r2: 0.7890, eval/r2: 0.7384, train/one: 0.8519, train/two: 0.7016, train/three: 0.8133, eval/one: 0.9500, eval/two: 0.5026, eval/three: 0.7626 \n",
      "Epoch: 75, train/loss: 0.1277, eval/loss: 0.2485, train/r2: 0.8163, eval/r2: 0.5875, train/one: 0.8563, train/two: 0.7787, train/three: 0.8138, eval/one: 0.7267, eval/two: 0.4823, eval/three: 0.5535 \n",
      "Epoch: 80, train/loss: 0.1375, eval/loss: 0.1793, train/r2: 0.7808, eval/r2: 0.7060, train/one: 0.8776, train/two: 0.6942, train/three: 0.7707, eval/one: 0.6442, eval/two: 0.6179, eval/three: 0.8558 \n",
      "Epoch: 85, train/loss: 0.1404, eval/loss: 0.2056, train/r2: 0.7778, eval/r2: 0.6278, train/one: 0.8468, train/two: 0.6414, train/three: 0.8454, eval/one: 0.8370, eval/two: 0.4226, eval/three: 0.6238 \n",
      "Epoch: 90, train/loss: 0.1264, eval/loss: 0.1955, train/r2: 0.7639, eval/r2: 0.6312, train/one: 0.7476, train/two: 0.7248, train/three: 0.8194, eval/one: 0.9067, eval/two: 0.5023, eval/three: 0.4846 \n",
      "Epoch: 95, train/loss: 0.1257, eval/loss: 0.1193, train/r2: 0.8011, eval/r2: 0.7502, train/one: 0.8767, train/two: 0.6651, train/three: 0.8613, eval/one: 0.8449, eval/two: 0.7221, eval/three: 0.6835 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 129 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 129 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-226/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656ab4a060c84885b6c5bc6e15d08aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.7771, eval/loss: 2.3734, train/r2: -2.1280, eval/r2: -2.2569, train/one: -1.1896, train/two: -2.9957, train/three: -2.1986, eval/one: -1.5774, eval/two: -4.0033, eval/three: -1.1901 \n",
      "Epoch: 5, train/loss: 0.8581, eval/loss: 0.6423, train/r2: -0.5721, eval/r2: -0.7904, train/one: -0.2784, train/two: -0.5720, train/three: -0.8659, eval/one: -0.1590, eval/two: -0.8655, eval/three: -1.3468 \n",
      "Epoch: 10, train/loss: 0.6475, eval/loss: 0.6670, train/r2: -0.2228, eval/r2: -0.2028, train/one: -0.2111, train/two: -0.1086, train/three: -0.3486, eval/one: -0.3235, eval/two: -0.3312, eval/three: 0.0463 \n",
      "Epoch: 15, train/loss: 0.5315, eval/loss: 0.4438, train/r2: 0.1175, eval/r2: 0.0844, train/one: 0.2574, train/two: 0.0091, train/three: 0.0861, eval/one: 0.5482, eval/two: -0.1776, eval/three: -0.1173 \n",
      "Epoch: 20, train/loss: 0.4215, eval/loss: 0.4275, train/r2: 0.3341, eval/r2: 0.2148, train/one: 0.6386, train/two: 0.1116, train/three: 0.2520, eval/one: 0.6802, eval/two: -0.0602, eval/three: 0.0243 \n",
      "Epoch: 25, train/loss: 0.4185, eval/loss: 0.4074, train/r2: 0.2877, eval/r2: 0.2197, train/one: 0.5252, train/two: 0.0904, train/three: 0.2475, eval/one: 0.5032, eval/two: -0.0443, eval/three: 0.2001 \n",
      "Epoch: 30, train/loss: 0.4339, eval/loss: 0.4228, train/r2: 0.3246, eval/r2: 0.0929, train/one: 0.7185, train/two: 0.1076, train/three: 0.1476, eval/one: -0.1667, eval/two: -0.0241, eval/three: 0.4695 \n",
      "Epoch: 35, train/loss: 0.3338, eval/loss: 0.3422, train/r2: 0.4184, eval/r2: 0.2483, train/one: 0.6783, train/two: 0.2276, train/three: 0.3494, eval/one: 0.2195, eval/two: 0.2790, eval/three: 0.2465 \n",
      "Epoch: 40, train/loss: 0.2956, eval/loss: 0.3358, train/r2: 0.5245, eval/r2: 0.4083, train/one: 0.6103, train/two: 0.5178, train/three: 0.4455, eval/one: 0.7001, eval/two: -0.0287, eval/three: 0.5535 \n",
      "Epoch: 45, train/loss: 0.2256, eval/loss: 0.3686, train/r2: 0.5772, eval/r2: 0.2057, train/one: 0.6789, train/two: 0.5401, train/three: 0.5127, eval/one: 0.8022, eval/two: -0.4579, eval/three: 0.2728 \n",
      "Epoch: 50, train/loss: 0.2500, eval/loss: 0.2893, train/r2: 0.6377, eval/r2: 0.4753, train/one: 0.6926, train/two: 0.6075, train/three: 0.6132, eval/one: 0.7331, eval/two: 0.2924, eval/three: 0.4003 \n",
      "Epoch: 55, train/loss: 0.2178, eval/loss: 0.3057, train/r2: 0.6545, eval/r2: 0.4698, train/one: 0.7534, train/two: 0.5640, train/three: 0.6461, eval/one: 0.5051, eval/two: 0.3040, eval/three: 0.6003 \n",
      "Epoch: 60, train/loss: 0.1944, eval/loss: 0.3869, train/r2: 0.6824, eval/r2: 0.3298, train/one: 0.7477, train/two: 0.5693, train/three: 0.7301, eval/one: 0.4899, eval/two: 0.1582, eval/three: 0.3412 \n",
      "Epoch: 65, train/loss: 0.1505, eval/loss: 0.2302, train/r2: 0.7258, eval/r2: 0.6295, train/one: 0.8076, train/two: 0.7164, train/three: 0.6536, eval/one: 0.8090, eval/two: 0.3797, eval/three: 0.6997 \n",
      "Epoch: 70, train/loss: 0.1759, eval/loss: 0.1209, train/r2: 0.7336, eval/r2: 0.7037, train/one: 0.7896, train/two: 0.7079, train/three: 0.7032, eval/one: 0.8211, eval/two: 0.5857, eval/three: 0.7042 \n",
      "Epoch: 75, train/loss: 0.1745, eval/loss: 0.2114, train/r2: 0.7320, eval/r2: 0.6528, train/one: 0.8609, train/two: 0.6023, train/three: 0.7328, eval/one: 0.7213, eval/two: 0.5219, eval/three: 0.7151 \n",
      "Epoch: 80, train/loss: 0.1619, eval/loss: 0.2484, train/r2: 0.7630, eval/r2: 0.5113, train/one: 0.7853, train/two: 0.7243, train/three: 0.7795, eval/one: 0.8012, eval/two: 0.1394, eval/three: 0.5933 \n",
      "Epoch: 85, train/loss: 0.1557, eval/loss: 0.2105, train/r2: 0.7434, eval/r2: 0.6672, train/one: 0.7783, train/two: 0.7287, train/three: 0.7233, eval/one: 0.7443, eval/two: 0.5598, eval/three: 0.6977 \n",
      "Epoch: 90, train/loss: 0.1710, eval/loss: 0.3170, train/r2: 0.7623, eval/r2: 0.1957, train/one: 0.8460, train/two: 0.7016, train/three: 0.7392, eval/one: 0.6656, eval/two: -0.7076, eval/three: 0.6292 \n",
      "Epoch: 95, train/loss: 0.1681, eval/loss: 0.2276, train/r2: 0.7434, eval/r2: 0.5574, train/one: 0.7844, train/two: 0.6957, train/three: 0.7501, eval/one: 0.6558, eval/two: 0.2016, eval/three: 0.8147 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 116 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 116 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-227/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-228\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d20adeed42945a79debe06f3745e39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.6692, eval/loss: 2.2891, train/r2: -1.9255, eval/r2: -1.5039, train/one: -1.4850, train/two: -2.3512, train/three: -1.9403, eval/one: -1.3809, eval/two: -2.4240, eval/three: -0.7069 \n",
      "Epoch: 5, train/loss: 0.7865, eval/loss: 0.7367, train/r2: -0.6879, eval/r2: -0.8654, train/one: -0.2053, train/two: -1.2679, train/three: -0.5905, eval/one: -0.4992, eval/two: -0.0316, eval/three: -2.0653 \n",
      "Epoch: 10, train/loss: 0.6441, eval/loss: 0.9857, train/r2: -0.1150, eval/r2: -0.7657, train/one: -0.1070, train/two: -0.0457, train/three: -0.1924, eval/one: -2.0860, eval/two: -0.2165, eval/three: 0.0053 \n",
      "Epoch: 15, train/loss: 0.5280, eval/loss: 0.6955, train/r2: 0.1130, eval/r2: 0.1136, train/one: 0.2411, train/two: 0.0440, train/three: 0.0539, eval/one: -0.0028, eval/two: 0.1889, eval/three: 0.1548 \n",
      "Epoch: 20, train/loss: 0.3620, eval/loss: 0.4108, train/r2: 0.2975, eval/r2: 0.3974, train/one: 0.5405, train/two: 0.0956, train/three: 0.2562, eval/one: 0.7182, eval/two: -0.0203, eval/three: 0.4945 \n",
      "Epoch: 25, train/loss: 0.3441, eval/loss: 0.3687, train/r2: 0.3371, eval/r2: 0.2241, train/one: 0.6483, train/two: 0.1615, train/three: 0.2014, eval/one: 0.4588, eval/two: 0.3450, eval/three: -0.1315 \n",
      "Epoch: 30, train/loss: 0.3146, eval/loss: 0.6320, train/r2: 0.4080, eval/r2: 0.4095, train/one: 0.6624, train/two: 0.3001, train/three: 0.2616, eval/one: 0.5653, eval/two: 0.1891, eval/three: 0.4740 \n",
      "Epoch: 35, train/loss: 0.2517, eval/loss: 0.4484, train/r2: 0.5104, eval/r2: 0.4298, train/one: 0.7534, train/two: 0.3953, train/three: 0.3824, eval/one: 0.6937, eval/two: 0.2614, eval/three: 0.3342 \n",
      "Epoch: 40, train/loss: 0.3355, eval/loss: 0.5234, train/r2: 0.4569, eval/r2: 0.2562, train/one: 0.6624, train/two: 0.3866, train/three: 0.3217, eval/one: 0.7160, eval/two: 0.1878, eval/three: -0.1353 \n",
      "Epoch: 45, train/loss: 0.2637, eval/loss: 0.4666, train/r2: 0.6079, eval/r2: 0.3933, train/one: 0.7527, train/two: 0.4303, train/three: 0.6406, eval/one: 0.4975, eval/two: 0.0493, eval/three: 0.6329 \n",
      "Epoch: 50, train/loss: 0.2219, eval/loss: 0.4198, train/r2: 0.6539, eval/r2: 0.4707, train/one: 0.7487, train/two: 0.5066, train/three: 0.7065, eval/one: 0.6757, eval/two: 0.1666, eval/three: 0.5699 \n",
      "Epoch: 55, train/loss: 0.2054, eval/loss: 0.3023, train/r2: 0.6414, eval/r2: 0.5869, train/one: 0.7742, train/two: 0.4633, train/three: 0.6867, eval/one: 0.6756, eval/two: 0.5598, eval/three: 0.5253 \n",
      "Epoch: 60, train/loss: 0.2261, eval/loss: 0.3120, train/r2: 0.6444, eval/r2: 0.4933, train/one: 0.8128, train/two: 0.4393, train/three: 0.6812, eval/one: 0.5777, eval/two: 0.4087, eval/three: 0.4937 \n",
      "Epoch: 65, train/loss: 0.1756, eval/loss: 0.2940, train/r2: 0.7347, eval/r2: 0.5507, train/one: 0.7831, train/two: 0.6205, train/three: 0.8004, eval/one: 0.6538, eval/two: 0.5278, eval/three: 0.4705 \n",
      "Epoch: 70, train/loss: 0.1494, eval/loss: 0.2856, train/r2: 0.7563, eval/r2: 0.6585, train/one: 0.8244, train/two: 0.6566, train/three: 0.7880, eval/one: 0.7394, eval/two: 0.5308, eval/three: 0.7053 \n",
      "Epoch: 75, train/loss: 0.1576, eval/loss: 0.2862, train/r2: 0.7547, eval/r2: 0.6023, train/one: 0.7967, train/two: 0.6868, train/three: 0.7805, eval/one: 0.6398, eval/two: 0.4829, eval/three: 0.6842 \n",
      "Epoch: 80, train/loss: 0.1927, eval/loss: 0.3792, train/r2: 0.6907, eval/r2: 0.5816, train/one: 0.7931, train/two: 0.5368, train/three: 0.7420, eval/one: 0.7818, eval/two: 0.3032, eval/three: 0.6597 \n",
      "Epoch: 85, train/loss: 0.1600, eval/loss: 0.2158, train/r2: 0.7678, eval/r2: 0.5898, train/one: 0.8598, train/two: 0.5877, train/three: 0.8558, eval/one: 0.7177, eval/two: 0.5277, eval/three: 0.5240 \n",
      "Epoch: 90, train/loss: 0.1515, eval/loss: 0.2901, train/r2: 0.7529, eval/r2: 0.4855, train/one: 0.8495, train/two: 0.6706, train/three: 0.7385, eval/one: 0.7622, eval/two: 0.4041, eval/three: 0.2901 \n",
      "Epoch: 95, train/loss: 0.1774, eval/loss: 0.4108, train/r2: 0.7349, eval/r2: 0.5617, train/one: 0.8086, train/two: 0.5733, train/three: 0.8227, eval/one: 0.8208, eval/two: 0.3155, eval/three: 0.5487 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 115 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 115 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-228/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-229\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2bedef064b4959b5da5ae0aa27fc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.7834, eval/loss: 2.4819, train/r2: -1.7460, eval/r2: -3.3398, train/one: -0.7239, train/two: -2.3340, train/three: -2.1800, eval/one: -2.8483, eval/two: -3.7263, eval/three: -3.4448 \n",
      "Epoch: 5, train/loss: 0.9180, eval/loss: 1.4923, train/r2: -0.2935, eval/r2: -1.9667, train/one: -0.3220, train/two: -0.3971, train/three: -0.1615, eval/one: -0.1334, eval/two: -5.4840, eval/three: -0.2826 \n",
      "Epoch: 10, train/loss: 0.6374, eval/loss: 0.6580, train/r2: -0.0515, eval/r2: -0.3045, train/one: 0.0945, train/two: -0.0713, train/three: -0.1778, eval/one: 0.0667, eval/two: -0.4312, eval/three: -0.5490 \n",
      "Epoch: 15, train/loss: 0.5207, eval/loss: 0.4779, train/r2: 0.2220, eval/r2: -0.0812, train/one: 0.3879, train/two: 0.1099, train/three: 0.1682, eval/one: -0.0383, eval/two: -0.2595, eval/three: 0.0543 \n",
      "Epoch: 20, train/loss: 0.4758, eval/loss: 0.3964, train/r2: 0.2452, eval/r2: 0.1626, train/one: 0.4092, train/two: 0.1481, train/three: 0.1783, eval/one: 0.4292, eval/two: -0.0189, eval/three: 0.0776 \n",
      "Epoch: 25, train/loss: 0.3964, eval/loss: 0.4590, train/r2: 0.4054, eval/r2: 0.0982, train/one: 0.6637, train/two: 0.2778, train/three: 0.2748, eval/one: 0.4185, eval/two: -0.1146, eval/three: -0.0093 \n",
      "Epoch: 30, train/loss: 0.3644, eval/loss: 0.2932, train/r2: 0.3730, eval/r2: 0.2585, train/one: 0.7025, train/two: 0.2322, train/three: 0.1843, eval/one: 0.7253, eval/two: 0.0532, eval/three: -0.0031 \n",
      "Epoch: 35, train/loss: 0.2672, eval/loss: 0.2637, train/r2: 0.4795, eval/r2: 0.2469, train/one: 0.7595, train/two: 0.3505, train/three: 0.3286, eval/one: 0.7058, eval/two: -0.0009, eval/three: 0.0357 \n",
      "Epoch: 40, train/loss: 0.3065, eval/loss: 0.1975, train/r2: 0.5249, eval/r2: 0.4658, train/one: 0.7113, train/two: 0.3537, train/three: 0.5096, eval/one: 0.8687, eval/two: 0.4820, eval/three: 0.0467 \n",
      "Epoch: 45, train/loss: 0.2718, eval/loss: 0.2691, train/r2: 0.6132, eval/r2: 0.2859, train/one: 0.7239, train/two: 0.5282, train/three: 0.5874, eval/one: 0.6191, eval/two: 0.0602, eval/three: 0.1785 \n",
      "Epoch: 50, train/loss: 0.2218, eval/loss: 0.2265, train/r2: 0.6757, eval/r2: 0.4839, train/one: 0.7792, train/two: 0.5800, train/three: 0.6679, eval/one: 0.5548, eval/two: 0.5357, eval/three: 0.3611 \n",
      "Epoch: 55, train/loss: 0.2333, eval/loss: 0.3031, train/r2: 0.6710, eval/r2: 0.4579, train/one: 0.8066, train/two: 0.4524, train/three: 0.7541, eval/one: 0.6480, eval/two: 0.3513, eval/three: 0.3746 \n",
      "Epoch: 60, train/loss: 0.2290, eval/loss: 0.2460, train/r2: 0.6709, eval/r2: 0.4500, train/one: 0.8237, train/two: 0.4984, train/three: 0.6906, eval/one: 0.6883, eval/two: 0.3244, eval/three: 0.3372 \n",
      "Epoch: 65, train/loss: 0.1666, eval/loss: 0.2178, train/r2: 0.7434, eval/r2: 0.5504, train/one: 0.8576, train/two: 0.6645, train/three: 0.7083, eval/one: 0.7960, eval/two: 0.5380, eval/three: 0.3170 \n",
      "Epoch: 70, train/loss: 0.1750, eval/loss: 0.2399, train/r2: 0.7202, eval/r2: 0.5859, train/one: 0.8092, train/two: 0.6178, train/three: 0.7336, eval/one: 0.5910, eval/two: 0.6495, eval/three: 0.5171 \n",
      "Epoch: 75, train/loss: 0.1877, eval/loss: 0.2210, train/r2: 0.7440, eval/r2: 0.5685, train/one: 0.8255, train/two: 0.5889, train/three: 0.8175, eval/one: 0.7550, eval/two: 0.5616, eval/three: 0.3889 \n",
      "Epoch: 80, train/loss: 0.1802, eval/loss: 0.2285, train/r2: 0.7387, eval/r2: 0.4800, train/one: 0.8525, train/two: 0.6528, train/three: 0.7109, eval/one: 0.5994, eval/two: 0.1788, eval/three: 0.6618 \n",
      "Epoch: 85, train/loss: 0.1389, eval/loss: 0.2899, train/r2: 0.7497, eval/r2: 0.5601, train/one: 0.8675, train/two: 0.7154, train/three: 0.6662, eval/one: 0.5897, eval/two: 0.3474, eval/three: 0.7433 \n",
      "Epoch: 90, train/loss: 0.1571, eval/loss: 0.2457, train/r2: 0.7696, eval/r2: 0.3676, train/one: 0.8898, train/two: 0.6730, train/three: 0.7461, eval/one: 0.6593, eval/two: -0.1723, eval/three: 0.6158 \n",
      "Epoch: 95, train/loss: 0.1489, eval/loss: 0.1599, train/r2: 0.7427, eval/r2: 0.6170, train/one: 0.8487, train/two: 0.6612, train/three: 0.7180, eval/one: 0.7334, eval/two: 0.4566, eval/three: 0.6611 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 128 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 128 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-229/metadata\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "#inputs_mean_std = []\n",
    "#targets_mean_std = []\n",
    "scores = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "splits = kfold.split(inputs)\n",
    "\n",
    "for fold, (train_idx, eval_idx) in enumerate(splits):\n",
    "    MODEL_NAME = f\"repro.resnet.paper.finetune.on.best.pretrain.fold.with.its.mu.sigma.fold.{fold}\"\n",
    "    checkpoint_name = f\"repro.finetune.on.best.pretrain.fold.with.its.mu.sigma.fold.{fold}.pt\"\n",
    "    \n",
    "    train_inputs = inputs[train_idx]\n",
    "    train_targets = targets[train_idx]\n",
    "    eval_inputs = inputs[eval_idx]\n",
    "    eval_targets = targets[eval_idx]\n",
    "\n",
    "    train_ds = get_dataset(\n",
    "        train_inputs, \n",
    "        train_targets,\n",
    "        config, \n",
    "        (inputs_mean, inputs_std),\n",
    "        (targets_means, targets_stds)\n",
    "    )\n",
    "    \n",
    "    #inputs_mean_std.append((fold, train_ds.s_mean, train_ds.s_std))\n",
    "    #targets_mean_std.append((fold, train_ds.concentration_means, train_ds.concentration_stds))\n",
    "    \n",
    "    eval_ds = get_dataset(\n",
    "        eval_inputs, \n",
    "        eval_targets, \n",
    "        config, \n",
    "        (inputs_mean, inputs_std),\n",
    "        (targets_means, targets_stds)\n",
    "        #(train_ds.s_mean, train_ds.s_std), \n",
    "        #(train_ds.concentration_means, train_ds.concentration_stds)\n",
    "    )\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    train_dl, eval_dl = return_dls(train_ds, eval_ds, BATCH_SIZE, len(eval_ds))\n",
    "    \n",
    "    ckpt = get_ckpt(\"/kaggle/working/repro.paper.pretrain.fold.3.pt\")[\"state_dict\"]\n",
    "    \n",
    "    model = ReZeroNet(**config).to(device)\n",
    "    model.load_state_dict(ckpt)\n",
    "    \n",
    "    if fold == 0: print(get_model_size(model))\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "    scheduler = get_scheduler(optimizer, train_dl, EPOCHS)\n",
    "    \n",
    "    score = train(\n",
    "            model, \n",
    "            optimizer, \n",
    "            device,\n",
    "            torch.float16,\n",
    "            scheduler,\n",
    "            train_dl, \n",
    "            eval_dl,\n",
    "            mse_loss_function,\n",
    "            EPOCHS,\n",
    "            checkpoint_name,\n",
    "            neptune_run=setup_neptune(),\n",
    "        )\n",
    "    \n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c66906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralTestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra,\n",
    "        concentrations,\n",
    "        dtype=None,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=0.0,\n",
    "        baseline_factor_bound=0.0,\n",
    "        baseline_period_lower_bound=100.0,\n",
    "        baseline_period_upper_bound=200.0,\n",
    "        augment_slope_std=0.0,\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=0,\n",
    "        spectrum_rolling_sigma=0.0,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    ):\n",
    "        self.dtype = dtype or torch.float32\n",
    "        self.combine_spectra_range = combine_spectra_range\n",
    "        self.baseline_factor_bound = baseline_factor_bound\n",
    "        self.augment_slope_std = augment_slope_std\n",
    "        self.augment_intercept_std = augment_intersept_std\n",
    "        self.baseline_period_lower_bound = baseline_period_lower_bound\n",
    "        self.baseline_period_upper_bound = baseline_period_upper_bound\n",
    "        self.rolling_bound = rolling_bound\n",
    "        self.spectrum_rolling_sigma = spectrum_rolling_sigma\n",
    "        self.augmentation_weight = torch.tensor(augmentation_weight, dtype=dtype)\n",
    "        self.original_dp_weight = original_datapoint_weight\n",
    "\n",
    "        # normalize spectra\n",
    "        spectra = torch.tensor(spectra, dtype=dtype)\n",
    "\n",
    "        if spectra_mean_std is None:\n",
    "            self.s_mean = torch.mean(spectra)\n",
    "            self.s_std = torch.std(spectra)\n",
    "        else:\n",
    "            self.s_mean, self.s_std = spectra_mean_std\n",
    "\n",
    "        self.spectra = torch.divide(\n",
    "            torch.subtract(spectra, self.s_mean),\n",
    "            self.s_std,\n",
    "        )\n",
    "\n",
    "        self.dummy_wns = np.tile(\n",
    "            np.arange(\n",
    "                0., 1., 1. / self.spectra.shape[2],\n",
    "                dtype=np_dtype_from_torch[self.dtype]\n",
    "            )[None, :self.spectra.shape[2]],\n",
    "            (self.spectra.shape[1], 1),\n",
    "        )\n",
    "\n",
    "        if False:\n",
    "            # normalize concentrations\n",
    "            concentrations = torch.tensor(concentrations, dtype=dtype)\n",
    "            if concentration_mean_std is None:\n",
    "                self.concentration_means = torch.nanmean(concentrations, dim=0)\n",
    "\n",
    "                self.concentration_stds = torch.maximum(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            torch.std(col[torch.logical_not(torch.isnan(col))])\n",
    "                            for col in concentrations.T\n",
    "                        ]\n",
    "                    ),\n",
    "                    torch.tensor([1e-3] * concentrations.shape[1]),\n",
    "                )\n",
    "            else:\n",
    "                self.concentration_means = concentration_mean_std[0]\n",
    "                self.concentration_stds = concentration_mean_std[1]\n",
    "\n",
    "            self.concentrations = torch.divide(\n",
    "                torch.subtract(\n",
    "                    concentrations,\n",
    "                    self.concentration_means,\n",
    "                ),\n",
    "                self.concentration_stds,\n",
    "            )\n",
    "\n",
    "    def pick_two(self, max_idx=None):\n",
    "        max_idx = max_idx or len(self)\n",
    "        return random.choices(range(max_idx), k=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 96\n",
    "\n",
    "    def augment_spectra(self, spectra):\n",
    "        if self.augment_slope_std > 0.0:\n",
    "\n",
    "            def spectrum_approximation(x, slope, intercept):\n",
    "                return (slope * x + intercept).reshape(-1, 1)[:, 0]\n",
    "\n",
    "            slope, inter = scipy.optimize.curve_fit(\n",
    "                spectrum_approximation,\n",
    "                self.dummy_wns,\n",
    "                spectra.reshape(-1, 1)[:, 0],\n",
    "                p0=np.random.rand(2),\n",
    "            )[0]\n",
    "\n",
    "            new_slope = slope * (\n",
    "                    np.random.gamma(\n",
    "                        shape=1. / self.augment_slope_std,\n",
    "                        scale=self.augment_slope_std,\n",
    "                        size=1,\n",
    "                    )\n",
    "            )[0]\n",
    "            new_intercept = inter * (\n",
    "                1.0 + np.random.randn(1) * self.augment_intercept_std\n",
    "            )[0]\n",
    "            spectra += torch.tensor(\n",
    "                (new_slope - slope)\n",
    "            ) * self.dummy_wns + new_intercept - inter\n",
    "\n",
    "        factor = self.baseline_factor_bound * torch.rand(size=(1,))\n",
    "        offset = torch.rand(size=(1,)) * 2.0 * torch.pi\n",
    "        period = self.baseline_period_lower_bound + (\n",
    "            self.baseline_period_upper_bound - self.baseline_period_lower_bound\n",
    "        ) * torch.rand(size=(1,))\n",
    "        permutations = factor * torch.cos(\n",
    "            2.0 * torch.pi / period * self.dummy_wns + offset\n",
    "        )\n",
    "        return self.roll_spectrum(\n",
    "            spectra + permutations * spectra,\n",
    "            delta=random.randint(-self.rolling_bound, self.rolling_bound),\n",
    "        )\n",
    "\n",
    "    def roll_spectrum(self, spectra, delta):\n",
    "        num_spectra = spectra.shape[0]\n",
    "        rolled_spectra = np.roll(spectra, delta, axis=1)\n",
    "        if delta > 0:\n",
    "            rolled_spectra[:, :delta] = (\n",
    "                np.random.rand(num_spectra, delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta:(delta + 1)]\n",
    "        elif delta < 0:\n",
    "            rolled_spectra[:, delta:] = (\n",
    "                np.random.rand(num_spectra, -delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta - 1:delta]\n",
    "        return rolled_spectra\n",
    "\n",
    "    def combine_k_items(self, indices, weights):\n",
    "        return (\n",
    "            # spectra\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None, None], self.spectra[indices, :, :]),\n",
    "                dim=0,\n",
    "            ),\n",
    "            # concentrations\n",
    "            #torch.sum(\n",
    "            #    torch.mul(weights[:, None], self.concentrations[indices, :]),\n",
    "            #    dim=0,\n",
    "            #)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if True:#self.combine_spectra_range < 1e-12:\n",
    "            spectrum = self.spectra[idx]\n",
    "            #spectrum = self.augment_spectra(spectrum)\n",
    "            return spectrum\n",
    "        else:\n",
    "            if random.random() < self.original_dp_weight:\n",
    "                one_weight = 1.\n",
    "                label_weight = torch.tensor(1.0, dtype=self.dtype)\n",
    "            else:\n",
    "                one_weight = random.uniform(0.0, self.combine_spectra_range)\n",
    "                label_weight = self.augmentation_weight\n",
    "            weights = torch.tensor([one_weight, (1 - one_weight)])\n",
    "            # just pick two random indices\n",
    "            indices = random.choices(range(len(self)), k=2)\n",
    "\n",
    "            mixed_spectra = self.combine_k_items(\n",
    "                indices=indices,\n",
    "                weights=weights,\n",
    "            )\n",
    "            mixed_spectra = self.augment_spectra(mixed_spectra[0])\n",
    "            return mixed_spectra\n",
    "        \n",
    "  \n",
    "def get_test_dataset(inputs, inputs_mean_std, targets_mean_std):\n",
    "    return SpectralTestDataset(\n",
    "        spectra=inputs[:, None, :],\n",
    "        concentrations=None,\n",
    "        dtype=torch.float32,\n",
    "        spectra_mean_std=inputs_mean_std,\n",
    "        concentration_mean_std=targets_mean_std,\n",
    "        combine_spectra_range=1.0,\n",
    "        baseline_factor_bound=config[\"baseline_factor_bound\"],\n",
    "        baseline_period_lower_bound=config[\"baseline_period_lower_bound\"],\n",
    "        baseline_period_upper_bound=(config[\"baseline_period_lower_bound\"] + config[\"baseline_period_span\"]),\n",
    "        augment_slope_std=config[\"augment_slope_std\"],\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=config[\"rolling_bound\"],\n",
    "        spectrum_rolling_sigma=0.01,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/repro.finetune.on.best.pretrain.fold.with.its.mu.sigma.fold.0.pt 68 0.7357055804453166\n",
      "/kaggle/working/repro.finetune.on.best.pretrain.fold.with.its.mu.sigma.fold.1.pt 88 0.7661680598895187\n",
      "/kaggle/working/repro.finetune.on.best.pretrain.fold.with.its.mu.sigma.fold.2.pt 88 0.7143326037131904\n",
      "/kaggle/working/repro.finetune.on.best.pretrain.fold.with.its.mu.sigma.fold.3.pt 96 0.7213412419488799\n",
      "/kaggle/working/repro.finetune.on.best.pretrain.fold.with.its.mu.sigma.fold.4.pt 71 0.6364491276006955\n"
     ]
    }
   ],
   "source": [
    "ckpt_paths = get_ckpt_paths(\"/kaggle/working/\", \"with.its.mu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42275738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(0.2364), tensor(0.1764)),\n",
       " (tensor([3.0587, 0.6427, 1.0438]), tensor([3.8679, 0.5630, 1.3215])))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs_mean_std = (inputs_mean, inputs_std)\n",
    "targets_mean_std = (targets_means, targets_stds)\n",
    "inputs_mean_std, targets_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93b4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.00982247,  1.39125497,  0.53356552],\n",
       "       [ 6.63028809,  1.80872054,  1.13359293],\n",
       "       [ 6.22267747,  0.27224768,  1.13458649],\n",
       "       [ 3.60593056,  0.52966372,  0.53291309],\n",
       "       [ 9.81596398,  1.12146247,  0.82346534],\n",
       "       [ 9.46179473,  1.68650149,  1.05576643],\n",
       "       [ 3.59212936,  0.88102133,  0.79815045],\n",
       "       [ 7.75197967,  1.84324387,  0.93168656],\n",
       "       [ 4.97660074,  1.51244423,  1.3385063 ],\n",
       "       [ 9.50430567,  0.56630862,  0.41033949],\n",
       "       [ 9.16543548,  0.88228763,  1.35785662],\n",
       "       [ 1.02511341,  1.13835696,  1.08100935],\n",
       "       [ 2.23051188,  1.50250885,  1.31818087],\n",
       "       [ 4.19139572,  1.17473906,  1.58882706],\n",
       "       [ 3.83402349,  1.12676904,  1.30375012],\n",
       "       [ 7.67528838,  1.14842805,  0.63733038],\n",
       "       [ 3.36627636,  1.41148447,  1.30162627],\n",
       "       [ 7.51511203,  0.81043116,  1.82874645],\n",
       "       [ 6.94564138,  1.52738447,  0.93329847],\n",
       "       [ 0.86857394,  1.02190374,  1.51624954],\n",
       "       [ 4.86469069,  1.31175499,  0.8180182 ],\n",
       "       [ 4.66713607,  1.19408495,  1.50601431],\n",
       "       [ 1.25203839,  1.42186766,  0.89938641],\n",
       "       [ 5.66454602,  1.16748976,  1.59452814],\n",
       "       [ 2.11118964,  1.4924191 ,  1.33197723],\n",
       "       [ 2.87445876,  0.98068708,  1.57323425],\n",
       "       [ 6.79462188,  1.25808955,  0.92874353],\n",
       "       [ 2.51242883,  1.25052257,  1.76303176],\n",
       "       [ 4.68977493,  1.33046674,  1.02877608],\n",
       "       [ 4.86234696,  0.68571396,  0.59099924],\n",
       "       [ 5.51282405,  0.87879618,  2.35765473],\n",
       "       [ 6.73409881,  1.16024559,  1.00011622],\n",
       "       [ 2.92841921,  1.15016111,  1.61666954],\n",
       "       [ 9.37112171,  1.01207733,  0.92681781],\n",
       "       [ 7.91259498,  1.33386843,  1.19799935],\n",
       "       [ 7.0480815 ,  0.4620272 ,  1.18615757],\n",
       "       [ 7.53516773,  1.20316202,  0.84101966],\n",
       "       [ 5.41917928,  1.54722683,  0.21542491],\n",
       "       [ 0.35330039,  1.04633708,  0.73753099],\n",
       "       [ 8.29342509,  1.11471504,  1.28424481],\n",
       "       [ 6.4673113 ,  0.90039442,  1.46180769],\n",
       "       [ 2.75913908,  1.17010415,  0.40783479],\n",
       "       [ 8.5381516 ,  0.2651464 ,  1.73453417],\n",
       "       [ 1.40369258,  0.90466521,  1.18732435],\n",
       "       [ 2.49309099,  1.17264041,  0.63984579],\n",
       "       [ 6.12468598,  0.80802541,  1.9212475 ],\n",
       "       [ 1.28094627,  0.44779614,  1.9344555 ],\n",
       "       [ 2.13725627,  0.87706224,  1.12172065],\n",
       "       [ 2.87552801,  1.29745599,  1.80945806],\n",
       "       [ 6.73669938,  0.65918988,  1.51863652],\n",
       "       [ 1.15100287,  1.28537612,  1.81469579],\n",
       "       [-0.77729397,  0.84022978,  1.92946517],\n",
       "       [ 0.45178156,  0.56321082,  1.64438922],\n",
       "       [ 3.6189256 ,  1.84319165,  0.64222772],\n",
       "       [ 0.56279064,  1.06558384,  1.55247349],\n",
       "       [ 5.35922543,  1.5890245 ,  1.39563351],\n",
       "       [ 5.30999406,  1.72388674,  1.48670914],\n",
       "       [ 3.72196486,  1.13296777,  1.67143579],\n",
       "       [ 5.78738581,  0.39032266,  1.83168314],\n",
       "       [ 4.20170669,  0.54717694,  1.82469157],\n",
       "       [ 2.56157074,  0.35302535,  1.68040554],\n",
       "       [ 4.07828338,  1.70042483,  1.4329306 ],\n",
       "       [ 2.46900262,  1.26491675,  1.01120254],\n",
       "       [ 9.3599568 ,  1.13980464,  1.95939255],\n",
       "       [ 8.05074122,  1.72748347,  0.67295968],\n",
       "       [10.05300961,  1.08320546,  0.57485455],\n",
       "       [11.20822795,  0.88313342,  0.89447967],\n",
       "       [ 2.30186412,  1.16789904,  0.58528618],\n",
       "       [ 6.66306328,  1.09202096,  0.97316925],\n",
       "       [ 8.90741658,  1.41688984,  1.26838316],\n",
       "       [ 9.30200432,  1.09333088,  1.29604003],\n",
       "       [ 7.67612941,  1.39941635,  1.39971451],\n",
       "       [ 5.17954877,  1.39690854,  1.98712578],\n",
       "       [ 8.47845481,  0.86747025,  1.17704449],\n",
       "       [ 5.58738902,  0.93900506,  1.96902056],\n",
       "       [ 9.56268466,  0.47756995,  1.06614425],\n",
       "       [ 4.07185136,  0.821757  ,  0.79028916],\n",
       "       [ 5.87136584,  0.88290725,  1.92090139],\n",
       "       [ 3.88464759,  1.74760296,  0.85229444],\n",
       "       [ 6.5782738 ,  0.76226671,  2.20230976],\n",
       "       [ 7.51943293,  0.65027075,  0.6838842 ],\n",
       "       [ 0.78842444,  1.1211502 ,  1.8562893 ],\n",
       "       [ 6.62418115,  0.67141249,  1.6468615 ],\n",
       "       [ 4.41055071,  1.24131926,  1.42665604],\n",
       "       [ 5.03280798,  0.76281906,  1.73748686],\n",
       "       [ 0.80120594,  1.04233422,  1.71987724],\n",
       "       [ 4.29965195,  1.26897008,  0.7711178 ],\n",
       "       [ 1.11220581,  1.02290823,  1.52267785],\n",
       "       [ 6.830808  ,  1.50258382,  1.51854968],\n",
       "       [ 3.02641045,  1.26689855,  2.02173425],\n",
       "       [ 4.90594515,  1.2706518 ,  1.29411924],\n",
       "       [ 1.55005227,  1.06125099,  1.89978906],\n",
       "       [ 6.66143977,  0.30168093,  1.75039065],\n",
       "       [ 4.04368422,  0.88168065,  1.37644563],\n",
       "       [ 1.19163854,  1.57301103,  1.16312203],\n",
       "       [ 4.24556459,  1.45609184,  1.07124312]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def inference(ckpt_name, i):\n",
    "    ckpt = get_ckpt(ckpt_name)\n",
    "    \n",
    "    test_inputs = get_test_data()\n",
    "    test_ds = get_test_dataset(test_inputs, inputs_mean_std, targets_mean_std) #[i][1:]\n",
    "    test_dl = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    \n",
    "    model = ReZeroNet(**config).to(device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    for inputs in test_dl:\n",
    "        with torch.inference_mode():\n",
    "            preds = model(inputs.cuda())\n",
    "            preds = preds.double() \n",
    "            all_preds.append(cuda_to_np(preds))\n",
    "            \n",
    "    preds = np.concatenate(all_preds)\n",
    "    mus = targets_mean_std[0] #[i][1:][0]\n",
    "    sigmas = targets_mean_std[1] #[i][1:][1]\n",
    "\n",
    "    for i in range(3):\n",
    "        preds[:, i] = reverse_zscore(preds[:, i], mus[i].numpy(), sigmas[i].numpy())\n",
    "    \n",
    "    return preds\n",
    "\n",
    "preds = inference(\"/kaggle/working/repro.finetune.on.best.pretrain.fold.with.its.mu.sigma.fold.1.pt\", 2)\n",
    "generate_csv(preds, \"repro.finetune.on.best.pretrain.fold.with.its.mu.sigma.fold.1.csv\")\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_inference(ckpt_paths):\n",
    "    test_inputs = get_test_data()\n",
    "    all_preds = []\n",
    "\n",
    "    for i, ckpt_path in enumerate(ckpt_paths):\n",
    "        ckpt = get_ckpt(ckpt_path)\n",
    "        \n",
    "        model = ReZeroNet(**config).to(device)\n",
    "        model.load_state_dict(ckpt[\"state_dict\"])\n",
    "        model.eval()\n",
    "\n",
    "        test_ds = get_test_dataset(test_inputs, inputs_mean_std[i][1:], targets_mean_std[i][1:])\n",
    "        test_dl = DataLoader(test_ds, batch_size=32)\n",
    "        \n",
    "        fold_preds = []\n",
    "        for inputs in test_dl:\n",
    "            with torch.inference_mode():\n",
    "                preds = model(inputs.cuda())\n",
    "                preds = cuda_to_np(preds.double())\n",
    "                fold_preds.append(preds)\n",
    "                \n",
    "        fold_preds = np.concatenate(fold_preds)\n",
    "        \n",
    "        means = targets_mean_std[i][1:][0]\n",
    "        stds = targets_mean_std[i][1:][1]\n",
    "        for i in range(3):\n",
    "            fold_preds[:, i] = reverse_zscore(fold_preds[:, i], means[i].numpy(), stds[i].numpy())\n",
    "            \n",
    "        all_preds.append(fold_preds)\n",
    "\n",
    "    return np.mean(all_preds, axis=0)\n",
    "\n",
    "preds = ensemble_inference(ckpt_paths)\n",
    "generate_csv(preds, \"paper.finetune.avg.pretrain.weights.ensemble.csv\")\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
