{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5238551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This file explains every line using simple words.\n",
    "# To actually run it, you also need:\n",
    "#   import numpy as np\n",
    "#   import os\n",
    "#   DATA_PATH = \"/path/to/folder/with/csvs\"\n",
    "# Each CSV is expected to have:\n",
    "#   - Many spectral columns (wavenumber columns) first\n",
    "#   - Then 4 label columns next (positions -5:-1)\n",
    "#   - Then 1 column at the very end with cross-validation fold index (position -1)\n",
    "\n",
    "import pandas as pd  # Load the pandas library and call it \"pd\" to read and handle CSV files\n",
    "\n",
    "# List of device/dataset names we plan to use\n",
    "dataset_names = ['anton_532', 'anton_785', 'kaiser', 'mettler_toledo', 'metrohm', 'tec5', 'timegate', 'tornado']\n",
    "\n",
    "# For each dataset, define the smallest wavenumber we allow\n",
    "lower_bounds = {\n",
    "    'anton_532': 200,\n",
    "    'anton_785': 100,\n",
    "    'kaiser': -37,\n",
    "    'mettler_toledo': 300,\n",
    "    'metrohm': 200,\n",
    "    'tec5': 85,\n",
    "    'timegate': 200,\n",
    "    'tornado': 300,\n",
    "}\n",
    "\n",
    "# For each dataset, define the largest wavenumber we allow\n",
    "upper_bounds = {\n",
    "    'anton_532': 3500,\n",
    "    'anton_785': 2300,\n",
    "    'kaiser': 1942,\n",
    "    'mettler_toledo': 3350,\n",
    "    'metrohm': 3350,\n",
    "    'tec5': 3210,\n",
    "    'timegate': 2000,\n",
    "    'tornado': 3300,\n",
    "}\n",
    "\n",
    "def get_csv_dataset(\n",
    "    dataset_name,      # text name of the dataset (must match a CSV file name)\n",
    "    lower_wn=-1000,    # requested minimum wavenumber (we will clamp to device-specific bounds)\n",
    "    upper_wn=10000,    # requested maximum wavenumber (we will clamp to device-specific bounds)\n",
    "    dtype=None,        # desired numeric dtype, default will be np.float64\n",
    "):\n",
    "    # Make sure the lower bound is not smaller than what the device supports\n",
    "    lower_wn = max(lower_wn, lower_bounds[dataset_name])\n",
    "    # Make sure the upper bound is not larger than what the device supports\n",
    "    upper_wn = min(upper_wn, upper_bounds[dataset_name])\n",
    "    # If dtype is not given, use np.float64 (NOTE: np must be imported)\n",
    "    dtype = dtype or np.float64\n",
    "\n",
    "    # Read the CSV file for this dataset from the DATA_PATH folder\n",
    "    # The file is expected to be named \"<dataset_name>.csv\"\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(\n",
    "            DATA_PATH,\n",
    "            '%s.csv' % dataset_name,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Figure out which spectral columns (by wavenumber) fall inside [lower_wn, upper_wn]\n",
    "    # df.columns[:-5] are the spectral columns (strings); convert to float wavenumbers\n",
    "    spectra_selection = np.logical_and(\n",
    "        lower_wn <= np.array([float(one) for one in df.columns[:-5]]),\n",
    "        np.array([float(one) for one in df.columns[:-5]]) <= upper_wn,\n",
    "    )\n",
    "\n",
    "    # Pull out the spectral matrix (all rows, spectral columns only, then slice columns we selected)\n",
    "    # .values gives us a NumPy array\n",
    "    spectra = df.iloc[:, :-5].iloc[:, spectra_selection].values\n",
    "    # Pull out the label matrix (the 4 columns right before the last column)\n",
    "    label = df.iloc[:, -5:-1].values\n",
    "    # Pull out the last column as cross-validation fold indices\n",
    "    cv_indices = df.iloc[:, -1].values\n",
    "    # All row indices as a NumPy array, e.g., [0, 1, 2, ..., N-1]\n",
    "    all_indices = np.array(range(len(cv_indices)))\n",
    "\n",
    "    # Build cross-validation folds:\n",
    "    # For each unique fold id, create a pair (train_indices, val_indices)\n",
    "    cv_folds = [\n",
    "        (\n",
    "            # training indices are all rows not equal to this fold id\n",
    "            all_indices[cv_indices != fold_idx],\n",
    "            # validation indices are all rows equal to this fold id\n",
    "            all_indices[cv_indices == fold_idx],\n",
    "        )\n",
    "        # range(len(set(cv_indices))) assumes fold ids are 0..K-1 without gaps\n",
    "        for fold_idx in range(len(set(cv_indices)))\n",
    "    ]\n",
    "    \n",
    "    # Save the wavenumbers (as floats) for the selected spectral columns\n",
    "    wavenumbers = np.array([\n",
    "        float(one) for one in df.columns[:-5]\n",
    "    ])[spectra_selection]\n",
    "\n",
    "    # Return:\n",
    "    # - spectra: spectral matrix (num_samples x num_selected_wavenumbers)\n",
    "    # - label: label matrix (num_samples x 4)\n",
    "    # - None: placeholder (could be for extra metadata)\n",
    "    # - cv_folds: list of (train_idx, val_idx) pairs\n",
    "    # - wavenumbers: the wavenumber axis corresponding to spectra columns\n",
    "    return (\n",
    "        spectra.astype(dtype),\n",
    "        label.astype(dtype),\n",
    "        None,\n",
    "        cv_folds,\n",
    "        wavenumbers.astype(dtype)\n",
    "    )\n",
    "\n",
    "def load_joint_dataset(\n",
    "    dataset_names,          # list of dataset names to load and merge\n",
    "    lower_wn=-1000,         # requested min wavenumber across all devices\n",
    "    upper_wn=10000,         # requested max wavenumber across all devices\n",
    "    dtype=None,             # desired numeric dtype, default np.float64\n",
    "    leave_out_one_device=False,  # if True, each device becomes its own validation split\n",
    "):\n",
    "    # If dtype is not given, use np.float64 (NOTE: np must be imported)\n",
    "    dtype = dtype or np.float64\n",
    "\n",
    "    # Compute the common wavenumber window that all devices share:\n",
    "    # the max of all lower bounds\n",
    "    lower_wn = max(\n",
    "        lower_wn,\n",
    "        *[lower_bounds[name] for name in dataset_names])\n",
    "    # and the min of all upper bounds\n",
    "    upper_wn = min(\n",
    "        upper_wn,\n",
    "        *[upper_bounds[name] for name in dataset_names]\n",
    "    )\n",
    "\n",
    "    # Print the final chosen wavenumber limits\n",
    "    print(\"Lower WN: \", lower_wn)\n",
    "    print(\"Upper WN: \", upper_wn)\n",
    "\n",
    "    # Load each dataset using the same clamped window and dtype\n",
    "    datasets = [\n",
    "        get_csv_dataset(\n",
    "            dataset_name,\n",
    "            lower_wn=lower_wn,\n",
    "            upper_wn=upper_wn,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        for dataset_name in dataset_names\n",
    "    ]\n",
    "\n",
    "    # Create a common integer wavenumber grid from lower_wn to upper_wn inclusive\n",
    "    joint_wns = np.arange(lower_wn, upper_wn + 1)\n",
    "    print(\"Joint WNS: \", joint_wns)\n",
    "    \n",
    "    # For each dataset, interpolate each spectrum onto the common wavenumber grid\n",
    "    # np.interp expects:\n",
    "    #   xp = original wavenumbers (wns)\n",
    "    #   fp = original spectrum values\n",
    "    #   joint_wns = new x positions to sample at\n",
    "    interpolated_data = [\n",
    "        np.array([\n",
    "            np.interp(\n",
    "                joint_wns,\n",
    "                xp=wns,\n",
    "                fp=spectrum,\n",
    "            )\n",
    "            for spectrum in spectra\n",
    "        ])\n",
    "        for spectra, _, _, _, wns in datasets\n",
    "    ]\n",
    "    \n",
    "    # Normalize each spectrum by its own maximum, then stack all datasets together\n",
    "    normed_spectra = np.concatenate(\n",
    "        [\n",
    "            spectra / np.max(spectra)\n",
    "            for spectra in interpolated_data\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    \n",
    "    # Compute starting index offsets for each dataset in the stacked array\n",
    "    # Example: [0, N0, N0+N1, N0+N1+N2, ...]\n",
    "    dataset_offsets = np.concatenate(\n",
    "        [\n",
    "            [0],\n",
    "            np.cumsum([len(one[0]) for one in datasets])[:-1]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Total number of spectra across all datasets\n",
    "    num_items = sum((len(one[0]) for one in datasets))\n",
    "    if leave_out_one_device:\n",
    "        # Validation indices are entire device blocks (leave-one-device-out CV)\n",
    "        val_indices = [\n",
    "            np.arange(start, end, 1)\n",
    "            for start, end in zip(\n",
    "                dataset_offsets,\n",
    "                np.concatenate([dataset_offsets[1:], np.array([num_items])])\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        # Otherwise, use the per-row CV folds inside each dataset,\n",
    "        # but shift (offset) them to match the global stacked indices\n",
    "        val_indices = [\n",
    "            val_idxs + offset\n",
    "            for one, offset in zip(datasets, dataset_offsets)\n",
    "            for train_idxs, val_idxs in one[3]\n",
    "        ]\n",
    "\n",
    "    # All global indices as a set for easy subtraction\n",
    "    all_indices = set(range(num_items))\n",
    "\n",
    "    # Build final CV folds as (train_indices, val_indices) pairs in global indexing\n",
    "    cv_folds = [\n",
    "        (np.array(list(all_indices - set(val_idxs))), val_idxs)\n",
    "        for val_idxs in val_indices\n",
    "    ]\n",
    "    return (\n",
    "        # Stacked, interpolated, per-spectrum-max-normalized spectra\n",
    "        normed_spectra,\n",
    "        # Stacked labels; only first 3 columns kept here (note: source had 4)\n",
    "        np.concatenate([one[1] for one in datasets])[:, :3],\n",
    "        # Cross-validation folds in global indices\n",
    "        cv_folds,\n",
    "        # Boundaries where each dataset starts in the stacked array (length = num_datasets + 1)\n",
    "        np.concatenate(\n",
    "            [\n",
    "                [0],\n",
    "                np.cumsum([len(one[0]) for one in datasets])\n",
    "            ]\n",
    "        ),\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
