{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7e06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -q  neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627d3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "SEED = 7031\n",
    "setup_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc8cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def average_state_dicts(state_dict_list):\n",
    "    n = len(state_dict_list)\n",
    "    # Ensure we don't modify the originals\n",
    "    avg_sd = OrderedDict()\n",
    "\n",
    "    # Iterate over every parameter/buffer key\n",
    "    for k in state_dict_list[0]:\n",
    "        # sum across models â†’ float32 to avoid overflow on int types\n",
    "        avg = sum(sd[k].float() for sd in state_dict_list) / n\n",
    "        # cast back to original dtype if needed\n",
    "        avg_sd[k] = avg.to(dtype=state_dict_list[0][k].dtype)\n",
    "\n",
    "    return avg_sd\n",
    "\n",
    "\n",
    "def cuda_to_np(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, train_dl, epochs):\n",
    "    total_training_steps = len(train_dl) * epochs\n",
    "    warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "\n",
    "\n",
    "def get_stats(tensor, p=True, r=False, minmax=False):\n",
    "    if minmax:\n",
    "        min, max = tensor.min(), tensor.max()\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Min: {min}, Max: {max} ,Mean: {mean}, Std: {std}\")\n",
    "        if r: return min, max, mean, std\n",
    "    else:\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Mean: {mean}, Std: {std}\")\n",
    "        if r: return mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-8)\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "    \n",
    "\n",
    "def get_index(iterable):\n",
    "    return random.randint(0, len(iterable) - 1)\n",
    "\n",
    "\n",
    "def get_indices(iterable, n):\n",
    "    return random.sample(range(len(iterable)), n)\n",
    "\n",
    "\n",
    "def split(inputs, targets, seed):\n",
    "    return train_test_split(\n",
    "        inputs,\n",
    "        targets, \n",
    "        test_size=0.2,\n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    ) \n",
    "\n",
    "\n",
    "def show_waves(waves, dpi=100):\n",
    "    \"\"\"\n",
    "    waves: numpy array of shape (3, N)\n",
    "    Creates three separate figures that stretch wide.\n",
    "    \"\"\"\n",
    "    N = waves.shape[1]\n",
    "    t = np.arange(N)\n",
    "\n",
    "    # Wide aspect ratio; height modest so each window fills width\n",
    "    for i in range(waves.shape[0]):\n",
    "        fig = plt.figure(figsize=(14, 4), dpi=dpi)  # wide figure\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(t, waves[i], linewidth=1)\n",
    "        ax.set_title(f\"Wave {i+1}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()  # reduce margins to use width\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def hf_ds_download(hf_token, repo_id):\n",
    "    login(hf_token[1:])\n",
    "    return snapshot_download(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "def get_spectra_features(X, b=False):\n",
    "    \"\"\"Create multi-channel features from spectra: raw, 1st derivative, 2nd derivative.\"\"\"\n",
    "    X_processed = np.zeros_like(X)\n",
    "    # Baseline correction and SNV\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        poly = np.polyfit(np.arange(X.shape[1]), X[i], 3)\n",
    "        baseline = np.polyval(poly, np.arange(X.shape[1]))\n",
    "        corrected_spec = X[i] - baseline\n",
    "        #X_processed[i] = (corrected_spec - corrected_spec.mean()) / (corrected_spec.std() + 1e-8)\n",
    "        X_processed[i] = corrected_spec\n",
    "        \n",
    "    # Calculate derivatives\n",
    "    deriv1 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=1, axis=1)\n",
    "    deriv2 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=2, axis=1)\n",
    "\n",
    "    if b: return np.stack([X_processed, deriv1, deriv2], axis=1)\n",
    "    return np.stack([deriv1, deriv2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c619231b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'sample_submission.csv'),\n",
       " (1, 'timegate.csv'),\n",
       " (2, 'mettler_toledo.csv'),\n",
       " (3, 'kaiser.csv'),\n",
       " (4, 'anton_532.csv'),\n",
       " (5, 'transfer_plate.csv'),\n",
       " (6, '96_samples.csv'),\n",
       " (7, 'tornado.csv'),\n",
       " (8, 'tec5.csv'),\n",
       " (9, 'metrohm.csv'),\n",
       " (10, 'anton_785.csv')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1423ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_transfer_data():\n",
    "    csv_path = os.path.join(path, files[5])\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    input_cols = df.columns[1:2049]\n",
    "    target_cols = df.columns[2050:]\n",
    "\n",
    "    targets  = df[target_cols].dropna().to_numpy()\n",
    "\n",
    "    df = df[input_cols]\n",
    "    df['Unnamed: 1'] = df['Unnamed: 1'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "    df['Unnamed: 2048'] = df['Unnamed: 2048'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "\n",
    "    inputs = df.to_numpy().reshape(-1, 2, 2048)\n",
    "    inputs = inputs.mean(axis=1)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def preprocess_transfer_data():\n",
    "    inputs, targets = load_transfer_data()\n",
    "    \n",
    "    spectra_selection = np.logical_and(\n",
    "        300 <= np.array([float(one) for one in range(2048)]),\n",
    "        np.array([float(one) for one in range(2048)]) <= 1942,\n",
    "    )\n",
    "    \n",
    "    inputs = inputs[:, spectra_selection]\n",
    "    \n",
    "    wns = np.array([\n",
    "        float(one) for one in range(2048)\n",
    "    ])[spectra_selection]\n",
    "    wavenumbers = np.arange(300, 1943)\n",
    "    \n",
    "    interpolated_data = np.array(\n",
    "        [np.interp(wavenumbers, xp=wns, fp=i) for i in inputs]\n",
    "    )\n",
    "    \n",
    "    normed_spectra = interpolated_data / np.max(interpolated_data)\n",
    "    return normed_spectra, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02fcb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "lower_bounds = {\n",
    "    'anton_532': 200,\n",
    "    'anton_785': 100,\n",
    "    'kaiser': -37,\n",
    "    'mettler_toledo': 300,\n",
    "    'metrohm': 200,\n",
    "    'tec5': 85,\n",
    "    'timegate': 200,\n",
    "    'tornado': 300,\n",
    "}\n",
    "\n",
    "\n",
    "upper_bounds = {\n",
    "    'anton_532': 3500,\n",
    "    'anton_785': 2300,\n",
    "    'kaiser': 1942,\n",
    "    'mettler_toledo': 3350,\n",
    "    'metrohm': 3350,\n",
    "    'tec5': 3210,\n",
    "    'timegate': 2000,\n",
    "    'tornado': 3300,\n",
    "}\n",
    "\n",
    "def get_dataset(name, lower=-1000, upper=10000):\n",
    "    path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "    df = pd.read_csv(os.path.join(path, name))\n",
    "\n",
    "    lower = max(lower, lower_bounds[name[:-4]])\n",
    "    upper = min(upper, upper_bounds[name[:-4]])\n",
    "\n",
    "    spectra_selection = np.logical_and(\n",
    "        lower <= np.array([float(one) for one in df.columns[:-5]]),\n",
    "        np.array([float(one) for one in df.columns[:-5]]) <= upper,\n",
    "    )\n",
    "    \n",
    "    spectra = df.iloc[:, :-5].iloc[:, spectra_selection].values\n",
    "    label = df.iloc[:, -5:-2].values\n",
    "\n",
    "    wavenumbers = np.array([\n",
    "        float(one) for one in df.columns[:-5]\n",
    "    ])[spectra_selection]\n",
    "\n",
    "    #indices = get_indices(spectra, num_samples)                         \n",
    "    return spectra, label, wavenumbers\n",
    "\n",
    "\n",
    "def load_datasets(ds_names, lower=-1000, upper=10000):\n",
    "        \n",
    "    lower = max(\n",
    "        lower,\n",
    "        *[lower_bounds[n[:-4]] for n in ds_names])\n",
    "    \n",
    "    upper = min(\n",
    "        upper,\n",
    "        *[upper_bounds[n[:-4]] for n in ds_names]\n",
    "    )\n",
    "\n",
    "    datasets = [get_dataset(name, lower, upper) for name in ds_names]\n",
    "    wavenumbers = np.arange(lower, upper + 1)\n",
    "\n",
    "    interpolated_data = [\n",
    "        np.array([\n",
    "            np.interp(\n",
    "                wavenumbers,\n",
    "                xp=wns,\n",
    "                fp=spectrum,\n",
    "            )\n",
    "            for spectrum in spectra\n",
    "        ])\n",
    "        for spectra, _, wns in datasets\n",
    "    ]\n",
    "\n",
    "    normed_spectra = np.concatenate(\n",
    "        [\n",
    "            spectra / np.max(spectra)\n",
    "            for spectra in interpolated_data\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    labels = np.concatenate([ds[1] for ds in datasets])\n",
    "    return normed_spectra, labels\n",
    "\n",
    "ds_names = [\"anton_532.csv\", \"anton_785.csv\", \"kaiser.csv\", \"mettler_toledo.csv\", \"metrohm.csv\", \"tornado.csv\", \"tec5.csv\", \"timegate.csv\"]\n",
    "inputs, targets = load_datasets(ds_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b83eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "np_dtype_from_torch = {\n",
    "    torch.float32: np.float32,\n",
    "    torch.float64: np.float64,\n",
    "}\n",
    "\n",
    "class SpectralDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra,\n",
    "        concentrations,\n",
    "        dtype=None,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=0.0,\n",
    "        baseline_factor_bound=0.0,\n",
    "        baseline_period_lower_bound=100.0,\n",
    "        baseline_period_upper_bound=200.0,\n",
    "        augment_slope_std=0.0,\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=0,\n",
    "        spectrum_rolling_sigma=0.0,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    ):\n",
    "        self.dtype = dtype or torch.float32\n",
    "        self.combine_spectra_range = combine_spectra_range\n",
    "        self.baseline_factor_bound = baseline_factor_bound\n",
    "        self.augment_slope_std = augment_slope_std\n",
    "        self.augment_intercept_std = augment_intersept_std\n",
    "        self.baseline_period_lower_bound = baseline_period_lower_bound\n",
    "        self.baseline_period_upper_bound = baseline_period_upper_bound\n",
    "        self.rolling_bound = rolling_bound\n",
    "        self.spectrum_rolling_sigma = spectrum_rolling_sigma\n",
    "        self.augmentation_weight = torch.tensor(augmentation_weight, dtype=dtype)\n",
    "        self.original_dp_weight = original_datapoint_weight\n",
    "\n",
    "        # normalize spectra\n",
    "        spectra = torch.tensor(spectra, dtype=dtype)\n",
    "\n",
    "        if spectra_mean_std is None:\n",
    "            self.s_mean = torch.mean(spectra)\n",
    "            self.s_std = torch.std(spectra)\n",
    "        else:\n",
    "            self.s_mean, self.s_std = spectra_mean_std\n",
    "\n",
    "        self.spectra = torch.divide(\n",
    "            torch.subtract(spectra, self.s_mean),\n",
    "            self.s_std,\n",
    "        )\n",
    "\n",
    "        self.dummy_wns = np.tile(\n",
    "            np.arange(\n",
    "                0., 1., 1. / self.spectra.shape[2],\n",
    "                dtype=np_dtype_from_torch[self.dtype]\n",
    "            )[None, :self.spectra.shape[2]],\n",
    "            (self.spectra.shape[1], 1),\n",
    "        )\n",
    "\n",
    "        # normalize concentrations\n",
    "        concentrations = torch.tensor(concentrations, dtype=dtype)\n",
    "        if concentration_mean_std is None:\n",
    "            self.concentration_means = torch.nanmean(concentrations, dim=0)\n",
    "\n",
    "            self.concentration_stds = torch.maximum(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        torch.std(col[torch.logical_not(torch.isnan(col))])\n",
    "                        for col in concentrations.T\n",
    "                    ]\n",
    "                ),\n",
    "                torch.tensor([1e-3] * concentrations.shape[1]),\n",
    "            )\n",
    "        else:\n",
    "            self.concentration_means = concentration_mean_std[0]\n",
    "            self.concentration_stds = concentration_mean_std[1]\n",
    "\n",
    "        self.concentrations = torch.divide(\n",
    "            torch.subtract(\n",
    "                concentrations,\n",
    "                self.concentration_means,\n",
    "            ),\n",
    "            self.concentration_stds,\n",
    "        )\n",
    "\n",
    "    def pick_two(self, max_idx=None):\n",
    "        max_idx = max_idx or len(self)\n",
    "        return random.choices(range(max_idx), k=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.concentrations)\n",
    "\n",
    "    def augment_spectra(self, spectra):\n",
    "        if self.augment_slope_std > 0.0:\n",
    "\n",
    "            def spectrum_approximation(x, slope, intercept):\n",
    "                return (slope * x + intercept).reshape(-1, 1)[:, 0]\n",
    "\n",
    "            slope, inter = scipy.optimize.curve_fit(\n",
    "                spectrum_approximation,\n",
    "                self.dummy_wns,\n",
    "                spectra.reshape(-1, 1)[:, 0],\n",
    "                p0=np.random.rand(2),\n",
    "            )[0]\n",
    "\n",
    "            new_slope = slope * (\n",
    "                    np.random.gamma(\n",
    "                        shape=1. / self.augment_slope_std,\n",
    "                        scale=self.augment_slope_std,\n",
    "                        size=1,\n",
    "                    )\n",
    "            )[0]\n",
    "            new_intercept = inter * (\n",
    "                1.0 + np.random.randn(1) * self.augment_intercept_std\n",
    "            )[0]\n",
    "            spectra += torch.tensor(\n",
    "                (new_slope - slope)\n",
    "            ) * self.dummy_wns + new_intercept - inter\n",
    "\n",
    "        factor = self.baseline_factor_bound * torch.rand(size=(1,))\n",
    "        offset = torch.rand(size=(1,)) * 2.0 * torch.pi\n",
    "        period = self.baseline_period_lower_bound + (\n",
    "            self.baseline_period_upper_bound - self.baseline_period_lower_bound\n",
    "        ) * torch.rand(size=(1,))\n",
    "        permutations = factor * torch.cos(\n",
    "            2.0 * torch.pi / period * self.dummy_wns + offset\n",
    "        )\n",
    "        return self.roll_spectrum(\n",
    "            spectra + permutations * spectra,\n",
    "            delta=random.randint(-self.rolling_bound, self.rolling_bound),\n",
    "        )\n",
    "\n",
    "    def roll_spectrum(self, spectra, delta):\n",
    "        num_spectra = spectra.shape[0]\n",
    "        rolled_spectra = np.roll(spectra, delta, axis=1)\n",
    "        if delta > 0:\n",
    "            rolled_spectra[:, :delta] = (\n",
    "                np.random.rand(num_spectra, delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta:(delta + 1)]\n",
    "        elif delta < 0:\n",
    "            rolled_spectra[:, delta:] = (\n",
    "                np.random.rand(num_spectra, -delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta - 1:delta]\n",
    "        return rolled_spectra\n",
    "\n",
    "    def combine_k_items(self, indices, weights):\n",
    "        return (\n",
    "            # spectra\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None, None], self.spectra[indices, :, :]),\n",
    "                dim=0,\n",
    "            ),\n",
    "            # concentrations\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None], self.concentrations[indices, :]),\n",
    "                dim=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.combine_spectra_range < 1e-12:\n",
    "            spectrum = self.spectra[idx]\n",
    "            spectrum = self.augment_spectra(spectrum)\n",
    "            return {\n",
    "                \"spectra\": spectrum,\n",
    "                \"concentrations\": self.concentrations[idx],\n",
    "                \"label_weight\": torch.tensor(1.0, dtype=self.dtype),\n",
    "            }\n",
    "        else:\n",
    "            if random.random() < self.original_dp_weight:\n",
    "                one_weight = 1.\n",
    "                label_weight = torch.tensor(1.0, dtype=self.dtype)\n",
    "            else:\n",
    "                one_weight = random.uniform(0.0, self.combine_spectra_range)\n",
    "                label_weight = self.augmentation_weight\n",
    "            weights = torch.tensor([one_weight, (1 - one_weight)])\n",
    "            # just pick two random indices\n",
    "            indices = random.choices(range(len(self)), k=2)\n",
    "\n",
    "            mixed_spectra, mixed_concentrations = self.combine_k_items(\n",
    "                indices=indices,\n",
    "                weights=weights,\n",
    "            )\n",
    "            mixed_spectra = self.augment_spectra(mixed_spectra)\n",
    "            return mixed_spectra, mixed_concentrations, label_weight\n",
    "\n",
    "\n",
    "config = {\n",
    "    'initial_cnn_channels': 32,\n",
    "    'cnn_channel_factor': 1.279574024454846,\n",
    "    'num_cnn_layers': 8,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'activation_function': 'ELU',\n",
    "    'fc_dropout': 0.10361700399831791,\n",
    "    'lr': 0.001,\n",
    "    'gamma': 0.9649606352621118,\n",
    "    'baseline_factor_bound': 0.748262317340447,\n",
    "    'baseline_period_lower_bound': 0.9703081695287203,\n",
    "    'baseline_period_span': 19.79744237606427,\n",
    "    'original_datapoint_weight': 0.4335003268130408,\n",
    "    'augment_slope_std': 0.08171025264382692,\n",
    "    'batch_size': 32,\n",
    "    'fc_dims': 226,\n",
    "    'rolling_bound': 2,\n",
    "    'num_blocks': 2,\n",
    "}\n",
    "\n",
    "def get_dataset(inputs, targets, config, inputs_mean_std=None, targets_mean_std=None):\n",
    "    return SpectralDataset(\n",
    "        spectra=inputs[:, None, :],\n",
    "        concentrations=targets,\n",
    "        dtype=torch.float32,\n",
    "        spectra_mean_std=inputs_mean_std,\n",
    "        concentration_mean_std=targets_mean_std,\n",
    "        combine_spectra_range=1.0,\n",
    "        baseline_factor_bound=config[\"baseline_factor_bound\"],\n",
    "        baseline_period_lower_bound=config[\"baseline_period_lower_bound\"],\n",
    "        baseline_period_upper_bound=(config[\"baseline_period_lower_bound\"] + config[\"baseline_period_span\"]),\n",
    "        augment_slope_std=config[\"augment_slope_std\"],\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=config[\"rolling_bound\"],\n",
    "        spectrum_rolling_sigma=0.01,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb8f5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+5232)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds, train_batch_size, eval_batch_size):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e76b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "        if DROPOUT: neptune_run[\"h_parameters\"] = {\"dropout\": DROPOUT}\n",
    "        if DROP_PATH_RATE: neptune_run[\"h_parameters\"] = {\"drop_path_rate\": DROP_PATH_RATE}\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3484e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().float().numpy()\n",
    "    targets = targets.cpu().detach().float().numpy()\n",
    "    \n",
    "    dim1 = r2_score(targets[:, 0], preds[:, 0])\n",
    "    dim2 = r2_score(targets[:, 1], preds[:, 1])\n",
    "    dim3 = r2_score(targets[:, 2], preds[:, 2])\n",
    "    \n",
    "    return dim1, dim2, dim3, r2_score(targets, preds)\n",
    "\n",
    "\n",
    "class MSEIgnoreNans(_Loss):\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        weights: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        mask = torch.isfinite(target)\n",
    "        mse = torch.mean(\n",
    "            torch.mul(\n",
    "                torch.square(input[mask] - target[mask]),\n",
    "                torch.tile(weights[:, None], dims=(1, target.shape[1]))[mask],\n",
    "            )\n",
    "        )\n",
    "        return torch.where(\n",
    "            torch.isfinite(mse),\n",
    "            mse,\n",
    "            torch.tensor(0.).to(target.device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "270d23c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A residual block with two 1D convolutional layers.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.elu = nn.ELU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.elu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.elu(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"A deeper ResNet-style 1D CNN for Raman spectra.\"\"\"\n",
    "    def __init__(self, dropout, input_channels=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.elu = nn.GELU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout), # Increased dropout for better regularization\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, stride=s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9bd69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Identity(torch.torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "# this is not a resnet yet\n",
    "class ReZeroBlock(torch.torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        activation_function,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dtype,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(ReZeroBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = torch.torch.nn.BatchNorm1d\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = divmod(kernel_size, 2)[0] if stride == 1 else 0\n",
    "\n",
    "        # does not change spatial dimension\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.bn1 = norm_layer(out_channels, dtype=dtype)\n",
    "        # Both self.conv2 and self.downsample layers\n",
    "        # downsample the input when stride != 1\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            groups=out_channels,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        if stride > 1:\n",
    "            down_conv = torch.nn.Conv1d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "                dtype=dtype,\n",
    "                # groups=out_channels,\n",
    "            )\n",
    "        else:\n",
    "            down_conv = Identity()\n",
    "\n",
    "        self.down_sample = torch.nn.Sequential(\n",
    "            down_conv,\n",
    "            norm_layer(out_channels),\n",
    "        )\n",
    "        self.bn2 = norm_layer(out_channels, dtype=dtype)\n",
    "        # does not change the spatial dimension\n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.bn3 = norm_layer(out_channels, dtype=dtype)\n",
    "        self.activation = activation_function(inplace=True)\n",
    "        self.factor = torch.torch.nn.parameter.Parameter(torch.tensor(0.0, dtype=dtype))\n",
    "\n",
    "    def next_spatial_dim(self, last_spatial_dim):\n",
    "        return math.floor(\n",
    "            (last_spatial_dim + 2 * self.padding - self.kernel_size)\n",
    "            / self.stride + 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        # not really the identity, but kind of\n",
    "        identity = self.down_sample(x)\n",
    "\n",
    "        return self.activation(out * self.factor + identity)\n",
    "\n",
    "\n",
    "class ResNetEncoder(torch.torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectrum_size,\n",
    "        cnn_encoder_channel_dims,\n",
    "        activation_function,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dtype,\n",
    "        num_blocks,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "\n",
    "        self.spatial_dims = [spectrum_size]\n",
    "        layers = []\n",
    "        for in_channels, out_channels in zip(\n",
    "            cnn_encoder_channel_dims[:-1],\n",
    "            cnn_encoder_channel_dims[1:],\n",
    "        ):\n",
    "            block = ReZeroBlock(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                activation_function=activation_function,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            layers.append(block)\n",
    "            self.spatial_dims.append(block.next_spatial_dim(self.spatial_dims[-1]))\n",
    "            for _ in range(num_blocks - 1):\n",
    "                block = ReZeroBlock(\n",
    "                    in_channels=out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    activation_function=activation_function,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    dtype=dtype,\n",
    "                )\n",
    "                layers.append(block)\n",
    "                self.spatial_dims.append(block.next_spatial_dim(self.spatial_dims[-1]))\n",
    "\n",
    "        self.resnet_layers = torch.torch.nn.Sequential(*layers)\n",
    "        if verbose:\n",
    "            print(\"CNN Encoder Channel Dims: %s\" % (cnn_encoder_channel_dims))\n",
    "            print(\"CNN Encoder Spatial Dims: %s\" % (self.spatial_dims))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet_layers(x)\n",
    "\n",
    "\n",
    "class ReZeroNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra_channels,\n",
    "        spectra_size,\n",
    "        initial_cnn_channels,\n",
    "        cnn_channel_factor,\n",
    "        num_cnn_layers,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        activation_function,\n",
    "        fc_dims,\n",
    "        fc_dropout=0.0,\n",
    "        dtype=None,\n",
    "        verbose=False,\n",
    "        fc_output_channels=1,\n",
    "        num_blocks=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc_output_channels = fc_output_channels\n",
    "        self.dtype = dtype or torch.float32\n",
    "\n",
    "        activation_function = getattr(torch.nn, activation_function)\n",
    "\n",
    "        # Setup CNN Encoder\n",
    "        cnn_encoder_channel_dims = [spectra_channels] + [\n",
    "            int(initial_cnn_channels * (cnn_channel_factor**idx))\n",
    "            for idx in range(num_cnn_layers)\n",
    "        ]\n",
    "        self.cnn_encoder = ResNetEncoder(\n",
    "            spectrum_size=spectra_size,\n",
    "            cnn_encoder_channel_dims=cnn_encoder_channel_dims,\n",
    "            activation_function=activation_function,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            num_blocks=num_blocks,\n",
    "            dtype=dtype,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.fc_dims = [\n",
    "            int(\n",
    "                self.cnn_encoder.spatial_dims[-1]\n",
    "            ) * int(cnn_encoder_channel_dims[-1])\n",
    "        ] + fc_dims\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Fc Dims: %s\" % self.fc_dims)\n",
    "        fc_layers = []\n",
    "        for idx, (in_dim, out_dim) in enumerate(\n",
    "                zip(self.fc_dims[:-2], self.fc_dims[1:-1])\n",
    "        ):\n",
    "            fc_layers.append(torch.nn.Linear(in_dim, out_dim))\n",
    "            fc_layers.append(torch.nn.ELU())\n",
    "            fc_layers.append(torch.nn.Dropout(fc_dropout / (2 ** idx)))\n",
    "        fc_layers.append(\n",
    "            torch.nn.Linear(\n",
    "                self.fc_dims[-2],\n",
    "                self.fc_dims[-1] * self.fc_output_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.fc_net = torch.nn.Sequential(*fc_layers)\n",
    "        if verbose:\n",
    "            num_params = sum(p.numel() for p in self.parameters())\n",
    "            print(\"Number of Parameters: %s\" % num_params)\n",
    "\n",
    "    def forward(self, spectra):\n",
    "        embeddings = self.cnn_encoder(spectra)\n",
    "        forecast = self.fc_net(embeddings.view(-1, self.fc_dims[0]))\n",
    "        if self.fc_output_channels > 1:\n",
    "            forecast = forecast.reshape(\n",
    "                -1, self.fc_output_channels, self.fc_dims[-1]\n",
    "            )\n",
    "        return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf964c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm for 1D with channels_last (N, L, C) or channels_first (N, C, L).\"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        if data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.data_format = data_format\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            # x: (N, L, C); normalize over last dim C\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        else:\n",
    "            # x: (N, C, L); normalize over channel dim C, per position L\n",
    "            u = x.mean(dim=1, keepdim=True)                       # (N,1,L)\n",
    "            s = (x - u).pow(2).mean(dim=1, keepdim=True)          # (N,1,L)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)                # (N,C,L)\n",
    "            # Broadcast weight/bias over (C,1)\n",
    "            x = self.weight[:, None] * x + self.bias[:, None]\n",
    "            return x\n",
    "        \n",
    "   \n",
    "class GRN(nn.Module):\n",
    "    \"\"\"GRN for 1D sequences in channels-last format (N, L, C).\"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # Broadcast over (N, L, C): parameters shape to (1,1,C)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, L, C)\n",
    "        # L2 norm across channels at each position\n",
    "        Gx = torch.norm(x, p=2, dim=-1, keepdim=True)                 # (N, L, 1)\n",
    "        # Normalize by mean over positions (sequence length)\n",
    "        Nx = Gx / (Gx.mean(dim=1, keepdim=True) + self.eps)           # (N, L, 1)\n",
    "        return self.gamma * (x * Nx) + self.beta + x                  # (N, L, C)\n",
    "    \n",
    "    \n",
    "from timm.layers import trunc_normal_, DropPath\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" ConvNeXtV2 Block (1D).\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv1d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise convs via linear\n",
    "        self.act = nn.GELU()\n",
    "        self.grn = GRN(4 * dim)\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)                 # (N, C, L)\n",
    "        x = x.transpose(1, 2)              # (N, L, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.pwconv2(x)\n",
    "        x = x.transpose(1, 2)              # (N, C, L)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ConvNeXtV2(nn.Module):\n",
    "    \"\"\" ConvNeXt V2 (1D)\n",
    "\n",
    "    Args:\n",
    "        in_chans (int): Number of input channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=1, num_classes=3,\n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768],\n",
    "                 drop_path_rate=0., dropout=0.1, head_init_scale=1.\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.depths = depths\n",
    "        self.downsample_layers = nn.ModuleList()  # stem + 3 intermediate downsampling conv layers\n",
    "\n",
    "        # Stem: 1D patchify with stride 4\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv1d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "        # Three downsample layers: halve length each stage\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                nn.Conv1d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        # Stages with residual Blocks (already 1D)\n",
    "        self.stages = nn.ModuleList()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j]) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        # Final norm and head: pool over length, then LayerNorm over channels\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.head.weight.data.mul_(head_init_scale)\n",
    "        self.head.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # x: (N, C_in, L)\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)  # (N, C_i, L_i)\n",
    "            x = self.stages[i](x)             # (N, C_i, L_i)\n",
    "\n",
    "        # Global average pooling over length -> (N, C)\n",
    "        x = x.mean(dim=-1)\n",
    "\n",
    "        # Final LayerNorm expects (N, C)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    #def convnext_pico(**kwargs):\n",
    "#    model = ConvNeXtV2(depths=[2, 2, 6, 2], dims=[64, 128, 256, 512], **kwargs)\n",
    "#    return model\n",
    "\n",
    "def convnextv2_atto(**kwargs):\n",
    "    model = ConvNeXtV2(depths=[2, 2, 6, 2], dims=[40, 80, 160, 320], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "111b3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    device,\n",
    "    amp_dtype,\n",
    "    scheduler,\n",
    "    train_dl,\n",
    "    eval_dl,\n",
    "    loss_fn,\n",
    "    epochs,\n",
    "    checkpoint_name,\n",
    "    score=-float(\"inf\"),\n",
    "    neptune_run=None,\n",
    "    p=True,\n",
    "):  \n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for inputs, targets, weights in train_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(device_type=device, dtype=amp_dtype, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets, weights)\n",
    "                  \n",
    "            if amp_dtype == torch.bfloat16:                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            scheduler.step()\n",
    "            if neptune_run is not None:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            total_loss += loss.detach().cpu()\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        all_logits = torch.cat(all_logits)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "        total_loss = total_loss / len(train_dl)\n",
    "        \n",
    "        model.eval()\n",
    "        eval_total_loss = 0.0\n",
    "        eval_all_logits = []\n",
    "        eval_all_targets = []\n",
    "\n",
    "        for inputs, targets, weights in eval_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                with torch.amp.autocast(device_type=device, dtype=amp_dtype, cache_enabled=True):\n",
    "                    logits = model(inputs)\n",
    "                    loss = loss_fn(logits, targets, weights)\n",
    "\n",
    "            eval_total_loss += loss.detach().cpu()\n",
    "            eval_all_logits.append(logits.detach().cpu())\n",
    "            eval_all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        eval_all_logits = torch.cat(eval_all_logits)\n",
    "        eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "        eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "        eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "        \n",
    "        if eval_r2 > score:\n",
    "            score = eval_r2\n",
    "            data = {\"state_dict\": model.state_dict()}\n",
    "            data[\"epoch\"] = epoch \n",
    "            data[\"score\"] = score\n",
    "            torch.save(data, f\"/kaggle/working/{checkpoint_name}\")\n",
    "        \n",
    "        if neptune_run is not None:\n",
    "            neptune_run[\"train/loss\"].append(total_loss)\n",
    "            neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "            neptune_run[\"train/r2\"].append(r2)\n",
    "            neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "            neptune_run[\"train/one\"].append(one)\n",
    "            neptune_run[\"train/two\"].append(two)\n",
    "            neptune_run[\"train/three\"].append(three)\n",
    "            neptune_run[\"eval/one\"].append(eval_one)\n",
    "            neptune_run[\"eval/two\"].append(eval_two)\n",
    "            neptune_run[\"eval/three\"].append(eval_three)\n",
    "            \n",
    "        if p and epoch % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, \"\n",
    "                f\"train/loss: {total_loss:.4f}, \"\n",
    "                f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "                f\"train/r2: {r2:.4f}, \"\n",
    "                f\"eval/r2: {eval_r2:.4f}, \"\n",
    "                f\"train/one: {one:.4f}, \"\n",
    "                f\"train/two: {two:.4f}, \"\n",
    "                f\"train/three: {three:.4f}, \"\n",
    "                f\"eval/one: {eval_one:.4f}, \"\n",
    "                f\"eval/two: {eval_two:.4f}, \"\n",
    "                f\"eval/three: {eval_three:.4f} \"\n",
    "            )\n",
    "            \n",
    "    if neptune_run is not None: neptune_run.stop()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a63a6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "\n",
    "DROPOUT = 0.5\n",
    "DROP_PATH_RATE = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESUME = False\n",
    "\n",
    "config[\"dtype\"] = torch.float32\n",
    "config[\"spectra_size\"] = 1643\n",
    "config[\"spectra_channels\"] = 1\n",
    "config[\"fc_dims\"] = [\n",
    "    config[\"fc_dims\"],\n",
    "    int(config[\"fc_dims\"] / 2),\n",
    "    3,\n",
    "]\n",
    "\n",
    "mse_loss_function = MSEIgnoreNans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45cf8cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.174923\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b22cc99bf4a49229397f920c21e1ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0294, eval/loss: 1.1018, train/r2: -0.0300, eval/r2: -0.0279, train/one: -0.0314, train/two: -0.0250, train/three: -0.0335, eval/one: -0.0106, eval/two: -0.0544, eval/three: -0.0188 \n",
      "Epoch: 5, train/loss: 0.9639, eval/loss: 0.9438, train/r2: 0.0132, eval/r2: 0.0058, train/one: -0.0134, train/two: -0.0255, train/three: 0.0785, eval/one: -0.0681, eval/two: -0.0938, eval/three: 0.1793 \n",
      "Epoch: 10, train/loss: 0.9072, eval/loss: 0.8362, train/r2: 0.0951, eval/r2: 0.1160, train/one: -0.0095, train/two: -0.0136, train/three: 0.3084, eval/one: -0.0499, eval/two: -0.0246, eval/three: 0.4224 \n",
      "Epoch: 15, train/loss: 0.8281, eval/loss: 0.8365, train/r2: 0.1699, eval/r2: 0.1384, train/one: 0.0049, train/two: 0.0017, train/three: 0.5031, eval/one: -0.0277, eval/two: -0.0106, eval/three: 0.4533 \n",
      "Epoch: 20, train/loss: 0.7777, eval/loss: 0.6700, train/r2: 0.2198, eval/r2: 0.2172, train/one: -0.0150, train/two: 0.0107, train/three: 0.6637, eval/one: -0.0598, eval/two: -0.0112, eval/three: 0.7226 \n",
      "Epoch: 25, train/loss: 0.7411, eval/loss: 0.7819, train/r2: 0.2547, eval/r2: 0.2701, train/one: 0.0034, train/two: 0.0219, train/three: 0.7388, eval/one: 0.0125, eval/two: 0.0447, eval/three: 0.7531 \n",
      "Epoch: 30, train/loss: 0.7288, eval/loss: 0.6931, train/r2: 0.2856, eval/r2: 0.2546, train/one: 0.0502, train/two: 0.0121, train/three: 0.7945, eval/one: -0.0251, eval/two: 0.0296, eval/three: 0.7594 \n",
      "Epoch: 35, train/loss: 0.6460, eval/loss: 0.6837, train/r2: 0.3320, eval/r2: 0.2815, train/one: 0.1453, train/two: 0.0427, train/three: 0.8079, eval/one: 0.0645, eval/two: 0.0076, eval/three: 0.7726 \n",
      "Epoch: 40, train/loss: 0.5698, eval/loss: 0.5772, train/r2: 0.4326, eval/r2: 0.4360, train/one: 0.3069, train/two: 0.1713, train/three: 0.8197, eval/one: 0.3180, eval/two: 0.1742, eval/three: 0.8159 \n",
      "Epoch: 45, train/loss: 0.4206, eval/loss: 0.4487, train/r2: 0.5578, eval/r2: 0.5320, train/one: 0.5012, train/two: 0.3451, train/three: 0.8272, eval/one: 0.4343, eval/two: 0.3451, eval/three: 0.8165 \n",
      "Epoch: 50, train/loss: 0.3440, eval/loss: 0.3688, train/r2: 0.6604, eval/r2: 0.6253, train/one: 0.6201, train/two: 0.5144, train/three: 0.8468, eval/one: 0.5942, eval/two: 0.4517, eval/three: 0.8299 \n",
      "Epoch: 55, train/loss: 0.2980, eval/loss: 0.3370, train/r2: 0.7032, eval/r2: 0.6410, train/one: 0.6778, train/two: 0.5526, train/three: 0.8792, eval/one: 0.6863, eval/two: 0.4022, eval/three: 0.8344 \n",
      "Epoch: 60, train/loss: 0.2736, eval/loss: 0.2932, train/r2: 0.7163, eval/r2: 0.6848, train/one: 0.6908, train/two: 0.5817, train/three: 0.8764, eval/one: 0.6053, eval/two: 0.5724, eval/three: 0.8768 \n",
      "Epoch: 65, train/loss: 0.2172, eval/loss: 0.2787, train/r2: 0.7797, eval/r2: 0.7107, train/one: 0.7636, train/two: 0.6682, train/three: 0.9072, eval/one: 0.7060, eval/two: 0.5423, eval/three: 0.8838 \n",
      "Epoch: 70, train/loss: 0.1896, eval/loss: 0.2380, train/r2: 0.8112, eval/r2: 0.7637, train/one: 0.8193, train/two: 0.7117, train/three: 0.9026, eval/one: 0.7325, eval/two: 0.6499, eval/three: 0.9087 \n",
      "Epoch: 75, train/loss: 0.1852, eval/loss: 0.2144, train/r2: 0.8181, eval/r2: 0.7709, train/one: 0.8098, train/two: 0.7331, train/three: 0.9114, eval/one: 0.7196, eval/two: 0.6817, eval/three: 0.9116 \n",
      "Epoch: 80, train/loss: 0.1614, eval/loss: 0.2100, train/r2: 0.8332, eval/r2: 0.7869, train/one: 0.8315, train/two: 0.7515, train/three: 0.9166, eval/one: 0.7629, eval/two: 0.6945, eval/three: 0.9032 \n",
      "Epoch: 85, train/loss: 0.1592, eval/loss: 0.2278, train/r2: 0.8386, eval/r2: 0.7788, train/one: 0.8485, train/two: 0.7448, train/three: 0.9225, eval/one: 0.7407, eval/two: 0.6781, eval/three: 0.9176 \n",
      "Epoch: 90, train/loss: 0.1459, eval/loss: 0.2237, train/r2: 0.8562, eval/r2: 0.7845, train/one: 0.8671, train/two: 0.7707, train/three: 0.9308, eval/one: 0.7432, eval/two: 0.6865, eval/three: 0.9240 \n",
      "Epoch: 95, train/loss: 0.1442, eval/loss: 0.2193, train/r2: 0.8530, eval/r2: 0.7780, train/one: 0.8642, train/two: 0.7761, train/three: 0.9187, eval/one: 0.7047, eval/two: 0.7154, eval/three: 0.9138 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 58 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 58 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-251/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff78bf3fb294bc5953c740c4a919aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0124, eval/loss: 1.1198, train/r2: -0.0260, eval/r2: -0.0047, train/one: -0.0371, train/two: -0.0335, train/three: -0.0076, eval/one: -0.0270, eval/two: 0.0021, eval/three: 0.0107 \n",
      "Epoch: 5, train/loss: 0.9805, eval/loss: 1.0333, train/r2: 0.0001, eval/r2: -0.0202, train/one: -0.0385, train/two: -0.0135, train/three: 0.0524, eval/one: 0.0070, eval/two: -0.0168, eval/three: -0.0507 \n",
      "Epoch: 10, train/loss: 0.9089, eval/loss: 0.9491, train/r2: 0.0934, eval/r2: 0.1036, train/one: -0.0124, train/two: -0.0156, train/three: 0.3080, eval/one: -0.0111, eval/two: 0.0154, eval/three: 0.3064 \n",
      "Epoch: 15, train/loss: 0.8819, eval/loss: 0.9727, train/r2: 0.1310, eval/r2: 0.1151, train/one: -0.0118, train/two: -0.0055, train/three: 0.4102, eval/one: 0.0143, eval/two: 0.0340, eval/three: 0.2971 \n",
      "Epoch: 20, train/loss: 0.8191, eval/loss: 0.8945, train/r2: 0.1841, eval/r2: 0.1539, train/one: 0.0079, train/two: 0.0096, train/three: 0.5348, eval/one: 0.0032, eval/two: -0.0139, eval/three: 0.4723 \n",
      "Epoch: 25, train/loss: 0.8030, eval/loss: 0.8074, train/r2: 0.2066, eval/r2: 0.2179, train/one: 0.0070, train/two: 0.0049, train/three: 0.6079, eval/one: 0.0255, eval/two: -0.0006, eval/three: 0.6288 \n",
      "Epoch: 30, train/loss: 0.7015, eval/loss: 0.8273, train/r2: 0.2451, eval/r2: 0.2466, train/one: 0.0109, train/two: 0.0038, train/three: 0.7206, eval/one: -0.0045, eval/two: -0.0113, eval/three: 0.7555 \n",
      "Epoch: 35, train/loss: 0.7166, eval/loss: 0.7128, train/r2: 0.2931, eval/r2: 0.3039, train/one: 0.0409, train/two: 0.0251, train/three: 0.8133, eval/one: 0.0368, eval/two: 0.0358, eval/three: 0.8392 \n",
      "Epoch: 40, train/loss: 0.6508, eval/loss: 0.7174, train/r2: 0.3572, eval/r2: 0.3359, train/one: 0.1893, train/two: 0.0827, train/three: 0.7997, eval/one: 0.1204, eval/two: 0.0805, eval/three: 0.8067 \n",
      "Epoch: 45, train/loss: 0.5094, eval/loss: 0.6180, train/r2: 0.4816, eval/r2: 0.4265, train/one: 0.3801, train/two: 0.2353, train/three: 0.8293, eval/one: 0.2393, eval/two: 0.1988, eval/three: 0.8414 \n",
      "Epoch: 50, train/loss: 0.3400, eval/loss: 0.3798, train/r2: 0.6673, eval/r2: 0.6625, train/one: 0.6224, train/two: 0.5282, train/three: 0.8512, eval/one: 0.6027, eval/two: 0.5237, eval/three: 0.8612 \n",
      "Epoch: 55, train/loss: 0.2838, eval/loss: 0.3365, train/r2: 0.7141, eval/r2: 0.7151, train/one: 0.6655, train/two: 0.5980, train/three: 0.8790, eval/one: 0.6695, eval/two: 0.5855, eval/three: 0.8904 \n",
      "Epoch: 60, train/loss: 0.2475, eval/loss: 0.3054, train/r2: 0.7464, eval/r2: 0.7423, train/one: 0.6938, train/two: 0.6440, train/three: 0.9015, eval/one: 0.7183, eval/two: 0.6374, eval/three: 0.8711 \n",
      "Epoch: 65, train/loss: 0.1930, eval/loss: 0.2568, train/r2: 0.8054, eval/r2: 0.7439, train/one: 0.7717, train/two: 0.7274, train/three: 0.9172, eval/one: 0.7177, eval/two: 0.6272, eval/three: 0.8868 \n",
      "Epoch: 70, train/loss: 0.1714, eval/loss: 0.2452, train/r2: 0.8304, eval/r2: 0.7912, train/one: 0.8186, train/two: 0.7453, train/three: 0.9273, eval/one: 0.7206, eval/two: 0.7314, eval/three: 0.9214 \n",
      "Epoch: 75, train/loss: 0.1495, eval/loss: 0.2215, train/r2: 0.8442, eval/r2: 0.7906, train/one: 0.8476, train/two: 0.7550, train/three: 0.9301, eval/one: 0.7808, eval/two: 0.7032, eval/three: 0.8877 \n",
      "Epoch: 80, train/loss: 0.1437, eval/loss: 0.2052, train/r2: 0.8594, eval/r2: 0.8208, train/one: 0.8548, train/two: 0.7874, train/three: 0.9359, eval/one: 0.8112, eval/two: 0.7225, eval/three: 0.9287 \n",
      "Epoch: 85, train/loss: 0.1320, eval/loss: 0.1826, train/r2: 0.8685, eval/r2: 0.8325, train/one: 0.8801, train/two: 0.7859, train/three: 0.9397, eval/one: 0.8144, eval/two: 0.7608, eval/three: 0.9223 \n",
      "Epoch: 90, train/loss: 0.1276, eval/loss: 0.1889, train/r2: 0.8808, eval/r2: 0.8310, train/one: 0.8908, train/two: 0.8031, train/three: 0.9484, eval/one: 0.8044, eval/two: 0.7592, eval/three: 0.9293 \n",
      "Epoch: 95, train/loss: 0.1190, eval/loss: 0.1765, train/r2: 0.8825, eval/r2: 0.8313, train/one: 0.8855, train/two: 0.8156, train/three: 0.9464, eval/one: 0.8333, eval/two: 0.7540, eval/three: 0.9065 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 51 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 51 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-252/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f5f7da337744b9a63043887fe06300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0718, eval/loss: 0.9210, train/r2: -0.0498, eval/r2: 0.0074, train/one: -0.0560, train/two: -0.0638, train/three: -0.0297, eval/one: -0.0236, eval/two: -0.0142, eval/three: 0.0600 \n",
      "Epoch: 5, train/loss: 1.0048, eval/loss: 0.8919, train/r2: 0.0044, eval/r2: 0.0168, train/one: -0.0162, train/two: -0.0237, train/three: 0.0530, eval/one: -0.0192, eval/two: 0.0077, eval/three: 0.0617 \n",
      "Epoch: 10, train/loss: 0.9179, eval/loss: 0.8239, train/r2: 0.0759, eval/r2: 0.1111, train/one: -0.0285, train/two: -0.0092, train/three: 0.2653, eval/one: 0.0096, eval/two: -0.0581, eval/three: 0.3817 \n",
      "Epoch: 15, train/loss: 0.8296, eval/loss: 0.8415, train/r2: 0.1660, eval/r2: 0.1471, train/one: 0.0131, train/two: -0.0100, train/three: 0.4950, eval/one: -0.0683, eval/two: 0.0095, eval/three: 0.5002 \n",
      "Epoch: 20, train/loss: 0.7545, eval/loss: 0.7472, train/r2: 0.2276, eval/r2: 0.2243, train/one: 0.0110, train/two: 0.0042, train/three: 0.6674, eval/one: -0.0419, eval/two: 0.0052, eval/three: 0.7097 \n",
      "Epoch: 25, train/loss: 0.7328, eval/loss: 0.8201, train/r2: 0.2701, eval/r2: 0.2125, train/one: 0.0574, train/two: 0.0103, train/three: 0.7426, eval/one: 0.0157, eval/two: -0.0321, eval/three: 0.6540 \n",
      "Epoch: 30, train/loss: 0.7286, eval/loss: 0.6595, train/r2: 0.2871, eval/r2: 0.2834, train/one: 0.1055, train/two: 0.0284, train/three: 0.7275, eval/one: 0.0691, eval/two: 0.0148, eval/three: 0.7664 \n",
      "Epoch: 35, train/loss: 0.6193, eval/loss: 0.5777, train/r2: 0.3819, eval/r2: 0.3794, train/one: 0.3134, train/two: 0.0374, train/three: 0.7949, eval/one: 0.3159, eval/two: 0.0482, eval/three: 0.7739 \n",
      "Epoch: 40, train/loss: 0.5483, eval/loss: 0.4958, train/r2: 0.4726, eval/r2: 0.4533, train/one: 0.4800, train/two: 0.1283, train/three: 0.8095, eval/one: 0.4308, eval/two: 0.1072, eval/three: 0.8218 \n",
      "Epoch: 45, train/loss: 0.4506, eval/loss: 0.4515, train/r2: 0.5618, eval/r2: 0.5030, train/one: 0.5464, train/two: 0.2957, train/three: 0.8434, eval/one: 0.3960, eval/two: 0.2377, eval/three: 0.8752 \n",
      "Epoch: 50, train/loss: 0.3678, eval/loss: 0.3178, train/r2: 0.6401, eval/r2: 0.6430, train/one: 0.5992, train/two: 0.4620, train/three: 0.8591, eval/one: 0.6450, eval/two: 0.4547, eval/three: 0.8293 \n",
      "Epoch: 55, train/loss: 0.2826, eval/loss: 0.3010, train/r2: 0.7140, eval/r2: 0.6852, train/one: 0.6818, train/two: 0.5925, train/three: 0.8678, eval/one: 0.6642, eval/two: 0.5199, eval/three: 0.8714 \n",
      "Epoch: 60, train/loss: 0.2488, eval/loss: 0.2616, train/r2: 0.7542, eval/r2: 0.7216, train/one: 0.7440, train/two: 0.6450, train/three: 0.8737, eval/one: 0.6677, eval/two: 0.5918, eval/three: 0.9052 \n",
      "Epoch: 65, train/loss: 0.2185, eval/loss: 0.2322, train/r2: 0.7801, eval/r2: 0.7547, train/one: 0.7796, train/two: 0.6625, train/three: 0.8983, eval/one: 0.7673, eval/two: 0.6012, eval/three: 0.8955 \n",
      "Epoch: 70, train/loss: 0.1758, eval/loss: 0.1950, train/r2: 0.8223, eval/r2: 0.8039, train/one: 0.8146, train/two: 0.7336, train/three: 0.9187, eval/one: 0.8189, eval/two: 0.6870, eval/three: 0.9057 \n",
      "Epoch: 75, train/loss: 0.1577, eval/loss: 0.1967, train/r2: 0.8423, eval/r2: 0.8035, train/one: 0.8408, train/two: 0.7742, train/three: 0.9118, eval/one: 0.7902, eval/two: 0.7099, eval/three: 0.9103 \n",
      "Epoch: 80, train/loss: 0.1386, eval/loss: 0.1643, train/r2: 0.8642, eval/r2: 0.8168, train/one: 0.8700, train/two: 0.7924, train/three: 0.9304, eval/one: 0.8537, eval/two: 0.6817, eval/three: 0.9150 \n",
      "Epoch: 85, train/loss: 0.1327, eval/loss: 0.1576, train/r2: 0.8640, eval/r2: 0.8318, train/one: 0.8673, train/two: 0.7991, train/three: 0.9256, eval/one: 0.8283, eval/two: 0.7446, eval/three: 0.9223 \n",
      "Epoch: 90, train/loss: 0.1289, eval/loss: 0.1464, train/r2: 0.8691, eval/r2: 0.8549, train/one: 0.8664, train/two: 0.8036, train/three: 0.9374, eval/one: 0.8587, eval/two: 0.7838, eval/three: 0.9223 \n",
      "Epoch: 95, train/loss: 0.1270, eval/loss: 0.1595, train/r2: 0.8716, eval/r2: 0.8338, train/one: 0.8775, train/two: 0.8035, train/three: 0.9339, eval/one: 0.8634, eval/two: 0.7296, eval/three: 0.9085 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 54 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 54 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-253/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ade9f46fdd4360aa57250b8c11a1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0450, eval/loss: 0.9299, train/r2: -0.0426, eval/r2: 0.0098, train/one: -0.0418, train/two: -0.0459, train/three: -0.0402, eval/one: -0.0199, eval/two: 0.0038, eval/three: 0.0455 \n",
      "Epoch: 5, train/loss: 1.0175, eval/loss: 0.9436, train/r2: -0.0106, eval/r2: 0.0235, train/one: -0.0557, train/two: -0.0355, train/three: 0.0595, eval/one: -0.0088, eval/two: -0.0044, eval/three: 0.0837 \n",
      "Epoch: 10, train/loss: 0.8919, eval/loss: 1.0012, train/r2: 0.0913, eval/r2: 0.0487, train/one: -0.0156, train/two: -0.0044, train/three: 0.2937, eval/one: -0.0178, eval/two: 0.0194, eval/three: 0.1446 \n",
      "Epoch: 15, train/loss: 0.8576, eval/loss: 0.8712, train/r2: 0.1606, eval/r2: 0.1560, train/one: 0.0076, train/two: 0.0138, train/three: 0.4604, eval/one: 0.0057, eval/two: 0.0086, eval/three: 0.4537 \n",
      "Epoch: 20, train/loss: 0.8119, eval/loss: 0.8018, train/r2: 0.2007, eval/r2: 0.1957, train/one: 0.0147, train/two: 0.0058, train/three: 0.5817, eval/one: 0.0205, eval/two: -0.0104, eval/three: 0.5769 \n",
      "Epoch: 25, train/loss: 0.7850, eval/loss: 0.7557, train/r2: 0.2300, eval/r2: 0.2614, train/one: 0.0381, train/two: -0.0028, train/three: 0.6547, eval/one: 0.0257, eval/two: 0.0077, eval/three: 0.7508 \n",
      "Epoch: 30, train/loss: 0.7358, eval/loss: 0.6945, train/r2: 0.2857, eval/r2: 0.2637, train/one: 0.0800, train/two: 0.0469, train/three: 0.7303, eval/one: 0.0948, eval/two: -0.0237, eval/three: 0.7200 \n",
      "Epoch: 35, train/loss: 0.6468, eval/loss: 0.6236, train/r2: 0.3471, eval/r2: 0.3397, train/one: 0.2550, train/two: 0.0497, train/three: 0.7366, eval/one: 0.1659, eval/two: 0.0315, eval/three: 0.8219 \n",
      "Epoch: 40, train/loss: 0.5821, eval/loss: 0.5532, train/r2: 0.4084, eval/r2: 0.3986, train/one: 0.3169, train/two: 0.1034, train/three: 0.8049, eval/one: 0.3032, eval/two: 0.1016, eval/three: 0.7910 \n",
      "Epoch: 45, train/loss: 0.4270, eval/loss: 0.4726, train/r2: 0.5605, eval/r2: 0.5138, train/one: 0.4984, train/two: 0.3879, train/three: 0.7953, eval/one: 0.3789, eval/two: 0.3689, eval/three: 0.7936 \n",
      "Epoch: 50, train/loss: 0.3300, eval/loss: 0.3760, train/r2: 0.6678, eval/r2: 0.6088, train/one: 0.5924, train/two: 0.5782, train/three: 0.8328, eval/one: 0.5714, eval/two: 0.4130, eval/three: 0.8420 \n",
      "Epoch: 55, train/loss: 0.2695, eval/loss: 0.2884, train/r2: 0.7340, eval/r2: 0.6882, train/one: 0.6972, train/two: 0.6502, train/three: 0.8545, eval/one: 0.7080, eval/two: 0.5039, eval/three: 0.8527 \n",
      "Epoch: 60, train/loss: 0.2182, eval/loss: 0.2750, train/r2: 0.7823, eval/r2: 0.7169, train/one: 0.7557, train/two: 0.7067, train/three: 0.8846, eval/one: 0.6378, eval/two: 0.6302, eval/three: 0.8827 \n",
      "Epoch: 65, train/loss: 0.1911, eval/loss: 0.2058, train/r2: 0.8043, eval/r2: 0.7778, train/one: 0.7816, train/two: 0.7236, train/three: 0.9078, eval/one: 0.7703, eval/two: 0.6540, eval/three: 0.9090 \n",
      "Epoch: 70, train/loss: 0.1571, eval/loss: 0.1986, train/r2: 0.8411, eval/r2: 0.7825, train/one: 0.8483, train/two: 0.7581, train/three: 0.9167, eval/one: 0.6985, eval/two: 0.7456, eval/three: 0.9032 \n",
      "Epoch: 75, train/loss: 0.1416, eval/loss: 0.2196, train/r2: 0.8573, eval/r2: 0.7753, train/one: 0.8528, train/two: 0.7943, train/three: 0.9249, eval/one: 0.7855, eval/two: 0.6261, eval/three: 0.9142 \n",
      "Epoch: 80, train/loss: 0.1325, eval/loss: 0.1755, train/r2: 0.8657, eval/r2: 0.8287, train/one: 0.8650, train/two: 0.8003, train/three: 0.9318, eval/one: 0.7975, eval/two: 0.7687, eval/three: 0.9201 \n",
      "Epoch: 85, train/loss: 0.1261, eval/loss: 0.1343, train/r2: 0.8750, eval/r2: 0.8692, train/one: 0.8683, train/two: 0.8256, train/three: 0.9312, eval/one: 0.8383, eval/two: 0.8209, eval/three: 0.9483 \n",
      "Epoch: 90, train/loss: 0.1194, eval/loss: 0.1453, train/r2: 0.8781, eval/r2: 0.8427, train/one: 0.8782, train/two: 0.8245, train/three: 0.9315, eval/one: 0.8097, eval/two: 0.7795, eval/three: 0.9388 \n",
      "Epoch: 95, train/loss: 0.1153, eval/loss: 0.1702, train/r2: 0.8860, eval/r2: 0.8208, train/one: 0.8847, train/two: 0.8339, train/three: 0.9393, eval/one: 0.8030, eval/two: 0.7356, eval/three: 0.9237 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 23 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 23 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-254/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ddcd307ac14d58a5463753795fa5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0688, eval/loss: 1.0113, train/r2: -0.0387, eval/r2: -0.0229, train/one: -0.0561, train/two: -0.0554, train/three: -0.0048, eval/one: 0.0034, eval/two: -0.0144, eval/three: -0.0578 \n",
      "Epoch: 5, train/loss: 0.9865, eval/loss: 1.0508, train/r2: 0.0131, eval/r2: -0.0478, train/one: -0.0194, train/two: -0.0178, train/three: 0.0764, eval/one: -0.0004, eval/two: -0.0269, eval/three: -0.1163 \n",
      "Epoch: 10, train/loss: 0.9057, eval/loss: 0.8151, train/r2: 0.0896, eval/r2: 0.1037, train/one: 0.0010, train/two: -0.0144, train/three: 0.2821, eval/one: -0.0418, eval/two: -0.0048, eval/three: 0.3577 \n",
      "Epoch: 15, train/loss: 0.8471, eval/loss: 0.8877, train/r2: 0.1298, eval/r2: 0.1471, train/one: -0.0139, train/two: 0.0010, train/three: 0.4024, eval/one: 0.0119, eval/two: -0.0250, eval/three: 0.4545 \n",
      "Epoch: 20, train/loss: 0.7990, eval/loss: 0.8217, train/r2: 0.1925, eval/r2: 0.2045, train/one: 0.0182, train/two: 0.0073, train/three: 0.5520, eval/one: -0.0162, eval/two: -0.0158, eval/three: 0.6454 \n",
      "Epoch: 25, train/loss: 0.7568, eval/loss: 0.7071, train/r2: 0.2409, eval/r2: 0.2320, train/one: 0.0350, train/two: 0.0060, train/three: 0.6818, eval/one: 0.0129, eval/two: 0.0017, eval/three: 0.6815 \n",
      "Epoch: 30, train/loss: 0.7201, eval/loss: 0.7293, train/r2: 0.3022, eval/r2: 0.2514, train/one: 0.0736, train/two: 0.0410, train/three: 0.7919, eval/one: 0.0704, eval/two: 0.0017, eval/three: 0.6821 \n",
      "Epoch: 35, train/loss: 0.6419, eval/loss: 0.5977, train/r2: 0.3488, eval/r2: 0.3453, train/one: 0.2190, train/two: 0.0429, train/three: 0.7845, eval/one: 0.2939, eval/two: -0.0360, eval/three: 0.7779 \n",
      "Epoch: 40, train/loss: 0.5595, eval/loss: 0.5296, train/r2: 0.4304, eval/r2: 0.4667, train/one: 0.3678, train/two: 0.1473, train/three: 0.7762, eval/one: 0.4616, eval/two: 0.1860, eval/three: 0.7525 \n",
      "Epoch: 45, train/loss: 0.4236, eval/loss: 0.4423, train/r2: 0.5874, eval/r2: 0.5549, train/one: 0.5944, train/two: 0.3582, train/three: 0.8094, eval/one: 0.5496, eval/two: 0.3087, eval/three: 0.8063 \n",
      "Epoch: 50, train/loss: 0.3256, eval/loss: 0.3596, train/r2: 0.6677, eval/r2: 0.6210, train/one: 0.6532, train/two: 0.4874, train/three: 0.8625, eval/one: 0.6300, eval/two: 0.3661, eval/three: 0.8670 \n",
      "Epoch: 55, train/loss: 0.2779, eval/loss: 0.2728, train/r2: 0.7295, eval/r2: 0.7110, train/one: 0.7298, train/two: 0.5909, train/three: 0.8678, eval/one: 0.6872, eval/two: 0.6097, eval/three: 0.8363 \n",
      "Epoch: 60, train/loss: 0.2490, eval/loss: 0.2903, train/r2: 0.7566, eval/r2: 0.7284, train/one: 0.7665, train/two: 0.6116, train/three: 0.8915, eval/one: 0.7557, eval/two: 0.5481, eval/three: 0.8816 \n",
      "Epoch: 65, train/loss: 0.2107, eval/loss: 0.2626, train/r2: 0.7889, eval/r2: 0.7363, train/one: 0.8056, train/two: 0.6717, train/three: 0.8893, eval/one: 0.7455, eval/two: 0.5712, eval/three: 0.8921 \n",
      "Epoch: 70, train/loss: 0.1727, eval/loss: 0.2008, train/r2: 0.8245, eval/r2: 0.7759, train/one: 0.8135, train/two: 0.7380, train/three: 0.9218, eval/one: 0.7444, eval/two: 0.6848, eval/three: 0.8984 \n",
      "Epoch: 75, train/loss: 0.1704, eval/loss: 0.2082, train/r2: 0.8324, eval/r2: 0.8031, train/one: 0.8442, train/two: 0.7350, train/three: 0.9182, eval/one: 0.8221, eval/two: 0.6506, eval/three: 0.9365 \n",
      "Epoch: 80, train/loss: 0.1431, eval/loss: 0.1700, train/r2: 0.8608, eval/r2: 0.8262, train/one: 0.8750, train/two: 0.7861, train/three: 0.9213, eval/one: 0.8253, eval/two: 0.7152, eval/three: 0.9381 \n",
      "Epoch: 85, train/loss: 0.1241, eval/loss: 0.1861, train/r2: 0.8772, eval/r2: 0.7982, train/one: 0.8824, train/two: 0.8181, train/three: 0.9309, eval/one: 0.7978, eval/two: 0.6848, eval/three: 0.9120 \n",
      "Epoch: 90, train/loss: 0.1302, eval/loss: 0.1491, train/r2: 0.8687, eval/r2: 0.8509, train/one: 0.8715, train/two: 0.8050, train/three: 0.9296, eval/one: 0.8654, eval/two: 0.7564, eval/three: 0.9310 \n",
      "Epoch: 95, train/loss: 0.1342, eval/loss: 0.1746, train/r2: 0.8654, eval/r2: 0.8304, train/one: 0.8758, train/two: 0.7935, train/three: 0.9268, eval/one: 0.8433, eval/two: 0.7173, eval/three: 0.9305 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 10 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 10 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-255/metadata\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "inputs_mean_std = []\n",
    "targets_mean_std = []\n",
    "scores = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "splits = kfold.split(inputs)\n",
    "\n",
    "for fold, (train_idx, eval_idx) in enumerate(splits):\n",
    "    MODEL_NAME = f\"cv2.pretrain.fold.{fold}\"\n",
    "    checkpoint_name = f\"cv2.pretrain.fold.{fold}.pt\"\n",
    "    \n",
    "    train_inputs = inputs[train_idx]\n",
    "    train_targets = targets[train_idx]\n",
    "    eval_inputs = inputs[eval_idx]\n",
    "    eval_targets = targets[eval_idx]\n",
    "\n",
    "    train_ds = get_dataset(train_inputs, train_targets, config)\n",
    "    \n",
    "    inputs_mean_std.append((fold, train_ds.s_mean, train_ds.s_std))\n",
    "    targets_mean_std.append((fold, train_ds.concentration_means, train_ds.concentration_stds))\n",
    "    \n",
    "    eval_ds = get_dataset(\n",
    "        eval_inputs, \n",
    "        eval_targets, \n",
    "        config, \n",
    "        (train_ds.s_mean, train_ds.s_std), \n",
    "        (train_ds.concentration_means, train_ds.concentration_stds)\n",
    "    )\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    train_dl, eval_dl = return_dls(train_ds, eval_ds, BATCH_SIZE, len(eval_ds))\n",
    "    \n",
    "    #model = ReZeroNet(**config).to(device)\n",
    "    model = convnextv2_atto().to(device)\n",
    "    \n",
    "    if fold == 0: print(get_model_size(model))\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "    scheduler = get_scheduler(optimizer, train_dl, EPOCHS)\n",
    "    \n",
    "    score = train(\n",
    "            model, \n",
    "            optimizer, \n",
    "            device,\n",
    "            torch.float16,\n",
    "            scheduler,\n",
    "            train_dl, \n",
    "            eval_dl,\n",
    "            mse_loss_function,\n",
    "            EPOCHS,\n",
    "            checkpoint_name,\n",
    "            neptune_run=setup_neptune(),\n",
    "        )\n",
    "    \n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05a617ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_792/3483774306.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(4000):\n",
    "    time.sleep(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ec5d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/cv2.pretrain.fold.0.pt 94 0.8159902640478484\n",
      "/kaggle/working/cv2.pretrain.fold.1.pt 93 0.8538997024308118\n",
      "/kaggle/working/cv2.pretrain.fold.2.pt 91 0.8577711669459226\n",
      "/kaggle/working/cv2.pretrain.fold.3.pt 85 0.8691773066244894\n",
      "/kaggle/working/cv2.pretrain.fold.4.pt 90 0.8509266921752051\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "\n",
    "\n",
    "def get_ckpt(path):\n",
    "    return torch.load(path, weights_only=False)\n",
    "\n",
    "\n",
    "def get_ckpt_paths(keyword):\n",
    "    output_dir = \"/kaggle/working\"\n",
    "    output_files = sorted(os.listdir(output_dir))\n",
    "\n",
    "    ckpt_paths = []\n",
    "    for f in output_files:\n",
    "        if keyword in f and \"csv\" not in f:\n",
    "            ckpt_path = os.path.join(output_dir, f)\n",
    "            ckpt_paths.append(ckpt_path)\n",
    "            ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "            print(ckpt_path, ckpt[\"epoch\"], ckpt[\"score\"])\n",
    "            \n",
    "    return ckpt_paths\n",
    "\n",
    "ckpt_paths = get_ckpt_paths(\"cv2\")\n",
    "#state_dicts = [get_ckpt(p)[\"state_dict\"] for p in ckpt_paths]\n",
    "\n",
    "#avg_weights = average_state_dicts(state_dicts)\n",
    "#torch.save(avg_weights, \"/kaggle/working/avg_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7deaa7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper.resnet.pretrain.fold.3.pt',\n",
       " 'paper.resnet.pretrain.fold.1.pt',\n",
       " '.neptune',\n",
       " 'paper.resnet.pretrain.fold.0.pt',\n",
       " 'avg_weights.pt',\n",
       " '.virtual_documents',\n",
       " 'paper.resnet.pretrain.fold.2.pt',\n",
       " 'paper.resnet.pretrain.fold.4.pt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/kaggle/working\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
