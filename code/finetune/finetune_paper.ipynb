{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ca4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "SEED = 1000\n",
    "setup_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8c02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def average_state_dicts(state_dict_list):\n",
    "    n = len(state_dict_list)\n",
    "    # Ensure we don't modify the originals\n",
    "    avg_sd = OrderedDict()\n",
    "\n",
    "    # Iterate over every parameter/buffer key\n",
    "    for k in state_dict_list[0]:\n",
    "        # sum across models â†’ float32 to avoid overflow on int types\n",
    "        avg = sum(sd[k].float() for sd in state_dict_list) / n\n",
    "        # cast back to original dtype if needed\n",
    "        avg_sd[k] = avg.to(dtype=state_dict_list[0][k].dtype)\n",
    "\n",
    "    return avg_sd\n",
    "\n",
    "\n",
    "def get_ckpt_paths(output_dir, keyword):\n",
    "    output_files = sorted(os.listdir(output_dir))\n",
    "\n",
    "    ckpt_paths = []\n",
    "    for f in output_files:\n",
    "        if keyword in f and \"csv\" not in f:\n",
    "            ckpt_path = os.path.join(output_dir, f)\n",
    "            ckpt_paths.append(ckpt_path)\n",
    "            ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "            print(ckpt_path, ckpt[\"epoch\"], ckpt[\"score\"])\n",
    "            \n",
    "    return ckpt_paths\n",
    "\n",
    "\n",
    "def rest(t=4000):\n",
    "    import time\n",
    "    [time.sleep(1) for i in range(t)]\n",
    "        \n",
    "        \n",
    "def generate_csv(preds, name):\n",
    "    column_names = ['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']\n",
    "    preds_df = pd.DataFrame(preds, columns=column_names)\n",
    "    preds_df.insert(0, 'ID', [i+1 for i in range(len(preds_df))])\n",
    "    preds_df.to_csv(name, index=False)\n",
    "    \n",
    "    \n",
    "def get_ckpt(path):\n",
    "    return torch.load(path, weights_only=False)\n",
    "\n",
    "\n",
    "def cuda_to_np(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, train_dl, epochs):\n",
    "    total_training_steps = len(train_dl) * epochs\n",
    "    warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "\n",
    "\n",
    "def get_stats(tensor, p=True, r=False, minmax=False):\n",
    "    if minmax:\n",
    "        min, max = tensor.min(), tensor.max()\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Min: {min}, Max: {max}, Mean: {mean}, Std: {std}\")\n",
    "        if r: return min, max, mean, std\n",
    "    else:\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Mean: {mean}, Std: {std}\")\n",
    "        if r: return mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-8)\n",
    "\n",
    "\n",
    "def reverse_zscore(tensor, mu, sigma):\n",
    "    return (tensor * sigma) + mu\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "    \n",
    "\n",
    "def get_index(iterable):\n",
    "    return random.randint(0, len(iterable) - 1)\n",
    "\n",
    "\n",
    "def get_indices(iterable, n):\n",
    "    return random.sample(range(len(iterable)), n)\n",
    "\n",
    "\n",
    "def split(inputs, targets, seed):\n",
    "    return train_test_split(\n",
    "        inputs,\n",
    "        targets, \n",
    "        test_size=0.2,\n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    ) \n",
    "\n",
    "\n",
    "def show_waves(waves, dpi=100):\n",
    "    \"\"\"\n",
    "    waves: numpy array of shape (3, N)\n",
    "    Creates three separate figures that stretch wide.\n",
    "    \"\"\"\n",
    "    N = waves.shape[1]\n",
    "    t = np.arange(N)\n",
    "\n",
    "    # Wide aspect ratio; height modest so each window fills width\n",
    "    for i in range(waves.shape[0]):\n",
    "        fig = plt.figure(figsize=(14, 4), dpi=dpi)  # wide figure\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(t, waves[i], linewidth=1)\n",
    "        ax.set_title(f\"Wave {i+1}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()  # reduce margins to use width\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def hf_ds_download(hf_token, repo_id):\n",
    "    login(hf_token[1:])\n",
    "    return snapshot_download(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "def get_spectra_features(X, b=False):\n",
    "    \"\"\"Create multi-channel features from spectra: raw, 1st derivative, 2nd derivative.\"\"\"\n",
    "    X_processed = np.zeros_like(X)\n",
    "    # Baseline correction and SNV\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        poly = np.polyfit(np.arange(X.shape[1]), X[i], 3)\n",
    "        baseline = np.polyval(poly, np.arange(X.shape[1]))\n",
    "        corrected_spec = X[i] - baseline\n",
    "        #X_processed[i] = (corrected_spec - corrected_spec.mean()) / (corrected_spec.std() + 1e-8)\n",
    "        X_processed[i] = corrected_spec\n",
    "        \n",
    "    # Calculate derivatives\n",
    "    deriv1 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=1, axis=1)\n",
    "    deriv2 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=2, axis=1)\n",
    "\n",
    "    if b: return np.stack([X_processed, deriv1, deriv2], axis=1)\n",
    "    return np.stack([deriv1, deriv2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfbbe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'sample_submission.csv')\n",
      "(1, 'timegate.csv')\n",
      "(2, 'mettler_toledo.csv')\n",
      "(3, 'kaiser.csv')\n",
      "(4, 'anton_532.csv')\n",
      "(5, 'transfer_plate.csv')\n",
      "(6, '96_samples.csv')\n",
      "(7, 'tornado.csv')\n",
      "(8, 'tec5.csv')\n",
      "(9, 'metrohm.csv')\n",
      "(10, 'anton_785.csv')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if True:\n",
    "    path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "    files = os.listdir(path)\n",
    "    [print((i, files[i])) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "748ea179-9ff5-4979-b9ce-76c9bec9410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    hf_token = \"xhf_XURkoNhwOIPtEdHfNeRpVkjEwKSkhtigFi\"\n",
    "    path = hf_ds_download(hf_token, \"ArbaazBeg/kaggle-spectogram\")\n",
    "    files = os.listdir(path)\n",
    "    [(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62074de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_comp_data(filepath, is_train=True):\n",
    "    \"\"\"Load and preprocess the Raman spectroscopy data\"\"\"\n",
    "    if is_train:\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Extract target variables\n",
    "        target_cols = ['Glucose (g/L)', 'Sodium Acetate (g/L)', 'Magnesium Acetate (g/L)']\n",
    "        y = df[target_cols].dropna().values\n",
    "        \n",
    "        # Process spectral data\n",
    "        X = df.iloc[:, :-4] # Remove last 4 columns (analyte info and targets)\n",
    "    else:\n",
    "        df = pd.read_csv(filepath, header=None)\n",
    "        X = df\n",
    "        y = None\n",
    "    \n",
    "    # Set column names\n",
    "    X.columns = [\"sample_id\"] + [str(i) for i in range(X.shape[1]-1)]\n",
    "    \n",
    "    # Fill sample_id using forward fill\n",
    "    X['sample_id'] = X['sample_id'].ffill()\n",
    "    \n",
    "    # Clean sample_id\n",
    "    if is_train:\n",
    "        X['sample_id'] = X['sample_id'].str.strip()\n",
    "    else:\n",
    "        X['sample_id'] = X['sample_id'].str.strip().str.replace('sample', '').astype(int)\n",
    "    \n",
    "    # Clean spectral data (remove brackets)\n",
    "    spectral_cols = X.columns[1:]\n",
    "    for col in spectral_cols:\n",
    "        X[col] = X[col].astype(str).str.replace('[', '', regex=False).str.replace(']', '', regex=False)\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fix_val_test_shape(X):\n",
    "    lower_wns = 300\n",
    "    upper_wns = 1942\n",
    "    joint_wns = np.arange(lower_wns, upper_wns + 1)\n",
    "    spectral_values = np.linspace(65, 3350, 2048)\n",
    "\n",
    "    spectra_selection = np.logical_and(\n",
    "        lower_wns <= spectral_values, spectral_values <= upper_wns,\n",
    "    )\n",
    "    wns = spectral_values[spectra_selection]\n",
    "    X = X[:, spectra_selection]\n",
    "    X = np.array([np.interp(joint_wns, xp=wns, fp=spectrum,)for spectrum in X])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e86f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = load_comp_data(os.path.join(path, 'transfer_plate.csv'), is_train=True)\n",
    "test_inputs, _ = load_comp_data(os.path.join(path, '96_samples.csv'), is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "824359e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.drop('sample_id', axis=1).values.reshape(-1, 2, 2048).mean(axis=1)\n",
    "test_inputs = test_inputs.drop('sample_id', axis=1).values.reshape(-1, 2, 2048).mean(axis=1)\n",
    "\n",
    "inputs = fix_val_test_shape(inputs)\n",
    "test_inputs = fix_val_test_shape(test_inputs)\n",
    "\n",
    "# Version 2 Update: Normalise Val and Test data like Train\n",
    "inputs = inputs / np.max(inputs)\n",
    "test_inputs = test_inputs / np.max(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e66bd721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 1643), (96, 3), (96, 1643))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, targets.shape, test_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba2c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "np_dtype_from_torch = {\n",
    "    torch.float32: np.float32,\n",
    "    torch.float64: np.float64,\n",
    "}\n",
    "\n",
    "class SpectralDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra,\n",
    "        concentrations,\n",
    "        dtype=None,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=0.0,\n",
    "        baseline_factor_bound=0.0,\n",
    "        baseline_period_lower_bound=100.0,\n",
    "        baseline_period_upper_bound=200.0,\n",
    "        augment_slope_std=0.0,\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=0,\n",
    "        spectrum_rolling_sigma=0.0,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    ):\n",
    "        self.dtype = dtype or torch.float32\n",
    "        self.combine_spectra_range = combine_spectra_range\n",
    "        self.baseline_factor_bound = baseline_factor_bound\n",
    "        self.augment_slope_std = augment_slope_std\n",
    "        self.augment_intercept_std = augment_intersept_std\n",
    "        self.baseline_period_lower_bound = baseline_period_lower_bound\n",
    "        self.baseline_period_upper_bound = baseline_period_upper_bound\n",
    "        self.rolling_bound = rolling_bound\n",
    "        self.spectrum_rolling_sigma = spectrum_rolling_sigma\n",
    "        self.augmentation_weight = torch.tensor(augmentation_weight, dtype=dtype)\n",
    "        self.original_dp_weight = original_datapoint_weight\n",
    "\n",
    "        # normalize spectra\n",
    "        spectra = torch.tensor(spectra, dtype=dtype)\n",
    "\n",
    "        if spectra_mean_std is None:\n",
    "            self.s_mean = torch.mean(spectra)\n",
    "            self.s_std = torch.std(spectra)\n",
    "        else:\n",
    "            self.s_mean, self.s_std = spectra_mean_std\n",
    "\n",
    "        self.spectra = torch.divide(\n",
    "            torch.subtract(spectra, self.s_mean),\n",
    "            self.s_std,\n",
    "        )\n",
    "\n",
    "        self.dummy_wns = np.tile(\n",
    "            np.arange(\n",
    "                0., 1., 1. / self.spectra.shape[2],\n",
    "                dtype=np_dtype_from_torch[self.dtype]\n",
    "            )[None, :self.spectra.shape[2]],\n",
    "            (self.spectra.shape[1], 1),\n",
    "        )\n",
    "\n",
    "        # normalize concentrations\n",
    "        concentrations = torch.tensor(concentrations, dtype=dtype)\n",
    "        if concentration_mean_std is None:\n",
    "            self.concentration_means = torch.nanmean(concentrations, dim=0)\n",
    "\n",
    "            self.concentration_stds = torch.maximum(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        torch.std(col[torch.logical_not(torch.isnan(col))])\n",
    "                        for col in concentrations.T\n",
    "                    ]\n",
    "                ),\n",
    "                torch.tensor([1e-3] * concentrations.shape[1]),\n",
    "            )\n",
    "        else:\n",
    "            self.concentration_means = concentration_mean_std[0]\n",
    "            self.concentration_stds = concentration_mean_std[1]\n",
    "\n",
    "        self.concentrations = torch.divide(\n",
    "            torch.subtract(\n",
    "                concentrations,\n",
    "                self.concentration_means,\n",
    "            ),\n",
    "            self.concentration_stds,\n",
    "        )\n",
    "\n",
    "    def pick_two(self, max_idx=None):\n",
    "        max_idx = max_idx or len(self)\n",
    "        return random.choices(range(max_idx), k=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.concentrations)\n",
    "\n",
    "    def augment_spectra(self, spectra):\n",
    "        if self.augment_slope_std > 0.0:\n",
    "\n",
    "            def spectrum_approximation(x, slope, intercept):\n",
    "                return (slope * x + intercept).reshape(-1, 1)[:, 0]\n",
    "\n",
    "            slope, inter = scipy.optimize.curve_fit(\n",
    "                spectrum_approximation,\n",
    "                self.dummy_wns,\n",
    "                spectra.reshape(-1, 1)[:, 0],\n",
    "                p0=np.random.rand(2),\n",
    "            )[0]\n",
    "\n",
    "            new_slope = slope * (\n",
    "                    np.random.gamma(\n",
    "                        shape=1. / self.augment_slope_std,\n",
    "                        scale=self.augment_slope_std,\n",
    "                        size=1,\n",
    "                    )\n",
    "            )[0]\n",
    "            new_intercept = inter * (\n",
    "                1.0 + np.random.randn(1) * self.augment_intercept_std\n",
    "            )[0]\n",
    "            spectra += torch.tensor(\n",
    "                (new_slope - slope)\n",
    "            ) * self.dummy_wns + new_intercept - inter\n",
    "\n",
    "        factor = self.baseline_factor_bound * torch.rand(size=(1,))\n",
    "        offset = torch.rand(size=(1,)) * 2.0 * torch.pi\n",
    "        period = self.baseline_period_lower_bound + (\n",
    "            self.baseline_period_upper_bound - self.baseline_period_lower_bound\n",
    "        ) * torch.rand(size=(1,))\n",
    "        permutations = factor * torch.cos(\n",
    "            2.0 * torch.pi / period * self.dummy_wns + offset\n",
    "        )\n",
    "        return self.roll_spectrum(\n",
    "            spectra + permutations * spectra,\n",
    "            delta=random.randint(-self.rolling_bound, self.rolling_bound),\n",
    "        )\n",
    "\n",
    "    def roll_spectrum(self, spectra, delta):\n",
    "        num_spectra = spectra.shape[0]\n",
    "        rolled_spectra = np.roll(spectra, delta, axis=1)\n",
    "        if delta > 0:\n",
    "            rolled_spectra[:, :delta] = (\n",
    "                np.random.rand(num_spectra, delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta:(delta + 1)]\n",
    "        elif delta < 0:\n",
    "            rolled_spectra[:, delta:] = (\n",
    "                np.random.rand(num_spectra, -delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta - 1:delta]\n",
    "        return rolled_spectra\n",
    "\n",
    "    def combine_k_items(self, indices, weights):\n",
    "        return (\n",
    "            # spectra\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None, None], self.spectra[indices, :, :]),\n",
    "                dim=0,\n",
    "            ),\n",
    "            # concentrations\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None], self.concentrations[indices, :]),\n",
    "                dim=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.combine_spectra_range < 1e-12:\n",
    "            spectrum = self.spectra[idx]\n",
    "            spectrum = self.augment_spectra(spectrum)\n",
    "            return {\n",
    "                \"spectra\": spectrum,\n",
    "                \"concentrations\": self.concentrations[idx],\n",
    "                \"label_weight\": torch.tensor(1.0, dtype=self.dtype),\n",
    "            }\n",
    "        else:\n",
    "            if random.random() < self.original_dp_weight:\n",
    "                one_weight = 1.\n",
    "                label_weight = torch.tensor(1.0, dtype=self.dtype)\n",
    "            else:\n",
    "                one_weight = random.uniform(0.0, self.combine_spectra_range)\n",
    "                label_weight = self.augmentation_weight\n",
    "            weights = torch.tensor([one_weight, (1 - one_weight)])\n",
    "            # just pick two random indices\n",
    "            indices = random.choices(range(len(self)), k=2)\n",
    "\n",
    "            mixed_spectra, mixed_concentrations = self.combine_k_items(\n",
    "                indices=indices,\n",
    "                weights=weights,\n",
    "            )\n",
    "            mixed_spectra = self.augment_spectra(mixed_spectra)\n",
    "            return mixed_spectra, mixed_concentrations, label_weight\n",
    "\n",
    "\n",
    "config = {\n",
    "    'initial_cnn_channels': 32,\n",
    "    'cnn_channel_factor': 1.279574024454846,\n",
    "    'num_cnn_layers': 8,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'activation_function': 'ELU',\n",
    "    'fc_dropout': 0.10361700399831791,\n",
    "    'lr': 0.001,\n",
    "    'gamma': 0.9649606352621118,\n",
    "    'baseline_factor_bound': 0.748262317340447,\n",
    "    'baseline_period_lower_bound': 0.9703081695287203,\n",
    "    'baseline_period_span': 19.79744237606427,\n",
    "    'original_datapoint_weight': 0.4335003268130408,\n",
    "    'augment_slope_std': 0.08171025264382692,\n",
    "    'batch_size': 32,\n",
    "    'fc_dims': 226,\n",
    "    'rolling_bound': 2,\n",
    "    'num_blocks': 2,\n",
    "}\n",
    "\n",
    "def get_dataset(inputs, targets, config, inputs_mean_std=None, targets_mean_std=None):\n",
    "    return SpectralDataset(\n",
    "        spectra=inputs[:, None, :],\n",
    "        concentrations=targets,\n",
    "        dtype=torch.float32,\n",
    "        spectra_mean_std=inputs_mean_std,\n",
    "        concentration_mean_std=targets_mean_std,\n",
    "        combine_spectra_range=1.0,\n",
    "        baseline_factor_bound=config[\"baseline_factor_bound\"],\n",
    "        baseline_period_lower_bound=config[\"baseline_period_lower_bound\"],\n",
    "        baseline_period_upper_bound=(config[\"baseline_period_lower_bound\"] + config[\"baseline_period_span\"]),\n",
    "        augment_slope_std=config[\"augment_slope_std\"],\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=config[\"rolling_bound\"],\n",
    "        spectrum_rolling_sigma=0.01,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c11034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+5232)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds, train_batch_size, eval_batch_size):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e343398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "        if DROPOUT: neptune_run[\"h_parameters\"] = {\"dropout\": DROPOUT}\n",
    "        if DROP_PATH_RATE: neptune_run[\"h_parameters\"] = {\"drop_path_rate\": DROP_PATH_RATE}\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4227b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().float().numpy()\n",
    "    targets = targets.cpu().detach().float().numpy()\n",
    "    \n",
    "    dim1 = r2_score(targets[:, 0], preds[:, 0])\n",
    "    dim2 = r2_score(targets[:, 1], preds[:, 1])\n",
    "    dim3 = r2_score(targets[:, 2], preds[:, 2])\n",
    "    \n",
    "    return dim1, dim2, dim3, r2_score(targets, preds)\n",
    "\n",
    "\n",
    "class MSEIgnoreNans(_Loss):\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        weights: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        mask = torch.isfinite(target)\n",
    "        mse = torch.mean(\n",
    "            torch.mul(\n",
    "                torch.square(input[mask] - target[mask]),\n",
    "                torch.tile(weights[:, None], dims=(1, target.shape[1]))[mask],\n",
    "            )\n",
    "        )\n",
    "        return torch.where(\n",
    "            torch.isfinite(mse),\n",
    "            mse,\n",
    "            torch.tensor(0.).to(target.device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ccc0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Identity(torch.torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "# this is not a resnet yet\n",
    "class ReZeroBlock(torch.torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        activation_function,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dtype,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(ReZeroBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = torch.torch.nn.BatchNorm1d\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = divmod(kernel_size, 2)[0] if stride == 1 else 0\n",
    "\n",
    "        # does not change spatial dimension\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.bn1 = norm_layer(out_channels, dtype=dtype)\n",
    "        # Both self.conv2 and self.downsample layers\n",
    "        # downsample the input when stride != 1\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            groups=out_channels,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        if stride > 1:\n",
    "            down_conv = torch.nn.Conv1d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "                dtype=dtype,\n",
    "                # groups=out_channels,\n",
    "            )\n",
    "        else:\n",
    "            down_conv = Identity()\n",
    "\n",
    "        self.down_sample = torch.nn.Sequential(\n",
    "            down_conv,\n",
    "            norm_layer(out_channels),\n",
    "        )\n",
    "        self.bn2 = norm_layer(out_channels, dtype=dtype)\n",
    "        # does not change the spatial dimension\n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.bn3 = norm_layer(out_channels, dtype=dtype)\n",
    "        self.activation = activation_function(inplace=True)\n",
    "        self.factor = torch.torch.nn.parameter.Parameter(torch.tensor(0.0, dtype=dtype))\n",
    "\n",
    "    def next_spatial_dim(self, last_spatial_dim):\n",
    "        return math.floor(\n",
    "            (last_spatial_dim + 2 * self.padding - self.kernel_size)\n",
    "            / self.stride + 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        # not really the identity, but kind of\n",
    "        identity = self.down_sample(x)\n",
    "\n",
    "        return self.activation(out * self.factor + identity)\n",
    "\n",
    "\n",
    "class ResNetEncoder(torch.torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectrum_size,\n",
    "        cnn_encoder_channel_dims,\n",
    "        activation_function,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dtype,\n",
    "        num_blocks,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "\n",
    "        self.spatial_dims = [spectrum_size]\n",
    "        layers = []\n",
    "        for in_channels, out_channels in zip(\n",
    "            cnn_encoder_channel_dims[:-1],\n",
    "            cnn_encoder_channel_dims[1:],\n",
    "        ):\n",
    "            block = ReZeroBlock(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                activation_function=activation_function,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            layers.append(block)\n",
    "            self.spatial_dims.append(block.next_spatial_dim(self.spatial_dims[-1]))\n",
    "            for _ in range(num_blocks - 1):\n",
    "                block = ReZeroBlock(\n",
    "                    in_channels=out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    activation_function=activation_function,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    dtype=dtype,\n",
    "                )\n",
    "                layers.append(block)\n",
    "                self.spatial_dims.append(block.next_spatial_dim(self.spatial_dims[-1]))\n",
    "\n",
    "        self.resnet_layers = torch.torch.nn.Sequential(*layers)\n",
    "        if verbose:\n",
    "            print(\"CNN Encoder Channel Dims: %s\" % (cnn_encoder_channel_dims))\n",
    "            print(\"CNN Encoder Spatial Dims: %s\" % (self.spatial_dims))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet_layers(x)\n",
    "\n",
    "\n",
    "class ReZeroNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra_channels,\n",
    "        spectra_size,\n",
    "        initial_cnn_channels,\n",
    "        cnn_channel_factor,\n",
    "        num_cnn_layers,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        activation_function,\n",
    "        fc_dims,\n",
    "        fc_dropout=0.0,\n",
    "        dtype=None,\n",
    "        verbose=False,\n",
    "        fc_output_channels=1,\n",
    "        num_blocks=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc_output_channels = fc_output_channels\n",
    "        self.dtype = dtype or torch.float32\n",
    "\n",
    "        activation_function = getattr(torch.nn, activation_function)\n",
    "\n",
    "        # Setup CNN Encoder\n",
    "        cnn_encoder_channel_dims = [spectra_channels] + [\n",
    "            int(initial_cnn_channels * (cnn_channel_factor**idx))\n",
    "            for idx in range(num_cnn_layers)\n",
    "        ]\n",
    "        self.cnn_encoder = ResNetEncoder(\n",
    "            spectrum_size=spectra_size,\n",
    "            cnn_encoder_channel_dims=cnn_encoder_channel_dims,\n",
    "            activation_function=activation_function,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            num_blocks=num_blocks,\n",
    "            dtype=dtype,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.fc_dims = [\n",
    "            int(\n",
    "                self.cnn_encoder.spatial_dims[-1]\n",
    "            ) * int(cnn_encoder_channel_dims[-1])\n",
    "        ] + fc_dims\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Fc Dims: %s\" % self.fc_dims)\n",
    "        fc_layers = []\n",
    "        for idx, (in_dim, out_dim) in enumerate(\n",
    "                zip(self.fc_dims[:-2], self.fc_dims[1:-1])\n",
    "        ):\n",
    "            fc_layers.append(torch.nn.Linear(in_dim, out_dim))\n",
    "            fc_layers.append(torch.nn.ELU())\n",
    "            fc_layers.append(torch.nn.Dropout(fc_dropout / (2 ** idx)))\n",
    "        fc_layers.append(\n",
    "            torch.nn.Linear(\n",
    "                self.fc_dims[-2],\n",
    "                self.fc_dims[-1] * self.fc_output_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.fc_net = torch.nn.Sequential(*fc_layers)\n",
    "        if verbose:\n",
    "            num_params = sum(p.numel() for p in self.parameters())\n",
    "            print(\"Number of Parameters: %s\" % num_params)\n",
    "\n",
    "    def forward(self, spectra):\n",
    "        embeddings = self.cnn_encoder(spectra)\n",
    "        forecast = self.fc_net(embeddings.view(-1, self.fc_dims[0]))\n",
    "        if self.fc_output_channels > 1:\n",
    "            forecast = forecast.reshape(\n",
    "                -1, self.fc_output_channels, self.fc_dims[-1]\n",
    "            )\n",
    "        return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1354d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm for 1D with channels_last (N, L, C) or channels_first (N, C, L).\"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        if data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.data_format = data_format\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            # x: (N, L, C); normalize over last dim C\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        else:\n",
    "            # x: (N, C, L); normalize over channel dim C, per position L\n",
    "            u = x.mean(dim=1, keepdim=True)                       # (N,1,L)\n",
    "            s = (x - u).pow(2).mean(dim=1, keepdim=True)          # (N,1,L)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)                # (N,C,L)\n",
    "            # Broadcast weight/bias over (C,1)\n",
    "            x = self.weight[:, None] * x + self.bias[:, None]\n",
    "            return x\n",
    "        \n",
    "   \n",
    "class GRN(nn.Module):\n",
    "    \"\"\"GRN for 1D sequences in channels-last format (N, L, C).\"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # Broadcast over (N, L, C): parameters shape to (1,1,C)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, L, C)\n",
    "        # L2 norm across channels at each position\n",
    "        Gx = torch.norm(x, p=2, dim=-1, keepdim=True)                 # (N, L, 1)\n",
    "        # Normalize by mean over positions (sequence length)\n",
    "        Nx = Gx / (Gx.mean(dim=1, keepdim=True) + self.eps)           # (N, L, 1)\n",
    "        return self.gamma * (x * Nx) + self.beta + x                  # (N, L, C)\n",
    "    \n",
    "    \n",
    "from timm.layers import trunc_normal_, DropPath\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" ConvNeXtV2 Block (1D).\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv1d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise convs via linear\n",
    "        self.act = nn.GELU()\n",
    "        self.grn = GRN(4 * dim)\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)                 # (N, C, L)\n",
    "        x = x.transpose(1, 2)              # (N, L, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.pwconv2(x)\n",
    "        x = x.transpose(1, 2)              # (N, C, L)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ConvNeXtV2(nn.Module):\n",
    "    \"\"\" ConvNeXt V2 (1D)\n",
    "\n",
    "    Args:\n",
    "        in_chans (int): Number of input channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=1, num_classes=3,\n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768],\n",
    "                 drop_path_rate=0., dropout=0.1, head_init_scale=1.\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.depths = depths\n",
    "        self.downsample_layers = nn.ModuleList()  # stem + 3 intermediate downsampling conv layers\n",
    "\n",
    "        # Stem: 1D patchify with stride 4\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv1d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "        # Three downsample layers: halve length each stage\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                nn.Conv1d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        # Stages with residual Blocks (already 1D)\n",
    "        self.stages = nn.ModuleList()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j]) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        # Final norm and head: pool over length, then LayerNorm over channels\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.head.weight.data.mul_(head_init_scale)\n",
    "        self.head.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # x: (N, C_in, L)\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)  # (N, C_i, L_i)\n",
    "            x = self.stages[i](x)             # (N, C_i, L_i)\n",
    "\n",
    "        # Global average pooling over length -> (N, C)\n",
    "        x = x.mean(dim=-1)\n",
    "\n",
    "        # Final LayerNorm expects (N, C)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    #def convnext_pico(**kwargs):\n",
    "#    model = ConvNeXtV2(depths=[2, 2, 6, 2], dims=[64, 128, 256, 512], **kwargs)\n",
    "#    return model\n",
    "\n",
    "def convnextv2_atto(**kwargs):\n",
    "    model = ConvNeXtV2(depths=[2, 2, 6, 2], dims=[40, 80, 160, 320], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "035dd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    device,\n",
    "    amp_dtype,\n",
    "    scheduler,\n",
    "    train_dl,\n",
    "    eval_dl,\n",
    "    loss_fn,\n",
    "    epochs,\n",
    "    checkpoint_name,\n",
    "    score=-float(\"inf\"),\n",
    "    neptune_run=None,\n",
    "    p=True,\n",
    "):  \n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for inputs, targets, weights in train_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(device_type=device, dtype=amp_dtype, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "                  \n",
    "            if amp_dtype == torch.bfloat16:                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            scheduler.step()\n",
    "            if neptune_run is not None:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            total_loss += loss.detach().cpu()\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        all_logits = torch.cat(all_logits)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "        total_loss = total_loss / len(train_dl)\n",
    "        \n",
    "        model.eval()\n",
    "        eval_total_loss = 0.0\n",
    "        eval_all_logits = []\n",
    "        eval_all_targets = []\n",
    "\n",
    "        for inputs, targets, weights in eval_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                with torch.amp.autocast(device_type=device, dtype=amp_dtype, cache_enabled=True):\n",
    "                    logits = model(inputs)\n",
    "                    loss = loss_fn(logits, targets)\n",
    "\n",
    "            eval_total_loss += loss.detach().cpu()\n",
    "            eval_all_logits.append(logits.detach().cpu())\n",
    "            eval_all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        eval_all_logits = torch.cat(eval_all_logits)\n",
    "        eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "        eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "        eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "        \n",
    "        if eval_r2 > score:\n",
    "            score = eval_r2\n",
    "            data = {\"state_dict\": model.state_dict()}\n",
    "            data[\"epoch\"] = epoch \n",
    "            data[\"score\"] = score\n",
    "            torch.save(data, f\"/kaggle/working/{checkpoint_name}\")\n",
    "        \n",
    "        if neptune_run is not None:\n",
    "            neptune_run[\"train/loss\"].append(total_loss)\n",
    "            neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "            neptune_run[\"train/r2\"].append(r2)\n",
    "            neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "            neptune_run[\"train/one\"].append(one)\n",
    "            neptune_run[\"train/two\"].append(two)\n",
    "            neptune_run[\"train/three\"].append(three)\n",
    "            neptune_run[\"eval/one\"].append(eval_one)\n",
    "            neptune_run[\"eval/two\"].append(eval_two)\n",
    "            neptune_run[\"eval/three\"].append(eval_three)\n",
    "            \n",
    "        if p and epoch % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, \"\n",
    "                f\"train/loss: {total_loss:.4f}, \"\n",
    "                f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "                f\"train/r2: {r2:.4f}, \"\n",
    "                f\"eval/r2: {eval_r2:.4f}, \"\n",
    "                f\"train/one: {one:.4f}, \"\n",
    "                f\"train/two: {two:.4f}, \"\n",
    "                f\"train/three: {three:.4f}, \"\n",
    "                f\"eval/one: {eval_one:.4f}, \"\n",
    "                f\"eval/two: {eval_two:.4f}, \"\n",
    "                f\"eval/three: {eval_three:.4f} \"\n",
    "            )\n",
    "            \n",
    "    if neptune_run is not None: neptune_run.stop()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dd5d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "\n",
    "DROPOUT = 0.5\n",
    "DROP_PATH_RATE = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESUME = False\n",
    "\n",
    "config[\"dtype\"] = torch.float32\n",
    "config[\"spectra_size\"] = 1643\n",
    "config[\"spectra_channels\"] = 1\n",
    "config[\"fc_dims\"] = [\n",
    "    config[\"fc_dims\"],\n",
    "    int(config[\"fc_dims\"] / 2),\n",
    "    3,\n",
    "]\n",
    "\n",
    "mse_loss_function = MSEIgnoreNans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54dd678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734309\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7968680c3a41d6beeab1ec36b423bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0304, eval/loss: 0.8932, train/r2: -0.0086, eval/r2: -0.1388, train/one: -0.0038, train/two: -0.0260, train/three: 0.0040, eval/one: -0.0053, eval/two: -0.0083, eval/three: -0.4029 \n",
      "Epoch: 5, train/loss: 0.9350, eval/loss: 1.0406, train/r2: -0.0284, eval/r2: -0.1738, train/one: -0.0783, train/two: -0.0111, train/three: 0.0042, eval/one: -0.3386, eval/two: -0.1173, eval/three: -0.0653 \n",
      "Epoch: 10, train/loss: 0.8460, eval/loss: 0.9769, train/r2: 0.0862, eval/r2: -0.1147, train/one: -0.0091, train/two: 0.0030, train/three: 0.2647, eval/one: 0.0020, eval/two: -0.2995, eval/three: -0.0464 \n",
      "Epoch: 15, train/loss: 0.7863, eval/loss: 0.9368, train/r2: 0.2077, eval/r2: 0.0252, train/one: -0.0025, train/two: -0.0413, train/three: 0.6668, eval/one: -0.0930, eval/two: -0.0519, eval/three: 0.2203 \n",
      "Epoch: 20, train/loss: 0.7106, eval/loss: 0.8992, train/r2: 0.2487, eval/r2: -0.0785, train/one: -0.0533, train/two: -0.0302, train/three: 0.8296, eval/one: 0.0157, eval/two: -0.0972, eval/three: -0.1541 \n",
      "Epoch: 25, train/loss: 0.6415, eval/loss: 0.8333, train/r2: 0.3095, eval/r2: 0.1131, train/one: 0.0440, train/two: 0.0710, train/three: 0.8135, eval/one: 0.0277, eval/two: -0.1714, eval/three: 0.4830 \n",
      "Epoch: 30, train/loss: 0.6148, eval/loss: 0.6003, train/r2: 0.3312, eval/r2: 0.0119, train/one: 0.0474, train/two: 0.1159, train/three: 0.8303, eval/one: -0.2500, eval/two: -0.3298, eval/three: 0.6157 \n",
      "Epoch: 35, train/loss: 0.5777, eval/loss: 0.4175, train/r2: 0.4088, eval/r2: 0.3050, train/one: 0.2353, train/two: 0.1465, train/three: 0.8446, eval/one: 0.1707, eval/two: -0.0916, eval/three: 0.8359 \n",
      "Epoch: 40, train/loss: 0.3927, eval/loss: 0.4504, train/r2: 0.6222, eval/r2: 0.4316, train/one: 0.5653, train/two: 0.5407, train/three: 0.7607, eval/one: 0.5185, eval/two: 0.0131, eval/three: 0.7633 \n",
      "Epoch: 45, train/loss: 0.2975, eval/loss: 0.3101, train/r2: 0.7305, eval/r2: 0.6595, train/one: 0.6418, train/two: 0.6445, train/three: 0.9051, eval/one: 0.5843, eval/two: 0.4960, eval/three: 0.8981 \n",
      "Epoch: 50, train/loss: 0.2176, eval/loss: 0.2641, train/r2: 0.7781, eval/r2: 0.6625, train/one: 0.6923, train/two: 0.7367, train/three: 0.9051, eval/one: 0.5284, eval/two: 0.6205, eval/three: 0.8385 \n",
      "Epoch: 55, train/loss: 0.1893, eval/loss: 0.1829, train/r2: 0.8205, eval/r2: 0.7444, train/one: 0.7660, train/two: 0.7942, train/three: 0.9012, eval/one: 0.7585, eval/two: 0.7633, eval/three: 0.7115 \n",
      "Epoch: 60, train/loss: 0.1316, eval/loss: 0.1930, train/r2: 0.8431, eval/r2: 0.7538, train/one: 0.8356, train/two: 0.7823, train/three: 0.9113, eval/one: 0.7516, eval/two: 0.8285, eval/three: 0.6813 \n",
      "Epoch: 65, train/loss: 0.1110, eval/loss: 0.1313, train/r2: 0.8843, eval/r2: 0.7390, train/one: 0.8624, train/two: 0.8975, train/three: 0.8931, eval/one: 0.8153, eval/two: 0.8070, eval/three: 0.5947 \n",
      "Epoch: 70, train/loss: 0.1118, eval/loss: 0.1123, train/r2: 0.8841, eval/r2: 0.8130, train/one: 0.8661, train/two: 0.8598, train/three: 0.9264, eval/one: 0.6965, eval/two: 0.8860, eval/three: 0.8566 \n",
      "Epoch: 75, train/loss: 0.0899, eval/loss: 0.1497, train/r2: 0.9044, eval/r2: 0.8154, train/one: 0.8991, train/two: 0.8898, train/three: 0.9243, eval/one: 0.7293, eval/two: 0.8560, eval/three: 0.8610 \n",
      "Epoch: 80, train/loss: 0.0985, eval/loss: 0.1477, train/r2: 0.9032, eval/r2: 0.8327, train/one: 0.8914, train/two: 0.9139, train/three: 0.9043, eval/one: 0.7464, eval/two: 0.9007, eval/three: 0.8510 \n",
      "Epoch: 85, train/loss: 0.0956, eval/loss: 0.1452, train/r2: 0.8982, eval/r2: 0.7840, train/one: 0.8655, train/two: 0.8982, train/three: 0.9307, eval/one: 0.8615, eval/two: 0.6541, eval/three: 0.8364 \n",
      "Epoch: 90, train/loss: 0.1056, eval/loss: 0.1100, train/r2: 0.8872, eval/r2: 0.8503, train/one: 0.8454, train/two: 0.8920, train/three: 0.9241, eval/one: 0.6934, eval/two: 0.9288, eval/three: 0.9287 \n",
      "Epoch: 95, train/loss: 0.1097, eval/loss: 0.1242, train/r2: 0.9154, eval/r2: 0.8363, train/one: 0.9125, train/two: 0.9140, train/three: 0.9197, eval/one: 0.7834, eval/two: 0.8310, eval/three: 0.8944 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 169 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 169 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-271/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3991fb29d8f64bbc884c295664d2d156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0524, eval/loss: 0.9440, train/r2: -0.0132, eval/r2: -0.1064, train/one: -0.0182, train/two: -0.0079, train/three: -0.0137, eval/one: -0.1101, eval/two: -0.2053, eval/three: -0.0037 \n",
      "Epoch: 5, train/loss: 0.9827, eval/loss: 0.8072, train/r2: 0.0052, eval/r2: -0.0260, train/one: -0.0129, train/two: -0.0015, train/three: 0.0300, eval/one: -0.0555, eval/two: -0.0056, eval/three: -0.0168 \n",
      "Epoch: 10, train/loss: 0.9143, eval/loss: 0.8156, train/r2: 0.0867, eval/r2: -0.0017, train/one: -0.0511, train/two: 0.0034, train/three: 0.3078, eval/one: 0.0107, eval/two: -0.0237, eval/three: 0.0080 \n",
      "Epoch: 15, train/loss: 0.8537, eval/loss: 0.7804, train/r2: 0.2245, eval/r2: 0.0437, train/one: 0.0065, train/two: 0.0057, train/three: 0.6613, eval/one: 0.0235, eval/two: -0.0259, eval/three: 0.1334 \n",
      "Epoch: 20, train/loss: 0.6835, eval/loss: 1.1853, train/r2: 0.2789, eval/r2: -0.0958, train/one: 0.0620, train/two: -0.0517, train/three: 0.8265, eval/one: -0.1064, eval/two: -0.3436, eval/three: 0.1626 \n",
      "Epoch: 25, train/loss: 0.7259, eval/loss: 1.0400, train/r2: 0.2418, eval/r2: -0.3529, train/one: 0.0364, train/two: -0.0071, train/three: 0.6962, eval/one: -0.5735, eval/two: -0.8661, eval/three: 0.3808 \n",
      "Epoch: 30, train/loss: 0.8270, eval/loss: 0.7041, train/r2: 0.2536, eval/r2: 0.1765, train/one: 0.0327, train/two: -0.0964, train/three: 0.8245, eval/one: 0.0429, eval/two: -0.3671, eval/three: 0.8536 \n",
      "Epoch: 35, train/loss: 0.6322, eval/loss: 0.7871, train/r2: 0.3549, eval/r2: 0.2963, train/one: 0.2507, train/two: -0.0393, train/three: 0.8532, eval/one: 0.0313, eval/two: 0.0134, eval/three: 0.8443 \n",
      "Epoch: 40, train/loss: 0.5194, eval/loss: 0.6139, train/r2: 0.4436, eval/r2: 0.3703, train/one: 0.3611, train/two: 0.0758, train/three: 0.8939, eval/one: 0.2612, eval/two: 0.0731, eval/three: 0.7768 \n",
      "Epoch: 45, train/loss: 0.3971, eval/loss: 0.3812, train/r2: 0.5956, eval/r2: 0.5816, train/one: 0.6064, train/two: 0.3379, train/three: 0.8425, eval/one: 0.5896, eval/two: 0.2960, eval/three: 0.8591 \n",
      "Epoch: 50, train/loss: 0.2980, eval/loss: 0.3230, train/r2: 0.7128, eval/r2: 0.6794, train/one: 0.6525, train/two: 0.5913, train/three: 0.8944, eval/one: 0.6263, eval/two: 0.5543, eval/three: 0.8577 \n",
      "Epoch: 55, train/loss: 0.1932, eval/loss: 0.2409, train/r2: 0.7999, eval/r2: 0.6945, train/one: 0.7574, train/two: 0.7421, train/three: 0.9001, eval/one: 0.5870, eval/two: 0.6632, eval/three: 0.8334 \n",
      "Epoch: 60, train/loss: 0.1813, eval/loss: 0.1742, train/r2: 0.8351, eval/r2: 0.7923, train/one: 0.8380, train/two: 0.7752, train/three: 0.8920, eval/one: 0.8127, eval/two: 0.7476, eval/three: 0.8166 \n",
      "Epoch: 65, train/loss: 0.1652, eval/loss: 0.1381, train/r2: 0.8483, eval/r2: 0.7574, train/one: 0.8381, train/two: 0.8279, train/three: 0.8788, eval/one: 0.5798, eval/two: 0.8238, eval/three: 0.8687 \n",
      "Epoch: 70, train/loss: 0.1102, eval/loss: 0.1811, train/r2: 0.8917, eval/r2: 0.7995, train/one: 0.9030, train/two: 0.8627, train/three: 0.9094, eval/one: 0.8400, eval/two: 0.6931, eval/three: 0.8653 \n",
      "Epoch: 75, train/loss: 0.1148, eval/loss: 0.1285, train/r2: 0.8887, eval/r2: 0.8561, train/one: 0.8713, train/two: 0.8916, train/three: 0.9032, eval/one: 0.8283, eval/two: 0.8523, eval/three: 0.8876 \n",
      "Epoch: 80, train/loss: 0.1031, eval/loss: 0.0970, train/r2: 0.8940, eval/r2: 0.8931, train/one: 0.8625, train/two: 0.9007, train/three: 0.9188, eval/one: 0.8499, eval/two: 0.9024, eval/three: 0.9269 \n",
      "Epoch: 85, train/loss: 0.1301, eval/loss: 0.1062, train/r2: 0.8521, eval/r2: 0.8443, train/one: 0.8115, train/two: 0.8281, train/three: 0.9168, eval/one: 0.8355, eval/two: 0.8814, eval/three: 0.8160 \n",
      "Epoch: 90, train/loss: 0.1186, eval/loss: 0.1046, train/r2: 0.8786, eval/r2: 0.8697, train/one: 0.8686, train/two: 0.8626, train/three: 0.9047, eval/one: 0.8894, eval/two: 0.8509, eval/three: 0.8688 \n",
      "Epoch: 95, train/loss: 0.1101, eval/loss: 0.0955, train/r2: 0.8921, eval/r2: 0.8468, train/one: 0.8935, train/two: 0.8606, train/three: 0.9223, eval/one: 0.8511, eval/two: 0.9094, eval/three: 0.7798 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 129 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 129 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-272/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da7608306a14e1a9c0cee2e6d43c0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 0.9650, eval/loss: 1.3911, train/r2: -0.0220, eval/r2: -0.0207, train/one: -0.0246, train/two: -0.0036, train/three: -0.0379, eval/one: -0.0274, eval/two: -0.0290, eval/three: -0.0056 \n",
      "Epoch: 5, train/loss: 0.9236, eval/loss: 0.9266, train/r2: 0.0051, eval/r2: -0.1875, train/one: -0.0278, train/two: -0.0024, train/three: 0.0455, eval/one: -0.2048, eval/two: -0.1039, eval/three: -0.2538 \n",
      "Epoch: 10, train/loss: 0.8926, eval/loss: 1.1786, train/r2: 0.0646, eval/r2: -0.0906, train/one: -0.0333, train/two: 0.0049, train/three: 0.2223, eval/one: -0.0090, eval/two: -0.2742, eval/three: 0.0115 \n",
      "Epoch: 15, train/loss: 0.7762, eval/loss: 0.9625, train/r2: 0.2232, eval/r2: -0.1435, train/one: 0.0020, train/two: 0.0063, train/three: 0.6613, eval/one: -0.0572, eval/two: -0.2940, eval/three: -0.0794 \n",
      "Epoch: 20, train/loss: 0.7193, eval/loss: 0.8702, train/r2: 0.3006, eval/r2: 0.0947, train/one: 0.0373, train/two: 0.0230, train/three: 0.8413, eval/one: 0.0544, eval/two: -0.3327, eval/three: 0.5624 \n",
      "Epoch: 25, train/loss: 0.6171, eval/loss: 0.7604, train/r2: 0.3005, eval/r2: 0.1988, train/one: 0.0868, train/two: -0.0152, train/three: 0.8299, eval/one: -0.0495, eval/two: -0.1343, eval/three: 0.7802 \n",
      "Epoch: 30, train/loss: 0.6208, eval/loss: 0.5082, train/r2: 0.3614, eval/r2: 0.2655, train/one: 0.1609, train/two: 0.0896, train/three: 0.8337, eval/one: 0.2254, eval/two: -0.2984, eval/three: 0.8694 \n",
      "Epoch: 35, train/loss: 0.4931, eval/loss: 0.5203, train/r2: 0.4884, eval/r2: 0.3264, train/one: 0.4145, train/two: 0.1682, train/three: 0.8825, eval/one: 0.2336, eval/two: 0.1443, eval/three: 0.6015 \n",
      "Epoch: 40, train/loss: 0.3490, eval/loss: 0.3564, train/r2: 0.6583, eval/r2: 0.5890, train/one: 0.6359, train/two: 0.4858, train/three: 0.8532, eval/one: 0.6476, eval/two: 0.2556, eval/three: 0.8638 \n",
      "Epoch: 45, train/loss: 0.2110, eval/loss: 0.3732, train/r2: 0.7683, eval/r2: 0.5248, train/one: 0.7494, train/two: 0.6968, train/three: 0.8588, eval/one: 0.5915, eval/two: 0.2418, eval/three: 0.7412 \n",
      "Epoch: 50, train/loss: 0.1490, eval/loss: 0.1246, train/r2: 0.8567, eval/r2: 0.8560, train/one: 0.8322, train/two: 0.8368, train/three: 0.9012, eval/one: 0.8583, eval/two: 0.7921, eval/three: 0.9176 \n",
      "Epoch: 55, train/loss: 0.1407, eval/loss: 0.2475, train/r2: 0.8524, eval/r2: 0.7405, train/one: 0.7950, train/two: 0.8540, train/three: 0.9083, eval/one: 0.7673, eval/two: 0.7145, eval/three: 0.7398 \n",
      "Epoch: 60, train/loss: 0.1029, eval/loss: 0.1860, train/r2: 0.8826, eval/r2: 0.7323, train/one: 0.8670, train/two: 0.8661, train/three: 0.9146, eval/one: 0.6786, eval/two: 0.8733, eval/three: 0.6452 \n",
      "Epoch: 65, train/loss: 0.1017, eval/loss: 0.1361, train/r2: 0.8999, eval/r2: 0.8555, train/one: 0.8849, train/two: 0.9068, train/three: 0.9081, eval/one: 0.8453, eval/two: 0.7923, eval/three: 0.9288 \n",
      "Epoch: 70, train/loss: 0.0960, eval/loss: 0.1188, train/r2: 0.9032, eval/r2: 0.7950, train/one: 0.8771, train/two: 0.8960, train/three: 0.9367, eval/one: 0.9137, eval/two: 0.5383, eval/three: 0.9330 \n",
      "Epoch: 75, train/loss: 0.0998, eval/loss: 0.0790, train/r2: 0.9058, eval/r2: 0.9142, train/one: 0.9230, train/two: 0.8996, train/three: 0.8946, eval/one: 0.8824, eval/two: 0.8979, eval/three: 0.9623 \n",
      "Epoch: 80, train/loss: 0.1053, eval/loss: 0.1064, train/r2: 0.9027, eval/r2: 0.8742, train/one: 0.8513, train/two: 0.9223, train/three: 0.9346, eval/one: 0.9515, eval/two: 0.8002, eval/three: 0.8710 \n",
      "Epoch: 85, train/loss: 0.0871, eval/loss: 0.1228, train/r2: 0.9071, eval/r2: 0.8744, train/one: 0.8851, train/two: 0.9207, train/three: 0.9154, eval/one: 0.8585, eval/two: 0.8450, eval/three: 0.9199 \n",
      "Epoch: 90, train/loss: 0.0985, eval/loss: 0.1151, train/r2: 0.9153, eval/r2: 0.8493, train/one: 0.9162, train/two: 0.9129, train/three: 0.9168, eval/one: 0.8930, eval/two: 0.7448, eval/three: 0.9101 \n",
      "Epoch: 95, train/loss: 0.0944, eval/loss: 0.1117, train/r2: 0.9117, eval/r2: 0.8708, train/one: 0.8808, train/two: 0.9440, train/three: 0.9104, eval/one: 0.8518, eval/two: 0.8206, eval/three: 0.9400 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 129 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 129 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-273/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3620b656702447baac40a51d1ec2a197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0381, eval/loss: 1.5364, train/r2: -0.0151, eval/r2: -0.1736, train/one: -0.0028, train/two: -0.0063, train/three: -0.0363, eval/one: -0.2290, eval/two: -0.0016, eval/three: -0.2902 \n",
      "Epoch: 5, train/loss: 0.8713, eval/loss: 1.2600, train/r2: -0.0318, eval/r2: -0.4653, train/one: -0.0243, train/two: -0.0209, train/three: -0.0502, eval/one: -0.0745, eval/two: -0.4591, eval/three: -0.8623 \n",
      "Epoch: 10, train/loss: 0.9980, eval/loss: 1.4902, train/r2: 0.0651, eval/r2: -0.4979, train/one: -0.0204, train/two: 0.0083, train/three: 0.2074, eval/one: -1.3945, eval/two: -0.0637, eval/three: -0.0357 \n",
      "Epoch: 15, train/loss: 0.7737, eval/loss: 1.2890, train/r2: 0.2103, eval/r2: 0.0350, train/one: -0.0006, train/two: 0.0101, train/three: 0.6215, eval/one: -0.0381, eval/two: -0.0075, eval/three: 0.1507 \n",
      "Epoch: 20, train/loss: 0.6772, eval/loss: 0.8765, train/r2: 0.2598, eval/r2: 0.1152, train/one: 0.0140, train/two: 0.0008, train/three: 0.7644, eval/one: -0.1376, eval/two: -0.0540, eval/three: 0.5371 \n",
      "Epoch: 25, train/loss: 0.7119, eval/loss: 0.8717, train/r2: 0.2932, eval/r2: 0.1824, train/one: 0.0547, train/two: -0.0088, train/three: 0.8337, eval/one: -0.0092, eval/two: -0.2157, eval/three: 0.7720 \n",
      "Epoch: 30, train/loss: 0.6655, eval/loss: 1.1561, train/r2: 0.3311, eval/r2: 0.2752, train/one: 0.1310, train/two: 0.0398, train/three: 0.8227, eval/one: -0.0959, eval/two: -0.0080, eval/three: 0.9294 \n",
      "Epoch: 35, train/loss: 0.5240, eval/loss: 0.7331, train/r2: 0.4852, eval/r2: 0.4705, train/one: 0.3828, train/two: 0.2166, train/three: 0.8560, eval/one: 0.4171, eval/two: 0.1227, eval/three: 0.8717 \n",
      "Epoch: 40, train/loss: 0.3800, eval/loss: 0.5206, train/r2: 0.6547, eval/r2: 0.6246, train/one: 0.6843, train/two: 0.4377, train/three: 0.8422, eval/one: 0.7239, eval/two: 0.4421, eval/three: 0.7079 \n",
      "Epoch: 45, train/loss: 0.2496, eval/loss: 0.4924, train/r2: 0.7575, eval/r2: 0.5373, train/one: 0.7798, train/two: 0.6093, train/three: 0.8836, eval/one: 0.2336, eval/two: 0.4447, eval/three: 0.9336 \n",
      "Epoch: 50, train/loss: 0.2328, eval/loss: 0.3908, train/r2: 0.7709, eval/r2: 0.6972, train/one: 0.7412, train/two: 0.6763, train/three: 0.8950, eval/one: 0.5711, eval/two: 0.6315, eval/three: 0.8889 \n",
      "Epoch: 55, train/loss: 0.1698, eval/loss: 0.3145, train/r2: 0.8292, eval/r2: 0.7679, train/one: 0.7853, train/two: 0.7947, train/three: 0.9077, eval/one: 0.6634, eval/two: 0.7337, eval/three: 0.9068 \n",
      "Epoch: 60, train/loss: 0.1622, eval/loss: 0.2391, train/r2: 0.8380, eval/r2: 0.7550, train/one: 0.7957, train/two: 0.8217, train/three: 0.8965, eval/one: 0.5987, eval/two: 0.8241, eval/three: 0.8421 \n",
      "Epoch: 65, train/loss: 0.1513, eval/loss: 0.2104, train/r2: 0.8631, eval/r2: 0.8022, train/one: 0.8405, train/two: 0.8446, train/three: 0.9042, eval/one: 0.7966, eval/two: 0.8530, eval/three: 0.7570 \n",
      "Epoch: 70, train/loss: 0.1098, eval/loss: 0.2134, train/r2: 0.8825, eval/r2: 0.8440, train/one: 0.8821, train/two: 0.8615, train/three: 0.9038, eval/one: 0.7996, eval/two: 0.8335, eval/three: 0.8989 \n",
      "Epoch: 75, train/loss: 0.0957, eval/loss: 0.2286, train/r2: 0.9107, eval/r2: 0.8127, train/one: 0.8899, train/two: 0.9150, train/three: 0.9273, eval/one: 0.6933, eval/two: 0.8934, eval/three: 0.8514 \n",
      "Epoch: 80, train/loss: 0.1002, eval/loss: 0.1258, train/r2: 0.8988, eval/r2: 0.8805, train/one: 0.8608, train/two: 0.9167, train/three: 0.9189, eval/one: 0.9203, eval/two: 0.9030, eval/three: 0.8183 \n",
      "Epoch: 85, train/loss: 0.0929, eval/loss: 0.1457, train/r2: 0.9102, eval/r2: 0.8382, train/one: 0.8876, train/two: 0.9131, train/three: 0.9300, eval/one: 0.8560, eval/two: 0.8716, eval/three: 0.7871 \n",
      "Epoch: 90, train/loss: 0.1286, eval/loss: 0.1640, train/r2: 0.8972, eval/r2: 0.7902, train/one: 0.8831, train/two: 0.9073, train/three: 0.9013, eval/one: 0.8564, eval/two: 0.8747, eval/three: 0.6395 \n",
      "Epoch: 95, train/loss: 0.0745, eval/loss: 0.1686, train/r2: 0.9154, eval/r2: 0.8674, train/one: 0.8753, train/two: 0.9376, train/three: 0.9332, eval/one: 0.8956, eval/two: 0.8823, eval/three: 0.8244 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 115 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 115 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-274/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04089553b0ec43f19f595ff8316fe270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.0525, eval/loss: 0.9772, train/r2: -0.0045, eval/r2: -0.0379, train/one: -0.0030, train/two: -0.0014, train/three: -0.0090, eval/one: -0.0017, eval/two: -0.0325, eval/three: -0.0796 \n",
      "Epoch: 5, train/loss: 0.9978, eval/loss: 0.9386, train/r2: 0.0080, eval/r2: -0.2470, train/one: -0.0003, train/two: -0.0238, train/three: 0.0482, eval/one: -0.0100, eval/two: -0.3368, eval/three: -0.3941 \n",
      "Epoch: 10, train/loss: 0.9016, eval/loss: 0.8882, train/r2: 0.0949, eval/r2: -0.0844, train/one: -0.0507, train/two: 0.0290, train/three: 0.3064, eval/one: -0.0015, eval/two: -0.0334, eval/three: -0.2184 \n",
      "Epoch: 15, train/loss: 0.7366, eval/loss: 0.7502, train/r2: 0.2829, eval/r2: -0.0418, train/one: 0.0396, train/two: 0.0861, train/three: 0.7231, eval/one: -0.1079, eval/two: -0.1962, eval/three: 0.1786 \n",
      "Epoch: 20, train/loss: 0.7314, eval/loss: 0.8153, train/r2: 0.2908, eval/r2: -0.0213, train/one: 0.0631, train/two: 0.0032, train/three: 0.8060, eval/one: -0.0921, eval/two: -0.2105, eval/three: 0.2388 \n",
      "Epoch: 25, train/loss: 0.7073, eval/loss: 0.8583, train/r2: 0.3370, eval/r2: -0.0288, train/one: 0.1158, train/two: 0.1067, train/three: 0.7886, eval/one: -0.0794, eval/two: -0.0828, eval/three: 0.0757 \n",
      "Epoch: 30, train/loss: 0.5223, eval/loss: 0.5172, train/r2: 0.4209, eval/r2: 0.3381, train/one: 0.3427, train/two: 0.1313, train/three: 0.7886, eval/one: 0.1922, eval/two: 0.0886, eval/three: 0.7336 \n",
      "Epoch: 35, train/loss: 0.3288, eval/loss: 0.3469, train/r2: 0.6200, eval/r2: 0.4429, train/one: 0.5937, train/two: 0.4057, train/three: 0.8607, eval/one: 0.6231, eval/two: 0.1112, eval/three: 0.5943 \n",
      "Epoch: 40, train/loss: 0.3249, eval/loss: 0.3205, train/r2: 0.6672, eval/r2: 0.5776, train/one: 0.5577, train/two: 0.6048, train/three: 0.8392, eval/one: 0.6677, eval/two: 0.3274, eval/three: 0.7376 \n",
      "Epoch: 45, train/loss: 0.1682, eval/loss: 0.1033, train/r2: 0.8382, eval/r2: 0.8372, train/one: 0.7890, train/two: 0.8218, train/three: 0.9039, eval/one: 0.8190, eval/two: 0.7566, eval/three: 0.9361 \n",
      "Epoch: 50, train/loss: 0.1368, eval/loss: 0.0908, train/r2: 0.8717, eval/r2: 0.8758, train/one: 0.8330, train/two: 0.8799, train/three: 0.9022, eval/one: 0.8306, eval/two: 0.8692, eval/three: 0.9276 \n",
      "Epoch: 55, train/loss: 0.1287, eval/loss: 0.0913, train/r2: 0.8573, eval/r2: 0.8950, train/one: 0.8700, train/two: 0.7843, train/three: 0.9176, eval/one: 0.9007, eval/two: 0.8478, eval/three: 0.9364 \n",
      "Epoch: 60, train/loss: 0.1379, eval/loss: 0.1040, train/r2: 0.8723, eval/r2: 0.8383, train/one: 0.8535, train/two: 0.8647, train/three: 0.8986, eval/one: 0.8064, eval/two: 0.8547, eval/three: 0.8538 \n",
      "Epoch: 65, train/loss: 0.1103, eval/loss: 0.1150, train/r2: 0.8902, eval/r2: 0.8818, train/one: 0.8803, train/two: 0.8982, train/three: 0.8921, eval/one: 0.7640, eval/two: 0.9198, eval/three: 0.9617 \n",
      "Epoch: 70, train/loss: 0.1019, eval/loss: 0.1031, train/r2: 0.8978, eval/r2: 0.8871, train/one: 0.8614, train/two: 0.9110, train/three: 0.9210, eval/one: 0.8208, eval/two: 0.9342, eval/three: 0.9063 \n",
      "Epoch: 75, train/loss: 0.1031, eval/loss: 0.0839, train/r2: 0.8948, eval/r2: 0.8828, train/one: 0.8673, train/two: 0.8990, train/three: 0.9180, eval/one: 0.8283, eval/two: 0.9367, eval/three: 0.8833 \n",
      "Epoch: 80, train/loss: 0.1076, eval/loss: 0.1187, train/r2: 0.8915, eval/r2: 0.8177, train/one: 0.8895, train/two: 0.8779, train/three: 0.9072, eval/one: 0.6782, eval/two: 0.8589, eval/three: 0.9161 \n",
      "Epoch: 85, train/loss: 0.0813, eval/loss: 0.0773, train/r2: 0.9093, eval/r2: 0.8953, train/one: 0.9054, train/two: 0.9194, train/three: 0.9030, eval/one: 0.8220, eval/two: 0.9242, eval/three: 0.9398 \n",
      "Epoch: 90, train/loss: 0.0939, eval/loss: 0.0928, train/r2: 0.9011, eval/r2: 0.8539, train/one: 0.9160, train/two: 0.8897, train/three: 0.8977, eval/one: 0.8335, eval/two: 0.8332, eval/three: 0.8950 \n",
      "Epoch: 95, train/loss: 0.0955, eval/loss: 0.1152, train/r2: 0.9068, eval/r2: 0.8357, train/one: 0.9110, train/two: 0.9019, train/three: 0.9075, eval/one: 0.8346, eval/two: 0.8338, eval/three: 0.8387 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 129 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 129 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-275/metadata\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "inputs_mean_std = []\n",
    "targets_mean_std = []\n",
    "scores = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "splits = kfold.split(inputs)\n",
    "\n",
    "for fold, (train_idx, eval_idx) in enumerate(splits):\n",
    "    MODEL_NAME = f\"resnet.finetune.avg.weights.p.data.fixed.F{fold}\"\n",
    "    checkpoint_name = f\"resnet.finetune.avg.weights.p.ckpt.data.fixed.F{fold}.pt\"\n",
    "    \n",
    "    train_inputs = inputs[train_idx]\n",
    "    train_targets = targets[train_idx]\n",
    "    eval_inputs = inputs[eval_idx]\n",
    "    eval_targets = targets[eval_idx]\n",
    "\n",
    "    train_ds = get_dataset(train_inputs, train_targets, config)\n",
    "    \n",
    "    inputs_mean_std.append((fold, train_ds.s_mean, train_ds.s_std))\n",
    "    targets_mean_std.append((fold, train_ds.concentration_means, train_ds.concentration_stds))\n",
    "    \n",
    "    eval_ds = get_dataset(\n",
    "        eval_inputs, \n",
    "        eval_targets, \n",
    "        config,\n",
    "        (train_ds.s_mean, train_ds.s_std), \n",
    "        (train_ds.concentration_means, train_ds.concentration_stds)\n",
    "    )\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    train_dl, eval_dl = return_dls(train_ds, eval_ds, BATCH_SIZE, len(eval_ds))\n",
    "    \n",
    "    #model = convnextv2_atto().to(device)\n",
    "    model = ReZeroNet(**config).to(device)\n",
    "    ckpt = get_ckpt(\"/kaggle/working/avg_weights_data_fixed.pt\")#[\"state_dict\"]\n",
    "    model.load_state_dict(ckpt)\n",
    "    \n",
    "    if fold == 0: print(get_model_size(model))\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "    scheduler = get_scheduler(optimizer, train_dl, EPOCHS)\n",
    "    \n",
    "    score = train(\n",
    "            model, \n",
    "            optimizer, \n",
    "            device,\n",
    "            torch.float16,\n",
    "            scheduler,\n",
    "            train_dl, \n",
    "            eval_dl,\n",
    "            loss_fn,\n",
    "            EPOCHS,\n",
    "            checkpoint_name,\n",
    "            neptune_run=setup_neptune(),\n",
    "        )\n",
    "    \n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4925ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralTestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra,\n",
    "        concentrations,\n",
    "        dtype=None,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=0.0,\n",
    "        baseline_factor_bound=0.0,\n",
    "        baseline_period_lower_bound=100.0,\n",
    "        baseline_period_upper_bound=200.0,\n",
    "        augment_slope_std=0.0,\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=0,\n",
    "        spectrum_rolling_sigma=0.0,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    ):\n",
    "        self.dtype = dtype or torch.float32\n",
    "        self.combine_spectra_range = combine_spectra_range\n",
    "        self.baseline_factor_bound = baseline_factor_bound\n",
    "        self.augment_slope_std = augment_slope_std\n",
    "        self.augment_intercept_std = augment_intersept_std\n",
    "        self.baseline_period_lower_bound = baseline_period_lower_bound\n",
    "        self.baseline_period_upper_bound = baseline_period_upper_bound\n",
    "        self.rolling_bound = rolling_bound\n",
    "        self.spectrum_rolling_sigma = spectrum_rolling_sigma\n",
    "        self.augmentation_weight = torch.tensor(augmentation_weight, dtype=dtype)\n",
    "        self.original_dp_weight = original_datapoint_weight\n",
    "\n",
    "        # normalize spectra\n",
    "        spectra = torch.tensor(spectra, dtype=dtype)\n",
    "\n",
    "        if spectra_mean_std is None:\n",
    "            self.s_mean = torch.mean(spectra)\n",
    "            self.s_std = torch.std(spectra)\n",
    "        else:\n",
    "            self.s_mean, self.s_std = spectra_mean_std\n",
    "\n",
    "        self.spectra = torch.divide(\n",
    "            torch.subtract(spectra, self.s_mean),\n",
    "            self.s_std,\n",
    "        )\n",
    "\n",
    "        self.dummy_wns = np.tile(\n",
    "            np.arange(\n",
    "                0., 1., 1. / self.spectra.shape[2],\n",
    "                dtype=np_dtype_from_torch[self.dtype]\n",
    "            )[None, :self.spectra.shape[2]],\n",
    "            (self.spectra.shape[1], 1),\n",
    "        )\n",
    "\n",
    "        if False:\n",
    "            # normalize concentrations\n",
    "            concentrations = torch.tensor(concentrations, dtype=dtype)\n",
    "            if concentration_mean_std is None:\n",
    "                self.concentration_means = torch.nanmean(concentrations, dim=0)\n",
    "\n",
    "                self.concentration_stds = torch.maximum(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            torch.std(col[torch.logical_not(torch.isnan(col))])\n",
    "                            for col in concentrations.T\n",
    "                        ]\n",
    "                    ),\n",
    "                    torch.tensor([1e-3] * concentrations.shape[1]),\n",
    "                )\n",
    "            else:\n",
    "                self.concentration_means = concentration_mean_std[0]\n",
    "                self.concentration_stds = concentration_mean_std[1]\n",
    "\n",
    "            self.concentrations = torch.divide(\n",
    "                torch.subtract(\n",
    "                    concentrations,\n",
    "                    self.concentration_means,\n",
    "                ),\n",
    "                self.concentration_stds,\n",
    "            )\n",
    "\n",
    "    def pick_two(self, max_idx=None):\n",
    "        max_idx = max_idx or len(self)\n",
    "        return random.choices(range(max_idx), k=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 96\n",
    "\n",
    "    def augment_spectra(self, spectra):\n",
    "        if self.augment_slope_std > 0.0:\n",
    "\n",
    "            def spectrum_approximation(x, slope, intercept):\n",
    "                return (slope * x + intercept).reshape(-1, 1)[:, 0]\n",
    "\n",
    "            slope, inter = scipy.optimize.curve_fit(\n",
    "                spectrum_approximation,\n",
    "                self.dummy_wns,\n",
    "                spectra.reshape(-1, 1)[:, 0],\n",
    "                p0=np.random.rand(2),\n",
    "            )[0]\n",
    "\n",
    "            new_slope = slope * (\n",
    "                    np.random.gamma(\n",
    "                        shape=1. / self.augment_slope_std,\n",
    "                        scale=self.augment_slope_std,\n",
    "                        size=1,\n",
    "                    )\n",
    "            )[0]\n",
    "            new_intercept = inter * (\n",
    "                1.0 + np.random.randn(1) * self.augment_intercept_std\n",
    "            )[0]\n",
    "            spectra += torch.tensor(\n",
    "                (new_slope - slope)\n",
    "            ) * self.dummy_wns + new_intercept - inter\n",
    "\n",
    "        factor = self.baseline_factor_bound * torch.rand(size=(1,))\n",
    "        offset = torch.rand(size=(1,)) * 2.0 * torch.pi\n",
    "        period = self.baseline_period_lower_bound + (\n",
    "            self.baseline_period_upper_bound - self.baseline_period_lower_bound\n",
    "        ) * torch.rand(size=(1,))\n",
    "        permutations = factor * torch.cos(\n",
    "            2.0 * torch.pi / period * self.dummy_wns + offset\n",
    "        )\n",
    "        return self.roll_spectrum(\n",
    "            spectra + permutations * spectra,\n",
    "            delta=random.randint(-self.rolling_bound, self.rolling_bound),\n",
    "        )\n",
    "\n",
    "    def roll_spectrum(self, spectra, delta):\n",
    "        num_spectra = spectra.shape[0]\n",
    "        rolled_spectra = np.roll(spectra, delta, axis=1)\n",
    "        if delta > 0:\n",
    "            rolled_spectra[:, :delta] = (\n",
    "                np.random.rand(num_spectra, delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta:(delta + 1)]\n",
    "        elif delta < 0:\n",
    "            rolled_spectra[:, delta:] = (\n",
    "                np.random.rand(num_spectra, -delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta - 1:delta]\n",
    "        return rolled_spectra\n",
    "\n",
    "    def combine_k_items(self, indices, weights):\n",
    "        return (\n",
    "            # spectra\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None, None], self.spectra[indices, :, :]),\n",
    "                dim=0,\n",
    "            ),\n",
    "            # concentrations\n",
    "            #torch.sum(\n",
    "            #    torch.mul(weights[:, None], self.concentrations[indices, :]),\n",
    "            #    dim=0,\n",
    "            #)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if True:#self.combine_spectra_range < 1e-12:\n",
    "            spectrum = self.spectra[idx]\n",
    "            #spectrum = self.augment_spectra(spectrum)\n",
    "            return spectrum\n",
    "        else:\n",
    "            if random.random() < self.original_dp_weight:\n",
    "                one_weight = 1.\n",
    "                label_weight = torch.tensor(1.0, dtype=self.dtype)\n",
    "            else:\n",
    "                one_weight = random.uniform(0.0, self.combine_spectra_range)\n",
    "                label_weight = self.augmentation_weight\n",
    "            weights = torch.tensor([one_weight, (1 - one_weight)])\n",
    "            # just pick two random indices\n",
    "            indices = random.choices(range(len(self)), k=2)\n",
    "\n",
    "            mixed_spectra = self.combine_k_items(\n",
    "                indices=indices,\n",
    "                weights=weights,\n",
    "            )\n",
    "            mixed_spectra = self.augment_spectra(mixed_spectra[0])\n",
    "            return mixed_spectra\n",
    "        \n",
    "  \n",
    "def get_test_dataset(inputs, inputs_mean_std, targets_mean_std):\n",
    "    return SpectralTestDataset(\n",
    "        spectra=inputs[:, None, :],\n",
    "        concentrations=None,\n",
    "        dtype=torch.float32,\n",
    "        spectra_mean_std=inputs_mean_std,\n",
    "        concentration_mean_std=targets_mean_std,\n",
    "        combine_spectra_range=1.0,\n",
    "        baseline_factor_bound=config[\"baseline_factor_bound\"],\n",
    "        baseline_period_lower_bound=config[\"baseline_period_lower_bound\"],\n",
    "        baseline_period_upper_bound=(config[\"baseline_period_lower_bound\"] + config[\"baseline_period_span\"]),\n",
    "        augment_slope_std=config[\"augment_slope_std\"],\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=config[\"rolling_bound\"],\n",
    "        spectrum_rolling_sigma=0.01,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94caa304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/resnet.finetune.avg.weights.p.ckpt.data.fixed.F0.pt 73 0.890884074150463\n",
      "/kaggle/working/resnet.finetune.avg.weights.p.ckpt.data.fixed.F1.pt 83 0.9202159693697608\n",
      "/kaggle/working/resnet.finetune.avg.weights.p.ckpt.data.fixed.F2.pt 75 0.9141962849557639\n",
      "/kaggle/working/resnet.finetune.avg.weights.p.ckpt.data.fixed.F3.pt 94 0.9173730713123304\n",
      "/kaggle/working/resnet.finetune.avg.weights.p.ckpt.data.fixed.F4.pt 92 0.933128269953417\n"
     ]
    }
   ],
   "source": [
    "ckpt_paths = get_ckpt_paths(\"/kaggle/working/\", \"avg.weights.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b492f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.9619, 1.1908, 1.5928]), tensor([2.8689, 0.5668, 0.6464]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_mean_std[2][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4786020e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.35193701, 0.70662384, 0.61738674],\n",
       "       [6.49085247, 1.96047767, 1.46188283],\n",
       "       [4.19197596, 0.30039638, 1.03181022],\n",
       "       [2.65833732, 0.77751213, 0.39134215],\n",
       "       [9.63981799, 0.51798429, 0.97882175],\n",
       "       [7.63364555, 1.93437445, 1.04053863],\n",
       "       [5.38055328, 0.48491612, 0.32809626],\n",
       "       [6.77800095, 1.96460996, 1.23086177],\n",
       "       [7.15960728, 1.74662916, 1.0126448 ],\n",
       "       [9.94141673, 0.70411267, 0.23760362],\n",
       "       [8.93824441, 1.03869283, 0.97580407],\n",
       "       [2.57959917, 0.79118592, 0.94668337],\n",
       "       [2.7824985 , 1.18457097, 1.11126463],\n",
       "       [3.64108348, 0.99573297, 1.37807992],\n",
       "       [3.97725705, 0.87835026, 0.92747408],\n",
       "       [7.05234218, 1.3170997 , 0.86020819],\n",
       "       [3.24437878, 0.98439018, 1.03394707],\n",
       "       [4.34187367, 0.99896576, 1.14726068],\n",
       "       [4.68313124, 1.44564602, 1.1899677 ],\n",
       "       [3.05547406, 0.90933227, 1.16126185],\n",
       "       [3.49208235, 1.05638682, 1.04363395],\n",
       "       [5.06547481, 1.05295697, 1.07452596],\n",
       "       [2.25640776, 1.14857055, 0.58797104],\n",
       "       [3.41427542, 1.49153181, 1.44268637],\n",
       "       [3.21858798, 1.21527937, 1.16738711],\n",
       "       [3.40717206, 0.9782371 , 1.21635993],\n",
       "       [5.09975132, 1.45630199, 0.91608473],\n",
       "       [4.08327673, 1.22641896, 1.38509243],\n",
       "       [5.05960492, 1.13049669, 1.06223608],\n",
       "       [5.10285488, 0.38577231, 0.25634522],\n",
       "       [4.69827326, 0.88298422, 1.28669303],\n",
       "       [5.73040653, 0.88088708, 1.04673507],\n",
       "       [4.22230142, 1.36394619, 1.26269043],\n",
       "       [5.88944533, 1.3262247 , 0.62398294],\n",
       "       [6.54078159, 1.41233733, 0.88271958],\n",
       "       [5.5778631 , 0.25551269, 0.82528037],\n",
       "       [6.21099975, 0.85633953, 0.76215505],\n",
       "       [5.47473972, 1.44071523, 0.4981005 ],\n",
       "       [2.97353272, 1.19551202, 0.65242078],\n",
       "       [7.66181749, 1.20616904, 1.4259693 ],\n",
       "       [4.94644119, 0.70565166, 1.14253491],\n",
       "       [3.29272792, 1.31860673, 0.51648944],\n",
       "       [7.09473111, 0.30440288, 1.4057327 ],\n",
       "       [3.14237603, 0.61200484, 1.08051607],\n",
       "       [1.97027239, 1.19538521, 0.82980345],\n",
       "       [5.56661143, 0.37844593, 1.44400346],\n",
       "       [1.91081512, 0.59805228, 1.34499783],\n",
       "       [3.54215466, 1.12330486, 1.05165287],\n",
       "       [3.3001719 , 1.15167491, 1.50577172],\n",
       "       [5.35531592, 0.68511992, 1.58172104],\n",
       "       [2.15434573, 0.74244847, 1.85300451],\n",
       "       [2.61351201, 1.21514034, 1.70761022],\n",
       "       [3.31813595, 1.11332661, 1.36767644],\n",
       "       [3.36620729, 1.59276767, 1.22377731],\n",
       "       [2.9035379 , 1.31794233, 1.42173816],\n",
       "       [3.10288305, 1.04823444, 1.86470826],\n",
       "       [5.29339833, 1.62596822, 1.5446913 ],\n",
       "       [3.08311193, 1.28551591, 1.71393972],\n",
       "       [4.85757735, 0.3481044 , 1.48639347],\n",
       "       [5.58689815, 0.45638227, 0.59453743],\n",
       "       [2.16028793, 0.5208099 , 1.35322669],\n",
       "       [2.59604178, 1.2047916 , 1.40453013],\n",
       "       [2.83811487, 1.28473751, 0.84088396],\n",
       "       [6.58138599, 1.06169484, 1.68735052],\n",
       "       [7.00635096, 1.63351411, 0.58215422],\n",
       "       [4.31244157, 0.70739702, 0.34805987],\n",
       "       [5.50736268, 1.4364391 , 1.27673023],\n",
       "       [2.78804864, 1.23018877, 0.78226809],\n",
       "       [5.45010865, 0.79964351, 0.7031705 ],\n",
       "       [7.46812888, 0.76771925, 0.81341645],\n",
       "       [8.28021992, 0.77441367, 0.9730257 ],\n",
       "       [7.48900921, 1.31662339, 1.18286926],\n",
       "       [3.46835108, 1.32771702, 1.77083134],\n",
       "       [5.69035056, 0.34962441, 1.09721654],\n",
       "       [3.12081496, 1.13571232, 1.61537702],\n",
       "       [6.00713703, 0.2152048 , 1.19213594],\n",
       "       [3.23199558, 0.71254219, 0.29865955],\n",
       "       [3.72321549, 1.17550793, 1.5138717 ],\n",
       "       [1.77006779, 1.19503823, 0.95928921],\n",
       "       [2.51722043, 0.52436273, 1.36227954],\n",
       "       [6.29296193, 0.51032633, 0.5828567 ],\n",
       "       [2.01181918, 0.76731123, 1.84341749],\n",
       "       [7.18470005, 0.90463447, 2.03275835],\n",
       "       [4.45894185, 0.73075524, 1.41940543],\n",
       "       [2.75240905, 0.90668952, 1.62906642],\n",
       "       [3.04702495, 1.07567718, 1.21489735],\n",
       "       [4.12815149, 1.23111905, 0.92588184],\n",
       "       [3.23457641, 0.88406309, 1.17171816],\n",
       "       [6.08170267, 1.35515583, 1.1043371 ],\n",
       "       [3.08985715, 1.18091584, 1.33033974],\n",
       "       [4.53731437, 1.31146604, 1.22286356],\n",
       "       [2.76944867, 1.19340089, 1.43393113],\n",
       "       [3.52238961, 0.43689501, 1.06692515],\n",
       "       [4.43739007, 0.39436537, 1.04155093],\n",
       "       [2.31181704, 1.37481686, 1.05536502],\n",
       "       [4.53979165, 1.83060812, 0.9968384 ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def inference(test_inputs, ckpt_name, i):\n",
    "    ckpt = get_ckpt(ckpt_name)\n",
    "    \n",
    "    test_ds = get_test_dataset(test_inputs, inputs_mean_std[i][1:], targets_mean_std[i][1:]) #[i][1:]\n",
    "    test_dl = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    \n",
    "    model = ReZeroNet(**config).to(device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    for inputs in test_dl:\n",
    "        with torch.inference_mode():\n",
    "            preds = model(inputs.cuda())\n",
    "            preds = preds.double() \n",
    "            all_preds.append(cuda_to_np(preds))\n",
    "            \n",
    "    preds = np.concatenate(all_preds)\n",
    "    mus = targets_mean_std[i][1:][0] #[i][1:][0]\n",
    "    sigmas = targets_mean_std[i][1:][1] #[i][1:][1]\n",
    "\n",
    "    for i in range(3):\n",
    "        preds[:, i] = reverse_zscore(preds[:, i], mus[i].numpy(), sigmas[i].numpy())\n",
    "    \n",
    "    return preds\n",
    "\n",
    "preds = inference(test_inputs, \"/kaggle/working/resnet.finetune.avg.weights.p.ckpt.data.fixed.F4.pt\", 4) # CAREFUL ABOUT INDEX\n",
    "generate_csv(preds, \"resnet.finetune.avg.weights.p.ckpt.data.fixed.F4.9331.csv\")\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48724221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.276526487, Max: 11.88990894, Mean: 3.208722795402778, Std: 3.1291512817697695\n",
      "Min: 0.03802425901420747, Max: 9.493976433275805, Mean: 2.430948129346428, Std: 2.314666198874381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats(targets, minmax=True), get_stats(preds, minmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fe51104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.276526487, Max: 11.88990894, Mean: 3.208722795402778, Std: 3.1291512817697695\n",
      "Min: 0.21520479832992123, Max: 9.941416729453636, Mean: 2.220094471803795, Std: 1.9768473694491508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats(targets, minmax=True), get_stats(preds, minmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b84e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_inference(ckpt_paths):\n",
    "    test_inputs = get_test_data()\n",
    "    all_preds = []\n",
    "\n",
    "    for i, ckpt_path in enumerate(ckpt_paths):\n",
    "        ckpt = get_ckpt(ckpt_path)\n",
    "        \n",
    "        model = ReZeroNet(**config).to(device)\n",
    "        model.load_state_dict(ckpt[\"state_dict\"])\n",
    "        model.eval()\n",
    "\n",
    "        test_ds = get_test_dataset(test_inputs, inputs_mean_std[i][1:], targets_mean_std[i][1:])\n",
    "        test_dl = DataLoader(test_ds, batch_size=32)\n",
    "        \n",
    "        fold_preds = []\n",
    "        for inputs in test_dl:\n",
    "            with torch.inference_mode():\n",
    "                preds = model(inputs.cuda())\n",
    "                preds = cuda_to_np(preds.double())\n",
    "                fold_preds.append(preds)\n",
    "                \n",
    "        fold_preds = np.concatenate(fold_preds)\n",
    "        \n",
    "        means = targets_mean_std[i][1:][0]\n",
    "        stds = targets_mean_std[i][1:][1]\n",
    "        for i in range(3):\n",
    "            fold_preds[:, i] = reverse_zscore(fold_preds[:, i], means[i].numpy(), stds[i].numpy())\n",
    "            \n",
    "        all_preds.append(fold_preds)\n",
    "\n",
    "    return np.mean(all_preds, axis=0)\n",
    "\n",
    "preds = ensemble_inference(ckpt_paths)\n",
    "generate_csv(preds, \"paper.finetune.avg.pretrain.weights.ensemble.csv\")\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
