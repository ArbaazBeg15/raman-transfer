{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6442c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "SEED = 1000\n",
    "setup_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3fb896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, train_dl, epochs):\n",
    "    total_training_steps = len(train_dl) * epochs\n",
    "    warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "\n",
    "\n",
    "def get_stats(tensor, p=True, r=False):\n",
    "    mean, std = tensor.mean(), tensor.std()\n",
    "    min, max =  tensor.min(), tensor.max()\n",
    "    \n",
    "    #if p: print(f\"Min: {min}, Max: {max}, Mean: {mean}, Std: {std}\")\n",
    "    if p: print(f\"Mean: {mean}, Std: {std}\")\n",
    "    if r: return mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-8)\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "    \n",
    "\n",
    "def get_index(iterable):\n",
    "    return random.randint(0, len(iterable) - 1)\n",
    "\n",
    "\n",
    "def get_indices(iterable, n):\n",
    "    return random.sample(range(len(iterable)), n)\n",
    "\n",
    "\n",
    "def split(inputs, targets, seed):\n",
    "    return train_test_split(\n",
    "        inputs,\n",
    "        targets, \n",
    "        test_size=0.2,\n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    ) \n",
    "\n",
    "\n",
    "def show_waves(waves, dpi=100):\n",
    "    \"\"\"\n",
    "    waves: numpy array of shape (3, N)\n",
    "    Creates three separate figures that stretch wide.\n",
    "    \"\"\"\n",
    "    N = waves.shape[1]\n",
    "    t = np.arange(N)\n",
    "\n",
    "    # Wide aspect ratio; height modest so each window fills width\n",
    "    for i in range(waves.shape[0]):\n",
    "        fig = plt.figure(figsize=(14, 4), dpi=dpi)  # wide figure\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(t, waves[i], linewidth=1)\n",
    "        ax.set_title(f\"Wave {i+1}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()  # reduce margins to use width\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def hf_ds_download(hf_token, repo_id):\n",
    "    login(hf_token[1:])\n",
    "    return snapshot_download(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "def get_spectra_features(X, b=False):\n",
    "    \"\"\"Create multi-channel features from spectra: raw, 1st derivative, 2nd derivative.\"\"\"\n",
    "    X_processed = np.zeros_like(X)\n",
    "    # Baseline correction and SNV\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        poly = np.polyfit(np.arange(X.shape[1]), X[i], 3)\n",
    "        baseline = np.polyval(poly, np.arange(X.shape[1]))\n",
    "        corrected_spec = X[i] - baseline\n",
    "        #X_processed[i] = (corrected_spec - corrected_spec.mean()) / (corrected_spec.std() + 1e-8)\n",
    "        X_processed[i] = corrected_spec\n",
    "        \n",
    "    # Calculate derivatives\n",
    "    deriv1 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=1, axis=1)\n",
    "    deriv2 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=2, axis=1)\n",
    "\n",
    "    if b: return np.stack([X_processed, deriv1, deriv2], axis=1)\n",
    "    return np.stack([deriv1, deriv2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc57c619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'sample_submission.csv'),\n",
       " (1, 'timegate.csv'),\n",
       " (2, 'mettler_toledo.csv'),\n",
       " (3, 'kaiser.csv'),\n",
       " (4, 'anton_532.csv'),\n",
       " (5, 'transfer_plate.csv'),\n",
       " (6, '96_samples.csv'),\n",
       " (7, 'tornado.csv'),\n",
       " (8, 'tec5.csv'),\n",
       " (9, 'metrohm.csv'),\n",
       " (10, 'anton_785.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3d530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_transfer_data():\n",
    "    csv_path = os.path.join(path, files[5])\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    input_cols = df.columns[1:2049]\n",
    "    target_cols = df.columns[2050:]\n",
    "\n",
    "    targets  = df[target_cols].dropna().to_numpy()\n",
    "\n",
    "    df = df[input_cols]\n",
    "    df['Unnamed: 1'] = df['Unnamed: 1'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "    df['Unnamed: 2048'] = df['Unnamed: 2048'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "\n",
    "    inputs = df.to_numpy().reshape(-1, 2, 2048)\n",
    "    inputs = inputs.mean(axis=1)\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    test = pd.read_csv(os.path.join(path, files[6]))\n",
    "\n",
    "    row1 = test.columns[1:].to_numpy().copy()\n",
    "    row1[-1] = \"5611\"\n",
    "    row1 = row1.astype(np.float64)\n",
    "\n",
    "\n",
    "    cols = test.columns[1:]\n",
    "    test = test[cols]\n",
    "    test[\" 5611]\"] = test[\" 5611]\"].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "    test = test.to_numpy()\n",
    "\n",
    "    test = np.insert(test, 0, row1, axis=0)\n",
    "    return test.reshape(-1, 2, 2048).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b26650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944b99a515ee4a519e9ad7b7a4af1096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, targets = load_transfer_data()\n",
    "inputs = get_spectra_features(inputs)\n",
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(targets)\n",
    "#train_inputs, eval_inputs, train_targets, eval_targets = split(inputs, targets, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    eval_inputs = torch.tensor(eval_inputs)\n",
    "    train_targets = torch.tensor(train_targets)\n",
    "    eval_targets = torch.tensor(eval_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    min, max, mu, sigma = get_stats(train_inputs, r=True)\n",
    "    train_inputs = zscore(train_inputs)\n",
    "    eval_inputs = zscore(eval_inputs)\n",
    "    get_stats(train_inputs), get_stats(eval_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe55238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "if False:\n",
    "    train_ds = TensorDataset(train_inputs.float(), train_targets.float())\n",
    "    eval_ds = TensorDataset(eval_inputs.float(), eval_targets.float())\n",
    "    len(train_ds), len(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9dcceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+5232)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds, train_batch_size, eval_batch_size):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "437ed80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "        if DROPOUT: neptune_run[\"h_parameters\"] = {\"dropout\": DROPOUT}\n",
    "        if DROP_PATH_RATE: neptune_run[\"h_parameters\"] = {\"drop_path_rate\": DROP_PATH_RATE}\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e978702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    \n",
    "    dim1 = r2_score(targets[:, 0], preds[:, 0])\n",
    "    dim2 = r2_score(targets[:, 1], preds[:, 1])\n",
    "    dim3 = r2_score(targets[:, 2], preds[:, 2])\n",
    "    \n",
    "    mean_r2 = (dim1 + dim2 + dim3) / 3\n",
    "    \n",
    "    return dim1, dim2, dim3, mean_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e882d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A residual block with two 1D convolutional layers.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.elu = nn.ELU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.elu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.elu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"A deeper ResNet-style 1D CNN for Raman spectra.\"\"\"\n",
    "    def __init__(self, dropout, input_channels=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.elu = nn.GELU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout), # Increased dropout for better regularization\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, stride=s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44fca775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    device,\n",
    "    scaler, \n",
    "    scheduler,\n",
    "    train_dl,\n",
    "    eval_dl,\n",
    "    epochs,\n",
    "    checkpoint_name,\n",
    "    score=-float(\"inf\"),\n",
    "    neptune_run=None,\n",
    "    p=True,\n",
    "):  \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for inputs, targets in train_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            if neptune_run is not None:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            total_loss += loss.detach().cpu()\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        all_logits = torch.cat(all_logits)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "        total_loss = total_loss / len(train_dl)\n",
    "        \n",
    "        model.eval()\n",
    "        eval_total_loss = 0.0\n",
    "        eval_all_logits = []\n",
    "        eval_all_targets = []\n",
    "\n",
    "        for inputs, targets in eval_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                #with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "\n",
    "            eval_total_loss += loss.detach().cpu()\n",
    "            eval_all_logits.append(logits.detach().cpu())\n",
    "            eval_all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        eval_all_logits = torch.cat(eval_all_logits)\n",
    "        eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "        eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "        eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "        \n",
    "        if eval_r2 > score:\n",
    "            score = eval_r2\n",
    "            data = {\"state_dict\": model.state_dict()}\n",
    "            data[\"epoch\"] = epoch \n",
    "            data[\"score\"] = score\n",
    "            torch.save(data, f\"/kaggle/working/{checkpoint_name}\")\n",
    "        \n",
    "        if neptune_run is not None:\n",
    "            neptune_run[\"train/loss\"].append(total_loss)\n",
    "            neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "            neptune_run[\"train/r2\"].append(r2)\n",
    "            neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "            neptune_run[\"train/one\"].append(one)\n",
    "            neptune_run[\"train/two\"].append(two)\n",
    "            neptune_run[\"train/three\"].append(three)\n",
    "            neptune_run[\"eval/one\"].append(eval_one)\n",
    "            neptune_run[\"eval/two\"].append(eval_two)\n",
    "            neptune_run[\"eval/three\"].append(eval_three)\n",
    "            \n",
    "        if p and epoch % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, \"\n",
    "                f\"train/loss: {total_loss:.4f}, \"\n",
    "                f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "                f\"train/r2: {r2:.4f}, \"\n",
    "                f\"eval/r2: {eval_r2:.4f}, \"\n",
    "                f\"train/one: {one:.4f}, \"\n",
    "                f\"train/two: {two:.4f}, \"\n",
    "                f\"train/three: {three:.4f}, \"\n",
    "                f\"eval/one: {eval_one:.4f}, \"\n",
    "                f\"eval/two: {eval_two:.4f}, \"\n",
    "                f\"eval/three: {eval_three:.4f} \"\n",
    "            )\n",
    "            \n",
    "    if neptune_run is not None: neptune_run.stop()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6326238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings#; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "EPOCHS = 500\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "\n",
    "DROPOUT = 0.5\n",
    "DROP_PATH_RATE = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "183441eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.981251\n",
      "None\n",
      "0.4595489137384006\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0a818ef6dc4c51b61511ba22c20329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 4.7262, eval/loss: 16.7062, train/r2: -0.4411, eval/r2: -9.2297, train/one: -0.5262, train/two: -1.0289, train/three: 0.2318, eval/one: -4.5972, eval/two: -1.8668, eval/three: -21.2251 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train/loss: 3.4597, eval/loss: 7.8044, train/r2: -0.3137, eval/r2: -4.5514, train/one: -0.0851, train/two: -1.0274, train/three: 0.1714, eval/one: -1.4933, eval/two: -1.2336, eval/three: -10.9272 \n",
      "Epoch: 10, train/loss: 1.3387, eval/loss: 6.3708, train/r2: -0.1226, eval/r2: -1.5944, train/one: 0.6529, train/two: -1.2982, train/three: 0.2775, eval/one: -1.3836, eval/two: -0.6698, eval/three: -2.7297 \n",
      "Epoch: 15, train/loss: 0.6749, eval/loss: 1.9105, train/r2: 0.0387, eval/r2: -0.0001, train/one: 0.8767, train/two: -0.8069, train/three: 0.0462, eval/one: 0.3121, eval/two: -0.1390, eval/three: -0.1733 \n",
      "Epoch: 20, train/loss: 0.4846, eval/loss: 1.3246, train/r2: 0.3272, eval/r2: 0.2260, train/one: 0.9101, train/two: -0.2088, train/three: 0.2802, eval/one: 0.5331, eval/two: -0.0109, eval/three: 0.1559 \n",
      "Epoch: 25, train/loss: 0.4451, eval/loss: 1.1604, train/r2: 0.2390, eval/r2: 0.2661, train/one: 0.9307, train/two: -0.7051, train/three: 0.4915, eval/one: 0.5945, eval/two: -0.1991, eval/three: 0.4027 \n",
      "Epoch: 30, train/loss: 0.4440, eval/loss: 0.9667, train/r2: 0.3884, eval/r2: 0.2818, train/one: 0.9130, train/two: -0.3619, train/three: 0.6140, eval/one: 0.6797, eval/two: -0.0880, eval/three: 0.2537 \n",
      "Epoch: 35, train/loss: 0.3935, eval/loss: 1.1380, train/r2: 0.3488, eval/r2: 0.3469, train/one: 0.9365, train/two: -0.4349, train/three: 0.5449, eval/one: 0.5894, eval/two: -0.2169, eval/three: 0.6682 \n",
      "Epoch: 40, train/loss: 0.3581, eval/loss: 1.0315, train/r2: 0.3570, eval/r2: 0.3392, train/one: 0.9483, train/two: -0.4209, train/three: 0.5435, eval/one: 0.6354, eval/two: -0.3356, eval/three: 0.7177 \n",
      "Epoch: 45, train/loss: 0.3388, eval/loss: 1.0969, train/r2: 0.4185, eval/r2: 0.3293, train/one: 0.9489, train/two: -0.2102, train/three: 0.5169, eval/one: 0.6083, eval/two: -0.3323, eval/three: 0.7118 \n",
      "Epoch: 50, train/loss: 0.3707, eval/loss: 0.7891, train/r2: 0.2889, eval/r2: 0.4178, train/one: 0.9526, train/two: -0.5366, train/three: 0.4506, eval/one: 0.7304, eval/two: -0.2413, eval/three: 0.7641 \n",
      "Epoch: 55, train/loss: 0.3722, eval/loss: 0.8596, train/r2: 0.4519, eval/r2: 0.4166, train/one: 0.9320, train/two: -0.1791, train/three: 0.6027, eval/one: 0.6993, eval/two: -0.2500, eval/three: 0.8005 \n",
      "Epoch: 60, train/loss: 0.2667, eval/loss: 0.7700, train/r2: 0.5569, eval/r2: 0.4244, train/one: 0.9554, train/two: -0.0918, train/three: 0.8070, eval/one: 0.7375, eval/two: -0.2518, eval/three: 0.7874 \n",
      "Epoch: 65, train/loss: 0.2953, eval/loss: 0.7927, train/r2: 0.5238, eval/r2: 0.4446, train/one: 0.9506, train/two: -0.0606, train/three: 0.6814, eval/one: 0.7242, eval/two: -0.2297, eval/three: 0.8394 \n",
      "Epoch: 70, train/loss: 0.2985, eval/loss: 0.7620, train/r2: 0.4956, eval/r2: 0.4349, train/one: 0.9526, train/two: -0.1482, train/three: 0.6825, eval/one: 0.7397, eval/two: -0.2211, eval/three: 0.7860 \n",
      "Epoch: 75, train/loss: 0.2825, eval/loss: 0.7795, train/r2: 0.5244, eval/r2: 0.4348, train/one: 0.9543, train/two: -0.1202, train/three: 0.7390, eval/one: 0.7318, eval/two: -0.2336, eval/three: 0.8061 \n",
      "Epoch: 80, train/loss: 0.2901, eval/loss: 0.7726, train/r2: 0.4885, eval/r2: 0.4525, train/one: 0.9569, train/two: -0.1312, train/three: 0.6398, eval/one: 0.7321, eval/two: -0.2092, eval/three: 0.8346 \n",
      "Epoch: 85, train/loss: 0.3117, eval/loss: 0.9688, train/r2: 0.5074, eval/r2: 0.4216, train/one: 0.9469, train/two: -0.0806, train/three: 0.6560, eval/one: 0.6520, eval/two: -0.1567, eval/three: 0.7696 \n",
      "Epoch: 90, train/loss: 0.3217, eval/loss: 0.7938, train/r2: 0.4839, eval/r2: 0.4171, train/one: 0.9462, train/two: -0.1316, train/three: 0.6372, eval/one: 0.7298, eval/two: -0.1778, eval/three: 0.6993 \n",
      "Epoch: 95, train/loss: 0.2385, eval/loss: 0.8603, train/r2: 0.5787, eval/r2: 0.4431, train/one: 0.9669, train/two: 0.2154, train/three: 0.5538, eval/one: 0.6956, eval/two: -0.1787, eval/three: 0.8123 \n",
      "Epoch: 100, train/loss: 0.2572, eval/loss: 0.8785, train/r2: 0.5267, eval/r2: 0.3653, train/one: 0.9645, train/two: -0.0321, train/three: 0.6476, eval/one: 0.7000, eval/two: -0.2633, eval/three: 0.6591 \n",
      "Epoch: 105, train/loss: 0.3101, eval/loss: 0.6857, train/r2: 0.4952, eval/r2: 0.5005, train/one: 0.9484, train/two: -0.1455, train/three: 0.6826, eval/one: 0.7650, eval/two: -0.0612, eval/three: 0.7977 \n",
      "Epoch: 110, train/loss: 0.3470, eval/loss: 0.8000, train/r2: 0.4878, eval/r2: 0.4592, train/one: 0.9367, train/two: -0.1073, train/three: 0.6339, eval/one: 0.7208, eval/two: -0.1100, eval/three: 0.7666 \n",
      "Epoch: 115, train/loss: 0.3057, eval/loss: 0.7592, train/r2: 0.5637, eval/r2: 0.4376, train/one: 0.9424, train/two: 0.0535, train/three: 0.6954, eval/one: 0.7454, eval/two: 0.0114, eval/three: 0.5561 \n",
      "Epoch: 120, train/loss: 0.2579, eval/loss: 0.7386, train/r2: 0.5558, eval/r2: 0.5484, train/one: 0.9635, train/two: 0.2218, train/three: 0.4820, eval/one: 0.7370, eval/two: 0.1360, eval/three: 0.7721 \n",
      "Epoch: 125, train/loss: 0.2643, eval/loss: 0.7264, train/r2: 0.6021, eval/r2: 0.5504, train/one: 0.9536, train/two: 0.1984, train/three: 0.6543, eval/one: 0.7419, eval/two: 0.1327, eval/three: 0.7765 \n",
      "Epoch: 130, train/loss: 0.2275, eval/loss: 0.9820, train/r2: 0.7382, eval/r2: 0.4237, train/one: 0.9508, train/two: 0.5286, train/three: 0.7350, eval/one: 0.6522, eval/two: 0.1499, eval/three: 0.4691 \n",
      "Epoch: 135, train/loss: 0.2730, eval/loss: 0.9092, train/r2: 0.6435, eval/r2: 0.4823, train/one: 0.9461, train/two: 0.3333, train/three: 0.6512, eval/one: 0.6743, eval/two: 0.1758, eval/three: 0.5968 \n",
      "Epoch: 140, train/loss: 0.2948, eval/loss: 0.7482, train/r2: 0.6359, eval/r2: 0.5846, train/one: 0.9408, train/two: 0.4294, train/three: 0.5373, eval/one: 0.7289, eval/two: 0.2656, eval/three: 0.7594 \n",
      "Epoch: 145, train/loss: 0.2589, eval/loss: 0.9135, train/r2: 0.6699, eval/r2: 0.5795, train/one: 0.9500, train/two: 0.5288, train/three: 0.5308, eval/one: 0.6573, eval/two: 0.2967, eval/three: 0.7845 \n",
      "Epoch: 150, train/loss: 0.2920, eval/loss: 0.7440, train/r2: 0.7175, eval/r2: 0.6180, train/one: 0.9308, train/two: 0.5418, train/three: 0.6798, eval/one: 0.7264, eval/two: 0.3493, eval/three: 0.7784 \n",
      "Epoch: 155, train/loss: 0.2421, eval/loss: 0.7762, train/r2: 0.6694, eval/r2: 0.6280, train/one: 0.9540, train/two: 0.3813, train/three: 0.6729, eval/one: 0.7106, eval/two: 0.3606, eval/three: 0.8129 \n",
      "Epoch: 160, train/loss: 0.2407, eval/loss: 0.8438, train/r2: 0.6658, eval/r2: 0.5411, train/one: 0.9548, train/two: 0.3636, train/three: 0.6790, eval/one: 0.6912, eval/two: 0.1119, eval/three: 0.8202 \n",
      "Epoch: 165, train/loss: 0.1868, eval/loss: 0.7142, train/r2: 0.7481, eval/r2: 0.6716, train/one: 0.9637, train/two: 0.4993, train/three: 0.7813, eval/one: 0.7324, eval/two: 0.4666, eval/three: 0.8159 \n",
      "Epoch: 170, train/loss: 0.1459, eval/loss: 1.1624, train/r2: 0.8057, eval/r2: 0.1988, train/one: 0.9725, train/two: 0.6939, train/three: 0.7507, eval/one: 0.5978, eval/two: -0.5813, eval/three: 0.5798 \n",
      "Epoch: 175, train/loss: 0.2047, eval/loss: 1.1091, train/r2: 0.7300, eval/r2: 0.1700, train/one: 0.9613, train/two: 0.5871, train/three: 0.6417, eval/one: 0.6260, eval/two: -0.6103, eval/three: 0.4943 \n",
      "Epoch: 180, train/loss: 0.1936, eval/loss: 0.7574, train/r2: 0.7925, eval/r2: 0.5913, train/one: 0.9569, train/two: 0.6712, train/three: 0.7495, eval/one: 0.7256, eval/two: 0.3630, eval/three: 0.6854 \n",
      "Epoch: 185, train/loss: 0.1974, eval/loss: 0.7446, train/r2: 0.7326, eval/r2: 0.6974, train/one: 0.9630, train/two: 0.5491, train/three: 0.6858, eval/one: 0.7171, eval/two: 0.6010, eval/three: 0.7741 \n",
      "Epoch: 190, train/loss: 0.1983, eval/loss: 0.7440, train/r2: 0.7670, eval/r2: 0.6686, train/one: 0.9589, train/two: 0.6559, train/three: 0.6861, eval/one: 0.7203, eval/two: 0.4928, eval/three: 0.7927 \n",
      "Epoch: 195, train/loss: 0.1910, eval/loss: 0.6822, train/r2: 0.7599, eval/r2: 0.6987, train/one: 0.9627, train/two: 0.6596, train/three: 0.6575, eval/one: 0.7432, eval/two: 0.5449, eval/three: 0.8080 \n",
      "Epoch: 200, train/loss: 0.1680, eval/loss: 0.6885, train/r2: 0.7997, eval/r2: 0.7182, train/one: 0.9650, train/two: 0.6647, train/three: 0.7695, eval/one: 0.7380, eval/two: 0.5987, eval/three: 0.8181 \n",
      "Epoch: 205, train/loss: 0.2648, eval/loss: 0.7006, train/r2: 0.7052, eval/r2: 0.6976, train/one: 0.9418, train/two: 0.4883, train/three: 0.6854, eval/one: 0.7355, eval/two: 0.5523, eval/three: 0.8051 \n",
      "Epoch: 210, train/loss: 0.2011, eval/loss: 0.7301, train/r2: 0.7633, eval/r2: 0.7019, train/one: 0.9571, train/two: 0.5689, train/three: 0.7640, eval/one: 0.7218, eval/two: 0.5619, eval/three: 0.8220 \n",
      "Epoch: 215, train/loss: 0.1873, eval/loss: 0.7100, train/r2: 0.8063, eval/r2: 0.7035, train/one: 0.9580, train/two: 0.7387, train/three: 0.7222, eval/one: 0.7301, eval/two: 0.5492, eval/three: 0.8311 \n",
      "Epoch: 220, train/loss: 0.2251, eval/loss: 0.6914, train/r2: 0.7561, eval/r2: 0.7082, train/one: 0.9496, train/two: 0.5698, train/three: 0.7491, eval/one: 0.7375, eval/two: 0.5460, eval/three: 0.8413 \n",
      "Epoch: 225, train/loss: 0.2007, eval/loss: 0.6605, train/r2: 0.8016, eval/r2: 0.7205, train/one: 0.9534, train/two: 0.7075, train/three: 0.7439, eval/one: 0.7497, eval/two: 0.5832, eval/three: 0.8287 \n",
      "Epoch: 230, train/loss: 0.1957, eval/loss: 0.6681, train/r2: 0.7367, eval/r2: 0.6912, train/one: 0.9636, train/two: 0.5929, train/three: 0.6537, eval/one: 0.7509, eval/two: 0.5499, eval/three: 0.7728 \n",
      "Epoch: 235, train/loss: 0.2157, eval/loss: 0.6801, train/r2: 0.7920, eval/r2: 0.6312, train/one: 0.9501, train/two: 0.7479, train/three: 0.6780, eval/one: 0.7534, eval/two: 0.4003, eval/three: 0.7399 \n",
      "Epoch: 240, train/loss: 0.2269, eval/loss: 0.6396, train/r2: 0.8016, eval/r2: 0.6978, train/one: 0.9444, train/two: 0.7426, train/three: 0.7178, eval/one: 0.7626, eval/two: 0.5610, eval/three: 0.7698 \n",
      "Epoch: 245, train/loss: 0.1988, eval/loss: 0.9332, train/r2: 0.7762, eval/r2: 0.6804, train/one: 0.9580, train/two: 0.7031, train/three: 0.6674, eval/one: 0.6367, eval/two: 0.6035, eval/three: 0.8010 \n",
      "Epoch: 250, train/loss: 0.2163, eval/loss: 0.6340, train/r2: 0.8041, eval/r2: 0.7321, train/one: 0.9483, train/two: 0.7723, train/three: 0.6917, eval/one: 0.7602, eval/two: 0.6253, eval/three: 0.8109 \n",
      "Epoch: 255, train/loss: 0.2190, eval/loss: 0.6757, train/r2: 0.7690, eval/r2: 0.7270, train/one: 0.9519, train/two: 0.7123, train/three: 0.6428, eval/one: 0.7428, eval/two: 0.6307, eval/three: 0.8075 \n",
      "Epoch: 260, train/loss: 0.2048, eval/loss: 0.6787, train/r2: 0.7672, eval/r2: 0.7015, train/one: 0.9572, train/two: 0.6993, train/three: 0.6451, eval/one: 0.7461, eval/two: 0.6324, eval/three: 0.7261 \n",
      "Epoch: 265, train/loss: 0.2173, eval/loss: 0.6869, train/r2: 0.7644, eval/r2: 0.6855, train/one: 0.9532, train/two: 0.7123, train/three: 0.6276, eval/one: 0.7442, eval/two: 0.5741, eval/three: 0.7381 \n",
      "Epoch: 270, train/loss: 0.2800, eval/loss: 0.7022, train/r2: 0.6855, eval/r2: 0.7196, train/one: 0.9400, train/two: 0.5347, train/three: 0.5818, eval/one: 0.7317, eval/two: 0.6007, eval/three: 0.8263 \n",
      "Epoch: 275, train/loss: 0.2248, eval/loss: 0.7041, train/r2: 0.7544, eval/r2: 0.7038, train/one: 0.9510, train/two: 0.6451, train/three: 0.6671, eval/one: 0.7330, eval/two: 0.5654, eval/three: 0.8130 \n",
      "Epoch: 280, train/loss: 0.2080, eval/loss: 0.7375, train/r2: 0.7822, eval/r2: 0.7138, train/one: 0.9527, train/two: 0.6388, train/three: 0.7550, eval/one: 0.7183, eval/two: 0.6540, eval/three: 0.7691 \n",
      "Epoch: 285, train/loss: 0.2134, eval/loss: 0.8461, train/r2: 0.8346, eval/r2: 0.5175, train/one: 0.9444, train/two: 0.7551, train/three: 0.8043, eval/one: 0.6976, eval/two: 0.2593, eval/three: 0.5956 \n",
      "Epoch: 290, train/loss: 0.1723, eval/loss: 0.8008, train/r2: 0.7944, eval/r2: 0.4694, train/one: 0.9641, train/two: 0.6541, train/three: 0.7652, eval/one: 0.7189, eval/two: -0.0945, eval/three: 0.7838 \n",
      "Epoch: 295, train/loss: 0.1905, eval/loss: 0.7474, train/r2: 0.7930, eval/r2: 0.5598, train/one: 0.9582, train/two: 0.6895, train/three: 0.7314, eval/one: 0.7316, eval/two: 0.1620, eval/three: 0.7858 \n",
      "Epoch: 300, train/loss: 0.1817, eval/loss: 0.8495, train/r2: 0.8596, eval/r2: 0.6576, train/one: 0.9527, train/two: 0.8022, train/three: 0.8237, eval/one: 0.6755, eval/two: 0.4870, eval/three: 0.8102 \n",
      "Epoch: 305, train/loss: 0.1765, eval/loss: 0.6810, train/r2: 0.7915, eval/r2: 0.6989, train/one: 0.9635, train/two: 0.6899, train/three: 0.7210, eval/one: 0.7443, eval/two: 0.5715, eval/three: 0.7808 \n",
      "Epoch: 310, train/loss: 0.1415, eval/loss: 0.7964, train/r2: 0.8020, eval/r2: 0.6536, train/one: 0.9750, train/two: 0.7165, train/three: 0.7146, eval/one: 0.7003, eval/two: 0.5171, eval/three: 0.7436 \n",
      "Epoch: 315, train/loss: 0.1555, eval/loss: 0.7959, train/r2: 0.8125, eval/r2: 0.7113, train/one: 0.9676, train/two: 0.6684, train/three: 0.8015, eval/one: 0.6926, eval/two: 0.6381, eval/three: 0.8033 \n",
      "Epoch: 320, train/loss: 0.1572, eval/loss: 0.7049, train/r2: 0.8125, eval/r2: 0.6804, train/one: 0.9687, train/two: 0.7817, train/three: 0.6871, eval/one: 0.7356, eval/two: 0.5008, eval/three: 0.8047 \n",
      "Epoch: 325, train/loss: 0.2145, eval/loss: 0.7197, train/r2: 0.7674, eval/r2: 0.6749, train/one: 0.9520, train/two: 0.5892, train/three: 0.7610, eval/one: 0.7297, eval/two: 0.4858, eval/three: 0.8092 \n",
      "Epoch: 330, train/loss: 0.1790, eval/loss: 0.7001, train/r2: 0.8098, eval/r2: 0.6566, train/one: 0.9611, train/two: 0.7781, train/three: 0.6901, eval/one: 0.7403, eval/two: 0.4195, eval/three: 0.8100 \n",
      "Epoch: 335, train/loss: 0.1877, eval/loss: 0.6788, train/r2: 0.7991, eval/r2: 0.6590, train/one: 0.9580, train/two: 0.6748, train/three: 0.7645, eval/one: 0.7500, eval/two: 0.4493, eval/three: 0.7778 \n",
      "Epoch: 340, train/loss: 0.1527, eval/loss: 0.6773, train/r2: 0.8386, eval/r2: 0.7119, train/one: 0.9659, train/two: 0.7538, train/three: 0.7960, eval/one: 0.7438, eval/two: 0.5832, eval/three: 0.8087 \n",
      "Epoch: 345, train/loss: 0.2306, eval/loss: 0.7112, train/r2: 0.7791, eval/r2: 0.7275, train/one: 0.9451, train/two: 0.6430, train/three: 0.7491, eval/one: 0.7274, eval/two: 0.6534, eval/three: 0.8016 \n",
      "Epoch: 350, train/loss: 0.1776, eval/loss: 0.6713, train/r2: 0.8125, eval/r2: 0.7179, train/one: 0.9599, train/two: 0.6918, train/three: 0.7858, eval/one: 0.7459, eval/two: 0.6108, eval/three: 0.7971 \n",
      "Epoch: 355, train/loss: 0.1802, eval/loss: 0.7284, train/r2: 0.8069, eval/r2: 0.6911, train/one: 0.9595, train/two: 0.6736, train/three: 0.7876, eval/one: 0.7250, eval/two: 0.5870, eval/three: 0.7613 \n",
      "Epoch: 360, train/loss: 0.1919, eval/loss: 0.6653, train/r2: 0.7951, eval/r2: 0.7171, train/one: 0.9573, train/two: 0.6851, train/three: 0.7428, eval/one: 0.7492, eval/two: 0.6346, eval/three: 0.7674 \n",
      "Epoch: 365, train/loss: 0.1674, eval/loss: 0.6277, train/r2: 0.8411, eval/r2: 0.7280, train/one: 0.9595, train/two: 0.7169, train/three: 0.8470, eval/one: 0.7639, eval/two: 0.6319, eval/three: 0.7883 \n",
      "Epoch: 370, train/loss: 0.1825, eval/loss: 0.6402, train/r2: 0.8044, eval/r2: 0.7248, train/one: 0.9603, train/two: 0.7492, train/three: 0.7036, eval/one: 0.7588, eval/two: 0.6240, eval/three: 0.7916 \n",
      "Epoch: 375, train/loss: 0.1873, eval/loss: 0.6929, train/r2: 0.8294, eval/r2: 0.7112, train/one: 0.9549, train/two: 0.7679, train/three: 0.7654, eval/one: 0.7376, eval/two: 0.6095, eval/three: 0.7865 \n",
      "Epoch: 380, train/loss: 0.2011, eval/loss: 0.6804, train/r2: 0.8246, eval/r2: 0.7084, train/one: 0.9505, train/two: 0.7568, train/three: 0.7664, eval/one: 0.7435, eval/two: 0.6019, eval/three: 0.7799 \n",
      "Epoch: 385, train/loss: 0.1847, eval/loss: 0.6743, train/r2: 0.7760, eval/r2: 0.7151, train/one: 0.9625, train/two: 0.6619, train/three: 0.7034, eval/one: 0.7449, eval/two: 0.5991, eval/three: 0.8013 \n",
      "Epoch: 390, train/loss: 0.1574, eval/loss: 0.6773, train/r2: 0.8071, eval/r2: 0.7264, train/one: 0.9686, train/two: 0.7224, train/three: 0.7302, eval/one: 0.7422, eval/two: 0.6318, eval/three: 0.8051 \n",
      "Epoch: 395, train/loss: 0.1865, eval/loss: 0.6405, train/r2: 0.8291, eval/r2: 0.7348, train/one: 0.9537, train/two: 0.6692, train/three: 0.8645, eval/one: 0.7573, eval/two: 0.6474, eval/three: 0.7998 \n",
      "Epoch: 400, train/loss: 0.1684, eval/loss: 0.6496, train/r2: 0.7819, eval/r2: 0.7284, train/one: 0.9681, train/two: 0.6986, train/three: 0.6790, eval/one: 0.7541, eval/two: 0.6324, eval/three: 0.7987 \n",
      "Epoch: 405, train/loss: 0.2324, eval/loss: 0.6490, train/r2: 0.7766, eval/r2: 0.7171, train/one: 0.9452, train/two: 0.6657, train/three: 0.7188, eval/one: 0.7556, eval/two: 0.5960, eval/three: 0.7998 \n",
      "Epoch: 410, train/loss: 0.1464, eval/loss: 0.6508, train/r2: 0.8193, eval/r2: 0.7256, train/one: 0.9704, train/two: 0.7051, train/three: 0.7824, eval/one: 0.7540, eval/two: 0.6277, eval/three: 0.7953 \n",
      "Epoch: 415, train/loss: 0.1604, eval/loss: 0.6626, train/r2: 0.8027, eval/r2: 0.7247, train/one: 0.9680, train/two: 0.7113, train/three: 0.7288, eval/one: 0.7491, eval/two: 0.6351, eval/three: 0.7900 \n",
      "Epoch: 420, train/loss: 0.1362, eval/loss: 0.6638, train/r2: 0.8137, eval/r2: 0.7264, train/one: 0.9745, train/two: 0.6745, train/three: 0.7919, eval/one: 0.7484, eval/two: 0.6408, eval/three: 0.7901 \n",
      "Epoch: 425, train/loss: 0.1435, eval/loss: 0.6687, train/r2: 0.8546, eval/r2: 0.7282, train/one: 0.9671, train/two: 0.7788, train/three: 0.8180, eval/one: 0.7459, eval/two: 0.6423, eval/three: 0.7963 \n",
      "Epoch: 430, train/loss: 0.1819, eval/loss: 0.6651, train/r2: 0.7733, eval/r2: 0.7275, train/one: 0.9645, train/two: 0.6995, train/three: 0.6558, eval/one: 0.7474, eval/two: 0.6343, eval/three: 0.8007 \n",
      "Epoch: 435, train/loss: 0.1529, eval/loss: 0.6481, train/r2: 0.8469, eval/r2: 0.7234, train/one: 0.9653, train/two: 0.8106, train/three: 0.7650, eval/one: 0.7553, eval/two: 0.6158, eval/three: 0.7991 \n",
      "Epoch: 440, train/loss: 0.1845, eval/loss: 0.6446, train/r2: 0.8209, eval/r2: 0.7167, train/one: 0.9573, train/two: 0.7733, train/three: 0.7321, eval/one: 0.7578, eval/two: 0.6019, eval/three: 0.7905 \n",
      "Epoch: 445, train/loss: 0.1979, eval/loss: 0.6418, train/r2: 0.7734, eval/r2: 0.7126, train/one: 0.9571, train/two: 0.5934, train/three: 0.7696, eval/one: 0.7597, eval/two: 0.5985, eval/three: 0.7795 \n",
      "Epoch: 450, train/loss: 0.1398, eval/loss: 0.6431, train/r2: 0.8519, eval/r2: 0.7096, train/one: 0.9686, train/two: 0.7595, train/three: 0.8276, eval/one: 0.7596, eval/two: 0.5956, eval/three: 0.7737 \n",
      "Epoch: 455, train/loss: 0.1700, eval/loss: 0.6452, train/r2: 0.8443, eval/r2: 0.7069, train/one: 0.9594, train/two: 0.8000, train/three: 0.7734, eval/one: 0.7591, eval/two: 0.5905, eval/three: 0.7710 \n",
      "Epoch: 460, train/loss: 0.1852, eval/loss: 0.6486, train/r2: 0.7811, eval/r2: 0.7054, train/one: 0.9614, train/two: 0.6509, train/three: 0.7310, eval/one: 0.7578, eval/two: 0.5893, eval/three: 0.7692 \n",
      "Epoch: 465, train/loss: 0.1924, eval/loss: 0.6486, train/r2: 0.8082, eval/r2: 0.7062, train/one: 0.9563, train/two: 0.7663, train/three: 0.7019, eval/one: 0.7577, eval/two: 0.5920, eval/three: 0.7687 \n",
      "Epoch: 470, train/loss: 0.1414, eval/loss: 0.6487, train/r2: 0.8088, eval/r2: 0.7070, train/one: 0.9729, train/two: 0.6403, train/three: 0.8133, eval/one: 0.7575, eval/two: 0.5932, eval/three: 0.7702 \n",
      "Epoch: 475, train/loss: 0.1485, eval/loss: 0.6491, train/r2: 0.8651, eval/r2: 0.7074, train/one: 0.9636, train/two: 0.7770, train/three: 0.8547, eval/one: 0.7573, eval/two: 0.5932, eval/three: 0.7717 \n",
      "Epoch: 480, train/loss: 0.1685, eval/loss: 0.6489, train/r2: 0.8115, eval/r2: 0.7083, train/one: 0.9641, train/two: 0.7384, train/three: 0.7321, eval/one: 0.7573, eval/two: 0.5953, eval/three: 0.7723 \n",
      "Epoch: 485, train/loss: 0.2009, eval/loss: 0.6497, train/r2: 0.8140, eval/r2: 0.7086, train/one: 0.9512, train/two: 0.6909, train/three: 0.7999, eval/one: 0.7569, eval/two: 0.5957, eval/three: 0.7733 \n",
      "Epoch: 490, train/loss: 0.1498, eval/loss: 0.6507, train/r2: 0.8299, eval/r2: 0.7086, train/one: 0.9679, train/two: 0.7320, train/three: 0.7897, eval/one: 0.7564, eval/two: 0.5951, eval/three: 0.7741 \n",
      "Epoch: 495, train/loss: 0.1828, eval/loss: 0.6512, train/r2: 0.8235, eval/r2: 0.7084, train/one: 0.9576, train/two: 0.7811, train/three: 0.7318, eval/one: 0.7562, eval/two: 0.5949, eval/three: 0.7741 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 253 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 253 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-109/metadata\n",
      "0.4816022265426024\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b32930f2ee41f5a24fec56ca00e34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 4.2981, eval/loss: 28.9346, train/r2: -0.3923, eval/r2: -9.7168, train/one: -0.4066, train/two: -0.7542, train/three: -0.0159, eval/one: -8.9601, eval/two: -0.1320, eval/three: -20.0582 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train/loss: 2.6903, eval/loss: 10.3084, train/r2: -0.0104, eval/r2: -1.8941, train/one: 0.1336, train/two: -0.5748, train/three: 0.4101, eval/one: -2.6438, eval/two: -0.0464, eval/three: -2.9921 \n",
      "Epoch: 10, train/loss: 1.1481, eval/loss: 1.2307, train/r2: 0.1841, eval/r2: -0.0328, train/one: 0.6814, train/two: -0.4473, train/three: 0.3182, eval/one: 0.6426, eval/two: -0.1831, eval/three: -0.5579 \n",
      "Epoch: 15, train/loss: 0.5708, eval/loss: 5.1511, train/r2: 0.1847, eval/r2: -0.3638, train/one: 0.8902, train/two: -0.8026, train/three: 0.4666, eval/one: -0.8267, eval/two: -0.1720, eval/three: -0.0926 \n",
      "Epoch: 20, train/loss: 0.4081, eval/loss: 15.4838, train/r2: 0.3335, eval/r2: -1.3533, train/one: 0.9310, train/two: -0.5285, train/three: 0.5979, eval/one: -4.6870, eval/two: -0.1236, eval/three: 0.7506 \n",
      "Epoch: 25, train/loss: 0.4703, eval/loss: 6.9604, train/r2: 0.3907, eval/r2: -0.3214, train/one: 0.9018, train/two: -0.3355, train/three: 0.6057, eval/one: -1.5258, eval/two: -0.1247, eval/three: 0.6862 \n",
      "Epoch: 30, train/loss: 0.3780, eval/loss: 2.7939, train/r2: 0.3815, eval/r2: 0.1821, train/one: 0.9381, train/two: -0.3370, train/three: 0.5435, eval/one: 0.0192, eval/two: -0.0201, eval/three: 0.5473 \n",
      "Epoch: 35, train/loss: 0.4692, eval/loss: 1.7975, train/r2: 0.3202, eval/r2: 0.3092, train/one: 0.9091, train/two: -0.5848, train/three: 0.6364, eval/one: 0.3879, eval/two: 0.0208, eval/three: 0.5188 \n",
      "Epoch: 40, train/loss: 0.4615, eval/loss: 0.4976, train/r2: 0.1991, eval/r2: 0.5980, train/one: 0.9289, train/two: -0.7924, train/three: 0.4607, eval/one: 0.8560, eval/two: 0.0319, eval/three: 0.9061 \n",
      "Epoch: 45, train/loss: 0.3833, eval/loss: 0.6572, train/r2: 0.2382, eval/r2: 0.4557, train/one: 0.9515, train/two: -0.7857, train/three: 0.5489, eval/one: 0.8098, eval/two: 0.0366, eval/three: 0.5206 \n",
      "Epoch: 50, train/loss: 0.3565, eval/loss: 0.4539, train/r2: 0.4003, eval/r2: 0.5834, train/one: 0.9460, train/two: -0.1959, train/three: 0.4509, eval/one: 0.8742, eval/two: 0.0353, eval/three: 0.8405 \n",
      "Epoch: 55, train/loss: 0.3717, eval/loss: 0.3767, train/r2: 0.3960, eval/r2: 0.5579, train/one: 0.9366, train/two: -0.3915, train/three: 0.6429, eval/one: 0.9067, eval/two: 0.0151, eval/three: 0.7519 \n",
      "Epoch: 60, train/loss: 0.3130, eval/loss: 0.3814, train/r2: 0.4621, eval/r2: 0.5728, train/one: 0.9502, train/two: -0.2467, train/three: 0.6829, eval/one: 0.9033, eval/two: 0.0236, eval/three: 0.7917 \n",
      "Epoch: 65, train/loss: 0.3057, eval/loss: 0.5962, train/r2: 0.4373, eval/r2: 0.5815, train/one: 0.9582, train/two: -0.2061, train/three: 0.5597, eval/one: 0.8198, eval/two: 0.0461, eval/three: 0.8784 \n",
      "Epoch: 70, train/loss: 0.3157, eval/loss: 0.3956, train/r2: 0.3681, eval/r2: 0.5925, train/one: 0.9629, train/two: -0.3770, train/three: 0.5184, eval/one: 0.8956, eval/two: 0.0380, eval/three: 0.8440 \n",
      "Epoch: 75, train/loss: 0.4034, eval/loss: 0.5892, train/r2: 0.3727, eval/r2: 0.5984, train/one: 0.9298, train/two: -0.3506, train/three: 0.5390, eval/one: 0.8207, eval/two: 0.0586, eval/three: 0.9158 \n",
      "Epoch: 80, train/loss: 0.3867, eval/loss: 0.3738, train/r2: 0.3047, eval/r2: 0.6153, train/one: 0.9425, train/two: -0.6005, train/three: 0.5721, eval/one: 0.9017, eval/two: 0.0262, eval/three: 0.9182 \n",
      "Epoch: 85, train/loss: 0.3263, eval/loss: 0.3731, train/r2: 0.4731, eval/r2: 0.6192, train/one: 0.9458, train/two: -0.1318, train/three: 0.6054, eval/one: 0.9014, eval/two: 0.0571, eval/three: 0.8992 \n",
      "Epoch: 90, train/loss: 0.2876, eval/loss: 0.3552, train/r2: 0.4529, eval/r2: 0.6178, train/one: 0.9602, train/two: -0.2960, train/three: 0.6943, eval/one: 0.9085, eval/two: 0.0248, eval/three: 0.9199 \n",
      "Epoch: 95, train/loss: 0.3187, eval/loss: 0.3724, train/r2: 0.3771, eval/r2: 0.5354, train/one: 0.9578, train/two: -0.4824, train/three: 0.6558, eval/one: 0.9107, eval/two: 0.0228, eval/three: 0.6728 \n",
      "Epoch: 100, train/loss: 0.3138, eval/loss: 0.5039, train/r2: 0.3806, eval/r2: 0.5620, train/one: 0.9590, train/two: -0.4861, train/three: 0.6690, eval/one: 0.8573, eval/two: 0.0524, eval/three: 0.7763 \n",
      "Epoch: 105, train/loss: 0.3189, eval/loss: 0.4501, train/r2: 0.4433, eval/r2: 0.5667, train/one: 0.9505, train/two: -0.2828, train/three: 0.6623, eval/one: 0.8774, eval/two: 0.0504, eval/three: 0.7722 \n",
      "Epoch: 110, train/loss: 0.2745, eval/loss: 0.6483, train/r2: 0.4524, eval/r2: 0.5784, train/one: 0.9676, train/two: -0.1912, train/three: 0.5807, eval/one: 0.8001, eval/two: 0.0491, eval/three: 0.8859 \n",
      "Epoch: 115, train/loss: 0.2993, eval/loss: 0.8142, train/r2: 0.4641, eval/r2: 0.3314, train/one: 0.9577, train/two: -0.1242, train/three: 0.5587, eval/one: 0.7627, eval/two: 0.0577, eval/three: 0.1737 \n",
      "Epoch: 120, train/loss: 0.3538, eval/loss: 0.4426, train/r2: 0.3822, eval/r2: 0.4052, train/one: 0.9443, train/two: -0.4566, train/three: 0.6588, eval/one: 0.8973, eval/two: 0.0869, eval/three: 0.2315 \n",
      "Epoch: 125, train/loss: 0.3175, eval/loss: 0.4129, train/r2: 0.3929, eval/r2: 0.5791, train/one: 0.9571, train/two: -0.4095, train/three: 0.6310, eval/one: 0.8903, eval/two: 0.0756, eval/three: 0.7715 \n",
      "Epoch: 130, train/loss: 0.2947, eval/loss: 0.6266, train/r2: 0.5357, eval/r2: 0.4938, train/one: 0.9476, train/two: -0.0806, train/three: 0.7401, eval/one: 0.8172, eval/two: 0.1134, eval/three: 0.5507 \n",
      "Epoch: 135, train/loss: 0.2964, eval/loss: 0.3489, train/r2: 0.5494, eval/r2: 0.6065, train/one: 0.9451, train/two: -0.0583, train/three: 0.7616, eval/one: 0.9118, eval/two: 0.1146, eval/three: 0.7932 \n",
      "Epoch: 140, train/loss: 0.3276, eval/loss: 0.4080, train/r2: 0.4286, eval/r2: 0.4879, train/one: 0.9482, train/two: -0.3556, train/three: 0.6932, eval/one: 0.9018, eval/two: 0.0935, eval/three: 0.4685 \n",
      "Epoch: 145, train/loss: 0.3494, eval/loss: 0.4695, train/r2: 0.3678, eval/r2: 0.2490, train/one: 0.9473, train/two: -0.5100, train/three: 0.6662, eval/one: 0.9035, eval/two: 0.1065, eval/three: -0.2631 \n",
      "Epoch: 150, train/loss: 0.2095, eval/loss: 0.3775, train/r2: 0.5743, eval/r2: 0.5878, train/one: 0.9743, train/two: -0.0138, train/three: 0.7626, eval/one: 0.9026, eval/two: 0.1422, eval/three: 0.7185 \n",
      "Epoch: 155, train/loss: 0.2279, eval/loss: 0.3610, train/r2: 0.5333, eval/r2: 0.6478, train/one: 0.9718, train/two: -0.1427, train/three: 0.7710, eval/one: 0.9026, eval/two: 0.1428, eval/three: 0.8979 \n",
      "Epoch: 160, train/loss: 0.2849, eval/loss: 0.3919, train/r2: 0.5752, eval/r2: 0.6208, train/one: 0.9474, train/two: 0.0555, train/three: 0.7228, eval/one: 0.8936, eval/two: 0.1474, eval/three: 0.8213 \n",
      "Epoch: 165, train/loss: 0.2272, eval/loss: 0.3251, train/r2: 0.5734, eval/r2: 0.6560, train/one: 0.9689, train/two: 0.0338, train/three: 0.7176, eval/one: 0.9153, eval/two: 0.1801, eval/three: 0.8726 \n",
      "Epoch: 170, train/loss: 0.2713, eval/loss: 0.3141, train/r2: 0.5964, eval/r2: 0.6919, train/one: 0.9508, train/two: 0.1489, train/three: 0.6896, eval/one: 0.9153, eval/two: 0.2765, eval/three: 0.8839 \n",
      "Epoch: 175, train/loss: 0.2549, eval/loss: 0.5230, train/r2: 0.5858, eval/r2: 0.5500, train/one: 0.9585, train/two: 0.1346, train/three: 0.6643, eval/one: 0.8501, eval/two: 0.3185, eval/three: 0.4815 \n",
      "Epoch: 180, train/loss: 0.2608, eval/loss: 0.4273, train/r2: 0.6310, eval/r2: 0.5996, train/one: 0.9503, train/two: 0.2244, train/three: 0.7184, eval/one: 0.8813, eval/two: 0.3670, eval/three: 0.5504 \n",
      "Epoch: 185, train/loss: 0.2325, eval/loss: 0.6819, train/r2: 0.6549, eval/r2: 0.6861, train/one: 0.9598, train/two: 0.3657, train/three: 0.6393, eval/one: 0.7743, eval/two: 0.4018, eval/three: 0.8822 \n",
      "Epoch: 190, train/loss: 0.2040, eval/loss: 0.4228, train/r2: 0.7625, eval/r2: 0.6902, train/one: 0.9559, train/two: 0.5662, train/three: 0.7652, eval/one: 0.8726, eval/two: 0.5494, eval/three: 0.6485 \n",
      "Epoch: 195, train/loss: 0.2204, eval/loss: 0.2505, train/r2: 0.6530, eval/r2: 0.7790, train/one: 0.9647, train/two: 0.3652, train/three: 0.6291, eval/one: 0.9294, eval/two: 0.5314, eval/three: 0.8762 \n",
      "Epoch: 200, train/loss: 0.2346, eval/loss: 0.4394, train/r2: 0.6841, eval/r2: 0.8093, train/one: 0.9539, train/two: 0.3692, train/three: 0.7292, eval/one: 0.8530, eval/two: 0.6896, eval/three: 0.8854 \n",
      "Epoch: 205, train/loss: 0.2221, eval/loss: 0.2875, train/r2: 0.7265, eval/r2: 0.8272, train/one: 0.9550, train/two: 0.5422, train/three: 0.6823, eval/one: 0.9093, eval/two: 0.7137, eval/three: 0.8586 \n",
      "Epoch: 210, train/loss: 0.2173, eval/loss: 0.2666, train/r2: 0.6994, eval/r2: 0.8120, train/one: 0.9592, train/two: 0.4373, train/three: 0.7015, eval/one: 0.9192, eval/two: 0.6522, eval/three: 0.8646 \n",
      "Epoch: 215, train/loss: 0.2238, eval/loss: 0.2980, train/r2: 0.6988, eval/r2: 0.8413, train/one: 0.9559, train/two: 0.3935, train/three: 0.7469, eval/one: 0.9036, eval/two: 0.7415, eval/three: 0.8787 \n",
      "Epoch: 220, train/loss: 0.1823, eval/loss: 0.3508, train/r2: 0.7811, eval/r2: 0.7827, train/one: 0.9621, train/two: 0.6266, train/three: 0.7547, eval/one: 0.8898, eval/two: 0.6796, eval/three: 0.7786 \n",
      "Epoch: 225, train/loss: 0.2173, eval/loss: 0.2661, train/r2: 0.7449, eval/r2: 0.7939, train/one: 0.9526, train/two: 0.5033, train/three: 0.7789, eval/one: 0.9208, eval/two: 0.7744, eval/three: 0.6866 \n",
      "Epoch: 230, train/loss: 0.2264, eval/loss: 0.3124, train/r2: 0.7262, eval/r2: 0.6054, train/one: 0.9525, train/two: 0.5019, train/three: 0.7242, eval/one: 0.9255, eval/two: 0.2134, eval/three: 0.6772 \n",
      "Epoch: 235, train/loss: 0.1620, eval/loss: 0.2679, train/r2: 0.8099, eval/r2: 0.7713, train/one: 0.9658, train/two: 0.6779, train/three: 0.7860, eval/one: 0.9225, eval/two: 0.7668, eval/three: 0.6247 \n",
      "Epoch: 240, train/loss: 0.1628, eval/loss: 0.3219, train/r2: 0.7869, eval/r2: 0.8177, train/one: 0.9680, train/two: 0.6078, train/three: 0.7850, eval/one: 0.8970, eval/two: 0.7325, eval/three: 0.8238 \n",
      "Epoch: 245, train/loss: 0.2079, eval/loss: 0.2430, train/r2: 0.7126, eval/r2: 0.8281, train/one: 0.9604, train/two: 0.4365, train/three: 0.7410, eval/one: 0.9258, eval/two: 0.8058, eval/three: 0.7525 \n",
      "Epoch: 250, train/loss: 0.2203, eval/loss: 0.3620, train/r2: 0.7485, eval/r2: 0.6514, train/one: 0.9513, train/two: 0.5227, train/three: 0.7716, eval/one: 0.9011, eval/two: 0.3197, eval/three: 0.7333 \n",
      "Epoch: 255, train/loss: 0.2012, eval/loss: 0.2429, train/r2: 0.7768, eval/r2: 0.8053, train/one: 0.9553, train/two: 0.6075, train/three: 0.7676, eval/one: 0.9291, eval/two: 0.6200, eval/three: 0.8668 \n",
      "Epoch: 260, train/loss: 0.1892, eval/loss: 0.2390, train/r2: 0.7380, eval/r2: 0.8558, train/one: 0.9654, train/two: 0.5473, train/three: 0.7012, eval/one: 0.9247, eval/two: 0.7527, eval/three: 0.8901 \n",
      "Epoch: 265, train/loss: 0.2044, eval/loss: 0.3719, train/r2: 0.7514, eval/r2: 0.8099, train/one: 0.9572, train/two: 0.5441, train/three: 0.7530, eval/one: 0.8787, eval/two: 0.7179, eval/three: 0.8333 \n",
      "Epoch: 270, train/loss: 0.1814, eval/loss: 0.4412, train/r2: 0.7615, eval/r2: 0.8343, train/one: 0.9630, train/two: 0.4976, train/three: 0.8239, eval/one: 0.8492, eval/two: 0.7915, eval/three: 0.8622 \n",
      "Epoch: 275, train/loss: 0.2080, eval/loss: 0.3058, train/r2: 0.7780, eval/r2: 0.8445, train/one: 0.9530, train/two: 0.6252, train/three: 0.7557, eval/one: 0.9003, eval/two: 0.7399, eval/three: 0.8934 \n",
      "Epoch: 280, train/loss: 0.1520, eval/loss: 0.2636, train/r2: 0.7904, eval/r2: 0.8516, train/one: 0.9707, train/two: 0.5763, train/three: 0.8242, eval/one: 0.9157, eval/two: 0.7546, eval/three: 0.8846 \n",
      "Epoch: 285, train/loss: 0.1994, eval/loss: 0.2585, train/r2: 0.7065, eval/r2: 0.8407, train/one: 0.9644, train/two: 0.4248, train/three: 0.7303, eval/one: 0.9189, eval/two: 0.7379, eval/three: 0.8654 \n",
      "Epoch: 290, train/loss: 0.1884, eval/loss: 0.2414, train/r2: 0.7852, eval/r2: 0.8359, train/one: 0.9592, train/two: 0.6310, train/three: 0.7654, eval/one: 0.9261, eval/two: 0.7088, eval/three: 0.8728 \n",
      "Epoch: 295, train/loss: 0.1690, eval/loss: 0.2990, train/r2: 0.7494, eval/r2: 0.8348, train/one: 0.9709, train/two: 0.5440, train/three: 0.7334, eval/one: 0.9037, eval/two: 0.7894, eval/three: 0.8112 \n",
      "Epoch: 300, train/loss: 0.1676, eval/loss: 0.2296, train/r2: 0.8005, eval/r2: 0.8425, train/one: 0.9643, train/two: 0.6313, train/three: 0.8058, eval/one: 0.9295, eval/two: 0.7919, eval/three: 0.8060 \n",
      "Epoch: 305, train/loss: 0.1645, eval/loss: 0.2208, train/r2: 0.8140, eval/r2: 0.8647, train/one: 0.9640, train/two: 0.6741, train/three: 0.8039, eval/one: 0.9305, eval/two: 0.8029, eval/three: 0.8607 \n",
      "Epoch: 310, train/loss: 0.1783, eval/loss: 0.2203, train/r2: 0.7823, eval/r2: 0.8615, train/one: 0.9650, train/two: 0.6967, train/three: 0.6853, eval/one: 0.9310, eval/two: 0.8108, eval/three: 0.8428 \n",
      "Epoch: 315, train/loss: 0.2106, eval/loss: 0.2228, train/r2: 0.7723, eval/r2: 0.8556, train/one: 0.9526, train/two: 0.6072, train/three: 0.7570, eval/one: 0.9309, eval/two: 0.7708, eval/three: 0.8651 \n",
      "Epoch: 320, train/loss: 0.1882, eval/loss: 0.2490, train/r2: 0.8007, eval/r2: 0.8378, train/one: 0.9570, train/two: 0.6564, train/three: 0.7887, eval/one: 0.9228, eval/two: 0.7346, eval/three: 0.8559 \n",
      "Epoch: 325, train/loss: 0.1889, eval/loss: 0.2426, train/r2: 0.7927, eval/r2: 0.8402, train/one: 0.9577, train/two: 0.6351, train/three: 0.7853, eval/one: 0.9251, eval/two: 0.7220, eval/three: 0.8736 \n",
      "Epoch: 330, train/loss: 0.1556, eval/loss: 0.2466, train/r2: 0.7903, eval/r2: 0.8628, train/one: 0.9708, train/two: 0.6393, train/three: 0.7609, eval/one: 0.9208, eval/two: 0.7960, eval/three: 0.8714 \n",
      "Epoch: 335, train/loss: 0.2243, eval/loss: 0.2996, train/r2: 0.7741, eval/r2: 0.8579, train/one: 0.9490, train/two: 0.6882, train/three: 0.6850, eval/one: 0.9009, eval/two: 0.8089, eval/three: 0.8637 \n",
      "Epoch: 340, train/loss: 0.2303, eval/loss: 0.2763, train/r2: 0.7393, eval/r2: 0.8309, train/one: 0.9501, train/two: 0.5660, train/three: 0.7018, eval/one: 0.9131, eval/two: 0.7211, eval/three: 0.8583 \n",
      "Epoch: 345, train/loss: 0.1647, eval/loss: 0.2402, train/r2: 0.8091, eval/r2: 0.8421, train/one: 0.9658, train/two: 0.7163, train/three: 0.7453, eval/one: 0.9257, eval/two: 0.7461, eval/three: 0.8547 \n",
      "Epoch: 350, train/loss: 0.2169, eval/loss: 0.2893, train/r2: 0.7846, eval/r2: 0.8247, train/one: 0.9488, train/two: 0.6404, train/three: 0.7647, eval/one: 0.9089, eval/two: 0.6989, eval/three: 0.8664 \n",
      "Epoch: 355, train/loss: 0.2001, eval/loss: 0.3084, train/r2: 0.7643, eval/r2: 0.8077, train/one: 0.9591, train/two: 0.6582, train/three: 0.6755, eval/one: 0.9034, eval/two: 0.6819, eval/three: 0.8378 \n",
      "Epoch: 360, train/loss: 0.1672, eval/loss: 0.2442, train/r2: 0.7725, eval/r2: 0.8542, train/one: 0.9682, train/two: 0.5752, train/three: 0.7740, eval/one: 0.9228, eval/two: 0.7690, eval/three: 0.8708 \n",
      "Epoch: 365, train/loss: 0.1782, eval/loss: 0.2317, train/r2: 0.8103, eval/r2: 0.8640, train/one: 0.9610, train/two: 0.7422, train/three: 0.7278, eval/one: 0.9264, eval/two: 0.8023, eval/three: 0.8632 \n",
      "Epoch: 370, train/loss: 0.1634, eval/loss: 0.2270, train/r2: 0.8323, eval/r2: 0.8689, train/one: 0.9622, train/two: 0.7210, train/three: 0.8138, eval/one: 0.9276, eval/two: 0.8119, eval/three: 0.8671 \n",
      "Epoch: 375, train/loss: 0.1373, eval/loss: 0.2229, train/r2: 0.8673, eval/r2: 0.8633, train/one: 0.9675, train/two: 0.7971, train/three: 0.8372, eval/one: 0.9298, eval/two: 0.8057, eval/three: 0.8544 \n",
      "Epoch: 380, train/loss: 0.2285, eval/loss: 0.2217, train/r2: 0.7612, eval/r2: 0.8686, train/one: 0.9493, train/two: 0.6729, train/three: 0.6612, eval/one: 0.9297, eval/two: 0.8026, eval/three: 0.8734 \n",
      "Epoch: 385, train/loss: 0.1665, eval/loss: 0.2379, train/r2: 0.8035, eval/r2: 0.8663, train/one: 0.9637, train/two: 0.6096, train/three: 0.8371, eval/one: 0.9237, eval/two: 0.8019, eval/three: 0.8734 \n",
      "Epoch: 390, train/loss: 0.1657, eval/loss: 0.2351, train/r2: 0.8000, eval/r2: 0.8629, train/one: 0.9653, train/two: 0.6403, train/three: 0.7945, eval/one: 0.9252, eval/two: 0.7998, eval/three: 0.8637 \n",
      "Epoch: 395, train/loss: 0.2122, eval/loss: 0.2393, train/r2: 0.8089, eval/r2: 0.8646, train/one: 0.9482, train/two: 0.7263, train/three: 0.7522, eval/one: 0.9234, eval/two: 0.7971, eval/three: 0.8731 \n",
      "Epoch: 400, train/loss: 0.1659, eval/loss: 0.2447, train/r2: 0.8102, eval/r2: 0.8602, train/one: 0.9633, train/two: 0.6393, train/three: 0.8281, eval/one: 0.9219, eval/two: 0.7843, eval/three: 0.8744 \n",
      "Epoch: 405, train/loss: 0.1545, eval/loss: 0.2461, train/r2: 0.8056, eval/r2: 0.8676, train/one: 0.9679, train/two: 0.6161, train/three: 0.8328, eval/one: 0.9205, eval/two: 0.8062, eval/three: 0.8762 \n",
      "Epoch: 410, train/loss: 0.2080, eval/loss: 0.2568, train/r2: 0.8013, eval/r2: 0.8663, train/one: 0.9487, train/two: 0.6214, train/three: 0.8340, eval/one: 0.9165, eval/two: 0.8106, eval/three: 0.8718 \n",
      "Epoch: 415, train/loss: 0.1939, eval/loss: 0.2511, train/r2: 0.7684, eval/r2: 0.8662, train/one: 0.9588, train/two: 0.5750, train/three: 0.7715, eval/one: 0.9187, eval/two: 0.8165, eval/three: 0.8634 \n",
      "Epoch: 420, train/loss: 0.1852, eval/loss: 0.2413, train/r2: 0.7978, eval/r2: 0.8671, train/one: 0.9590, train/two: 0.6719, train/three: 0.7625, eval/one: 0.9223, eval/two: 0.8208, eval/three: 0.8582 \n",
      "Epoch: 425, train/loss: 0.2036, eval/loss: 0.2327, train/r2: 0.7888, eval/r2: 0.8689, train/one: 0.9520, train/two: 0.5969, train/three: 0.8175, eval/one: 0.9254, eval/two: 0.8212, eval/three: 0.8600 \n",
      "Epoch: 430, train/loss: 0.2115, eval/loss: 0.2265, train/r2: 0.8121, eval/r2: 0.8687, train/one: 0.9483, train/two: 0.7447, train/three: 0.7433, eval/one: 0.9278, eval/two: 0.8223, eval/three: 0.8559 \n",
      "Epoch: 435, train/loss: 0.1568, eval/loss: 0.2264, train/r2: 0.8112, eval/r2: 0.8680, train/one: 0.9684, train/two: 0.7170, train/three: 0.7482, eval/one: 0.9279, eval/two: 0.8209, eval/three: 0.8551 \n",
      "Epoch: 440, train/loss: 0.1868, eval/loss: 0.2240, train/r2: 0.7766, eval/r2: 0.8698, train/one: 0.9612, train/two: 0.6243, train/three: 0.7444, eval/one: 0.9287, eval/two: 0.8220, eval/three: 0.8588 \n",
      "Epoch: 445, train/loss: 0.1360, eval/loss: 0.2244, train/r2: 0.8330, eval/r2: 0.8713, train/one: 0.9720, train/two: 0.7059, train/three: 0.8211, eval/one: 0.9283, eval/two: 0.8212, eval/three: 0.8644 \n",
      "Epoch: 450, train/loss: 0.2267, eval/loss: 0.2245, train/r2: 0.7808, eval/r2: 0.8709, train/one: 0.9454, train/two: 0.6236, train/three: 0.7733, eval/one: 0.9284, eval/two: 0.8194, eval/three: 0.8648 \n",
      "Epoch: 455, train/loss: 0.1475, eval/loss: 0.2230, train/r2: 0.8338, eval/r2: 0.8708, train/one: 0.9680, train/two: 0.7263, train/three: 0.8070, eval/one: 0.9289, eval/two: 0.8170, eval/three: 0.8663 \n",
      "Epoch: 460, train/loss: 0.1414, eval/loss: 0.2215, train/r2: 0.8381, eval/r2: 0.8709, train/one: 0.9699, train/two: 0.7405, train/three: 0.8040, eval/one: 0.9295, eval/two: 0.8161, eval/three: 0.8671 \n",
      "Epoch: 465, train/loss: 0.1807, eval/loss: 0.2210, train/r2: 0.8091, eval/r2: 0.8706, train/one: 0.9600, train/two: 0.7279, train/three: 0.7392, eval/one: 0.9297, eval/two: 0.8157, eval/three: 0.8664 \n",
      "Epoch: 470, train/loss: 0.1418, eval/loss: 0.2208, train/r2: 0.7944, eval/r2: 0.8707, train/one: 0.9740, train/two: 0.5836, train/three: 0.8256, eval/one: 0.9298, eval/two: 0.8164, eval/three: 0.8659 \n",
      "Epoch: 475, train/loss: 0.1433, eval/loss: 0.2215, train/r2: 0.8459, eval/r2: 0.8701, train/one: 0.9670, train/two: 0.7080, train/three: 0.8626, eval/one: 0.9296, eval/two: 0.8164, eval/three: 0.8644 \n",
      "Epoch: 480, train/loss: 0.1858, eval/loss: 0.2220, train/r2: 0.7734, eval/r2: 0.8698, train/one: 0.9644, train/two: 0.7250, train/three: 0.6309, eval/one: 0.9294, eval/two: 0.8165, eval/three: 0.8634 \n",
      "Epoch: 485, train/loss: 0.2297, eval/loss: 0.2225, train/r2: 0.8016, eval/r2: 0.8696, train/one: 0.9416, train/two: 0.6690, train/three: 0.7941, eval/one: 0.9293, eval/two: 0.8167, eval/three: 0.8630 \n",
      "Epoch: 490, train/loss: 0.2392, eval/loss: 0.2226, train/r2: 0.7722, eval/r2: 0.8697, train/one: 0.9417, train/two: 0.5994, train/three: 0.7755, eval/one: 0.9292, eval/two: 0.8168, eval/three: 0.8630 \n",
      "Epoch: 495, train/loss: 0.1606, eval/loss: 0.2227, train/r2: 0.8192, eval/r2: 0.8698, train/one: 0.9651, train/two: 0.6999, train/three: 0.7925, eval/one: 0.9292, eval/two: 0.8169, eval/three: 0.8632 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 63 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 63 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-110/metadata\n",
      "0.42318559905125713\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f23f56311ed4d1db5ebe733965c8a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 5.5439, eval/loss: 6.5073, train/r2: -0.4571, eval/r2: -0.9281, train/one: -0.9436, train/two: -0.6668, train/three: 0.2391, eval/one: -0.9122, eval/two: -0.3613, eval/three: -1.5108 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train/loss: 3.7763, eval/loss: 6.6999, train/r2: -0.3099, eval/r2: -0.6442, train/one: -0.2808, train/two: -0.7656, train/three: 0.1166, eval/one: -1.0301, eval/two: -0.3749, eval/three: -0.5276 \n",
      "Epoch: 10, train/loss: 1.6497, eval/loss: 66.5510, train/r2: -0.0350, eval/r2: -7.0874, train/one: 0.5020, train/two: -0.7976, train/three: 0.1904, eval/one: -20.2371, eval/two: -0.3546, eval/three: -0.6705 \n",
      "Epoch: 15, train/loss: 0.8342, eval/loss: 26.8259, train/r2: 0.2096, eval/r2: -2.8824, train/one: 0.7851, train/two: -0.3926, train/three: 0.2364, eval/one: -7.4639, eval/two: -0.0309, eval/three: -1.1523 \n",
      "Epoch: 20, train/loss: 0.6451, eval/loss: 13.0055, train/r2: 0.2528, eval/r2: -1.2668, train/one: 0.8512, train/two: -0.4296, train/three: 0.3366, eval/one: -3.0825, eval/two: -0.9016, eval/three: 0.1837 \n",
      "Epoch: 25, train/loss: 0.4861, eval/loss: 6.1759, train/r2: 0.3136, eval/r2: -0.8736, train/one: 0.9031, train/two: -0.4501, train/three: 0.4878, eval/one: -0.8596, eval/two: -1.7900, eval/three: 0.0286 \n",
      "Epoch: 30, train/loss: 0.4665, eval/loss: 3.0555, train/r2: 0.2980, eval/r2: -0.2349, train/one: 0.9118, train/two: -0.5660, train/three: 0.5482, eval/one: 0.1060, eval/two: -1.2369, eval/three: 0.4262 \n",
      "Epoch: 35, train/loss: 0.4356, eval/loss: 1.8177, train/r2: 0.1931, eval/r2: 0.2526, train/one: 0.9371, train/two: -0.8502, train/three: 0.4925, eval/one: 0.4710, eval/two: -0.3156, eval/three: 0.6025 \n",
      "Epoch: 40, train/loss: 0.4120, eval/loss: 0.9275, train/r2: 0.2968, eval/r2: 0.4557, train/one: 0.9325, train/two: -0.6172, train/three: 0.5752, eval/one: 0.7492, eval/two: 0.0280, eval/three: 0.5900 \n",
      "Epoch: 45, train/loss: 0.3439, eval/loss: 0.7008, train/r2: 0.4196, eval/r2: 0.5197, train/one: 0.9446, train/two: -0.1953, train/three: 0.5095, eval/one: 0.8170, eval/two: 0.0907, eval/three: 0.6514 \n",
      "Epoch: 50, train/loss: 0.3589, eval/loss: 0.6722, train/r2: 0.3067, eval/r2: 0.5394, train/one: 0.9534, train/two: -0.4707, train/three: 0.4374, eval/one: 0.8231, eval/two: 0.0909, eval/three: 0.7042 \n",
      "Epoch: 55, train/loss: 0.2802, eval/loss: 0.9084, train/r2: 0.4213, eval/r2: 0.5147, train/one: 0.9666, train/two: -0.3928, train/three: 0.6900, eval/one: 0.7463, eval/two: 0.0654, eval/three: 0.7326 \n",
      "Epoch: 60, train/loss: 0.3342, eval/loss: 0.7061, train/r2: 0.4335, eval/r2: 0.5267, train/one: 0.9463, train/two: -0.1822, train/three: 0.5363, eval/one: 0.8126, eval/two: 0.0544, eval/three: 0.7130 \n",
      "Epoch: 65, train/loss: 0.2986, eval/loss: 0.7226, train/r2: 0.4310, eval/r2: 0.5428, train/one: 0.9585, train/two: -0.3485, train/three: 0.6830, eval/one: 0.8056, eval/two: 0.0992, eval/three: 0.7236 \n",
      "Epoch: 70, train/loss: 0.3329, eval/loss: 0.6645, train/r2: 0.4189, eval/r2: 0.5630, train/one: 0.9471, train/two: -0.3477, train/three: 0.6572, eval/one: 0.8227, eval/two: 0.1248, eval/three: 0.7415 \n",
      "Epoch: 75, train/loss: 0.2942, eval/loss: 0.6878, train/r2: 0.4390, eval/r2: 0.5570, train/one: 0.9605, train/two: -0.2148, train/three: 0.5714, eval/one: 0.8163, eval/two: 0.1395, eval/three: 0.7151 \n",
      "Epoch: 80, train/loss: 0.3230, eval/loss: 0.7541, train/r2: 0.4950, eval/r2: 0.5310, train/one: 0.9426, train/two: -0.0299, train/three: 0.5722, eval/one: 0.7983, eval/two: 0.1339, eval/three: 0.6608 \n",
      "Epoch: 85, train/loss: 0.2368, eval/loss: 0.6119, train/r2: 0.5451, eval/r2: 0.5462, train/one: 0.9675, train/two: -0.0792, train/three: 0.7471, eval/one: 0.8420, eval/two: 0.0787, eval/three: 0.7179 \n",
      "Epoch: 90, train/loss: 0.3482, eval/loss: 0.7797, train/r2: 0.4133, eval/r2: 0.5222, train/one: 0.9424, train/two: -0.3179, train/three: 0.6153, eval/one: 0.7901, eval/two: 0.1039, eval/three: 0.6725 \n",
      "Epoch: 95, train/loss: 0.2941, eval/loss: 0.7070, train/r2: 0.4778, eval/r2: 0.5412, train/one: 0.9555, train/two: -0.1181, train/three: 0.5959, eval/one: 0.8114, eval/two: 0.1046, eval/three: 0.7075 \n",
      "Epoch: 100, train/loss: 0.2882, eval/loss: 0.6605, train/r2: 0.4988, eval/r2: 0.5528, train/one: 0.9536, train/two: -0.1876, train/three: 0.7303, eval/one: 0.8254, eval/two: 0.1118, eval/three: 0.7213 \n",
      "Epoch: 105, train/loss: 0.2499, eval/loss: 0.6479, train/r2: 0.5542, eval/r2: 0.5792, train/one: 0.9609, train/two: -0.0796, train/three: 0.7812, eval/one: 0.8264, eval/two: 0.1538, eval/three: 0.7575 \n",
      "Epoch: 110, train/loss: 0.2838, eval/loss: 0.7605, train/r2: 0.5568, eval/r2: 0.5660, train/one: 0.9498, train/two: 0.1158, train/three: 0.6049, eval/one: 0.7911, eval/two: 0.1724, eval/three: 0.7346 \n",
      "Epoch: 115, train/loss: 0.3354, eval/loss: 0.8528, train/r2: 0.4249, eval/r2: 0.5537, train/one: 0.9465, train/two: -0.2367, train/three: 0.5649, eval/one: 0.7621, eval/two: 0.1786, eval/three: 0.7205 \n",
      "Epoch: 120, train/loss: 0.2944, eval/loss: 0.6839, train/r2: 0.4883, eval/r2: 0.5884, train/one: 0.9533, train/two: -0.1527, train/three: 0.6643, eval/one: 0.8152, eval/two: 0.2338, eval/three: 0.7162 \n",
      "Epoch: 125, train/loss: 0.3168, eval/loss: 0.8007, train/r2: 0.4507, eval/r2: 0.5398, train/one: 0.9503, train/two: -0.1780, train/three: 0.5799, eval/one: 0.7842, eval/two: 0.2291, eval/three: 0.6060 \n",
      "Epoch: 130, train/loss: 0.2389, eval/loss: 0.9747, train/r2: 0.5640, eval/r2: 0.5313, train/one: 0.9653, train/two: 0.0527, train/three: 0.6740, eval/one: 0.7297, eval/two: 0.3208, eval/three: 0.5435 \n",
      "Epoch: 135, train/loss: 0.2813, eval/loss: 0.7085, train/r2: 0.5496, eval/r2: 0.6378, train/one: 0.9517, train/two: 0.0974, train/three: 0.5997, eval/one: 0.8028, eval/two: 0.3742, eval/three: 0.7363 \n",
      "Epoch: 140, train/loss: 0.2731, eval/loss: 0.7398, train/r2: 0.6384, eval/r2: 0.6591, train/one: 0.9420, train/two: 0.2131, train/three: 0.7601, eval/one: 0.7905, eval/two: 0.4392, eval/three: 0.7476 \n",
      "Epoch: 145, train/loss: 0.2633, eval/loss: 0.5558, train/r2: 0.5956, eval/r2: 0.6783, train/one: 0.9530, train/two: 0.2340, train/three: 0.5999, eval/one: 0.8508, eval/two: 0.4712, eval/three: 0.7129 \n",
      "Epoch: 150, train/loss: 0.2397, eval/loss: 0.6252, train/r2: 0.6334, eval/r2: 0.6877, train/one: 0.9552, train/two: 0.1615, train/three: 0.7835, eval/one: 0.8261, eval/two: 0.4903, eval/three: 0.7466 \n",
      "Epoch: 155, train/loss: 0.2158, eval/loss: 0.6414, train/r2: 0.7001, eval/r2: 0.6581, train/one: 0.9577, train/two: 0.4761, train/three: 0.6665, eval/one: 0.8259, eval/two: 0.4864, eval/three: 0.6620 \n",
      "Epoch: 160, train/loss: 0.2908, eval/loss: 0.6384, train/r2: 0.5846, eval/r2: 0.6446, train/one: 0.9446, train/two: 0.2760, train/three: 0.5332, eval/one: 0.8352, eval/two: 0.6511, eval/three: 0.4475 \n",
      "Epoch: 165, train/loss: 0.1965, eval/loss: 1.1621, train/r2: 0.7318, eval/r2: 0.6554, train/one: 0.9611, train/two: 0.5553, train/three: 0.6791, eval/one: 0.6529, eval/two: 0.5966, eval/three: 0.7166 \n",
      "Epoch: 170, train/loss: 0.2209, eval/loss: 0.5919, train/r2: 0.7179, eval/r2: 0.7043, train/one: 0.9529, train/two: 0.4795, train/three: 0.7213, eval/one: 0.8395, eval/two: 0.6333, eval/three: 0.6402 \n",
      "Epoch: 175, train/loss: 0.2052, eval/loss: 0.6904, train/r2: 0.7519, eval/r2: 0.3137, train/one: 0.9548, train/two: 0.5791, train/three: 0.7219, eval/one: 0.8379, eval/two: -0.4841, eval/three: 0.5873 \n",
      "Epoch: 180, train/loss: 0.1786, eval/loss: 0.6458, train/r2: 0.7675, eval/r2: 0.5464, train/one: 0.9631, train/two: 0.6122, train/three: 0.7272, eval/one: 0.8337, eval/two: 0.1714, eval/three: 0.6343 \n",
      "Epoch: 185, train/loss: 0.1878, eval/loss: 0.6640, train/r2: 0.7502, eval/r2: 0.6832, train/one: 0.9621, train/two: 0.6066, train/three: 0.6820, eval/one: 0.8172, eval/two: 0.5888, eval/three: 0.6436 \n",
      "Epoch: 190, train/loss: 0.1853, eval/loss: 0.6056, train/r2: 0.7589, eval/r2: 0.7503, train/one: 0.9623, train/two: 0.6554, train/three: 0.6591, eval/one: 0.8331, eval/two: 0.8234, eval/three: 0.5943 \n",
      "Epoch: 195, train/loss: 0.2624, eval/loss: 0.6614, train/r2: 0.7800, eval/r2: 0.7372, train/one: 0.9288, train/two: 0.6381, train/three: 0.7730, eval/one: 0.8109, eval/two: 0.6651, eval/three: 0.7356 \n",
      "Epoch: 200, train/loss: 0.1792, eval/loss: 0.7092, train/r2: 0.7998, eval/r2: 0.3205, train/one: 0.9583, train/two: 0.6669, train/three: 0.7742, eval/one: 0.8249, eval/two: -0.6360, eval/three: 0.7726 \n",
      "Epoch: 205, train/loss: 0.1810, eval/loss: 0.6234, train/r2: 0.7356, eval/r2: 0.7732, train/one: 0.9658, train/two: 0.4977, train/three: 0.7432, eval/one: 0.8210, eval/two: 0.7705, eval/three: 0.7281 \n",
      "Epoch: 210, train/loss: 0.1882, eval/loss: 0.4218, train/r2: 0.8122, eval/r2: 0.8058, train/one: 0.9530, train/two: 0.6803, train/three: 0.8033, eval/one: 0.8829, eval/two: 0.7482, eval/three: 0.7863 \n",
      "Epoch: 215, train/loss: 0.1949, eval/loss: 0.6650, train/r2: 0.7673, eval/r2: 0.7240, train/one: 0.9573, train/two: 0.6558, train/three: 0.6888, eval/one: 0.8083, eval/two: 0.5575, eval/three: 0.8062 \n",
      "Epoch: 220, train/loss: 0.1938, eval/loss: 0.7427, train/r2: 0.7898, eval/r2: 0.7137, train/one: 0.9548, train/two: 0.7161, train/three: 0.6986, eval/one: 0.7846, eval/two: 0.5830, eval/three: 0.7733 \n",
      "Epoch: 225, train/loss: 0.2038, eval/loss: 0.6467, train/r2: 0.7890, eval/r2: 0.6487, train/one: 0.9519, train/two: 0.7838, train/three: 0.6314, eval/one: 0.8225, eval/two: 0.3946, eval/three: 0.7290 \n",
      "Epoch: 230, train/loss: 0.1826, eval/loss: 0.6003, train/r2: 0.7835, eval/r2: 0.6736, train/one: 0.9596, train/two: 0.6669, train/three: 0.7240, eval/one: 0.8369, eval/two: 0.4814, eval/three: 0.7026 \n",
      "Epoch: 235, train/loss: 0.2028, eval/loss: 0.5923, train/r2: 0.8039, eval/r2: 0.7288, train/one: 0.9491, train/two: 0.7165, train/three: 0.7462, eval/one: 0.8350, eval/two: 0.6382, eval/three: 0.7132 \n",
      "Epoch: 240, train/loss: 0.2005, eval/loss: 0.6015, train/r2: 0.8154, eval/r2: 0.7071, train/one: 0.9480, train/two: 0.7104, train/three: 0.7876, eval/one: 0.8366, eval/two: 0.6580, eval/three: 0.6266 \n",
      "Epoch: 245, train/loss: 0.2178, eval/loss: 0.5674, train/r2: 0.7786, eval/r2: 0.7545, train/one: 0.9467, train/two: 0.6636, train/three: 0.7255, eval/one: 0.8443, eval/two: 0.7927, eval/three: 0.6265 \n",
      "Epoch: 250, train/loss: 0.1665, eval/loss: 0.6018, train/r2: 0.8120, eval/r2: 0.7764, train/one: 0.9614, train/two: 0.6760, train/three: 0.7987, eval/one: 0.8277, eval/two: 0.7692, eval/three: 0.7325 \n",
      "Epoch: 255, train/loss: 0.1721, eval/loss: 0.6382, train/r2: 0.8427, eval/r2: 0.7792, train/one: 0.9557, train/two: 0.7912, train/three: 0.7811, eval/one: 0.8163, eval/two: 0.8110, eval/three: 0.7102 \n",
      "Epoch: 260, train/loss: 0.1837, eval/loss: 0.6434, train/r2: 0.8252, eval/r2: 0.7669, train/one: 0.9528, train/two: 0.6975, train/three: 0.8253, eval/one: 0.8181, eval/two: 0.8486, eval/three: 0.6339 \n",
      "Epoch: 265, train/loss: 0.1888, eval/loss: 0.5971, train/r2: 0.8141, eval/r2: 0.7154, train/one: 0.9527, train/two: 0.6984, train/three: 0.7911, eval/one: 0.8377, eval/two: 0.6895, eval/three: 0.6189 \n",
      "Epoch: 270, train/loss: 0.1682, eval/loss: 0.5432, train/r2: 0.7865, eval/r2: 0.7196, train/one: 0.9646, train/two: 0.6567, train/three: 0.7384, eval/one: 0.8503, eval/two: 0.5494, eval/three: 0.7592 \n",
      "Epoch: 275, train/loss: 0.1972, eval/loss: 0.4983, train/r2: 0.7703, eval/r2: 0.8154, train/one: 0.9553, train/two: 0.6079, train/three: 0.7478, eval/one: 0.8573, eval/two: 0.8087, eval/three: 0.7802 \n",
      "Epoch: 280, train/loss: 0.2243, eval/loss: 0.5412, train/r2: 0.7851, eval/r2: 0.7962, train/one: 0.9424, train/two: 0.6128, train/three: 0.8000, eval/one: 0.8474, eval/two: 0.8428, eval/three: 0.6984 \n",
      "Epoch: 285, train/loss: 0.2006, eval/loss: 0.6103, train/r2: 0.7966, eval/r2: 0.7827, train/one: 0.9512, train/two: 0.7248, train/three: 0.7138, eval/one: 0.8256, eval/two: 0.8244, eval/three: 0.6982 \n",
      "Epoch: 290, train/loss: 0.2085, eval/loss: 0.5735, train/r2: 0.7633, eval/r2: 0.7913, train/one: 0.9522, train/two: 0.6204, train/three: 0.7175, eval/one: 0.8344, eval/two: 0.7621, eval/three: 0.7775 \n",
      "Epoch: 295, train/loss: 0.1912, eval/loss: 0.6712, train/r2: 0.7923, eval/r2: 0.7510, train/one: 0.9554, train/two: 0.7157, train/three: 0.7059, eval/one: 0.8058, eval/two: 0.6847, eval/three: 0.7625 \n",
      "Epoch: 300, train/loss: 0.1732, eval/loss: 0.6148, train/r2: 0.8057, eval/r2: 0.7316, train/one: 0.9593, train/two: 0.6328, train/three: 0.8252, eval/one: 0.8268, eval/two: 0.6350, eval/three: 0.7329 \n",
      "Epoch: 305, train/loss: 0.1679, eval/loss: 0.6014, train/r2: 0.7953, eval/r2: 0.7374, train/one: 0.9628, train/two: 0.6188, train/three: 0.8045, eval/one: 0.8351, eval/two: 0.7715, eval/three: 0.6057 \n",
      "Epoch: 310, train/loss: 0.2087, eval/loss: 0.5585, train/r2: 0.8171, eval/r2: 0.7514, train/one: 0.9445, train/two: 0.7076, train/three: 0.7994, eval/one: 0.8457, eval/two: 0.7311, eval/three: 0.6773 \n",
      "Epoch: 315, train/loss: 0.1757, eval/loss: 0.6208, train/r2: 0.8115, eval/r2: 0.6655, train/one: 0.9587, train/two: 0.7434, train/three: 0.7324, eval/one: 0.8303, eval/two: 0.4521, eval/three: 0.7140 \n",
      "Epoch: 320, train/loss: 0.1724, eval/loss: 0.5994, train/r2: 0.8252, eval/r2: 0.7341, train/one: 0.9578, train/two: 0.7402, train/three: 0.7776, eval/one: 0.8331, eval/two: 0.6793, eval/three: 0.6900 \n",
      "Epoch: 325, train/loss: 0.1854, eval/loss: 0.5638, train/r2: 0.7785, eval/r2: 0.7709, train/one: 0.9594, train/two: 0.6708, train/three: 0.7052, eval/one: 0.8411, eval/two: 0.7537, eval/three: 0.7180 \n",
      "Epoch: 330, train/loss: 0.1698, eval/loss: 0.6365, train/r2: 0.7938, eval/r2: 0.7471, train/one: 0.9631, train/two: 0.6829, train/three: 0.7355, eval/one: 0.8174, eval/two: 0.6602, eval/three: 0.7637 \n",
      "Epoch: 335, train/loss: 0.1919, eval/loss: 0.5413, train/r2: 0.7920, eval/r2: 0.7992, train/one: 0.9542, train/two: 0.6334, train/three: 0.7884, eval/one: 0.8448, eval/two: 0.7868, eval/three: 0.7660 \n",
      "Epoch: 340, train/loss: 0.1484, eval/loss: 0.6239, train/r2: 0.7963, eval/r2: 0.7535, train/one: 0.9706, train/two: 0.6471, train/three: 0.7714, eval/one: 0.8216, eval/two: 0.6893, eval/three: 0.7496 \n",
      "Epoch: 345, train/loss: 0.1263, eval/loss: 0.6307, train/r2: 0.8476, eval/r2: 0.7505, train/one: 0.9726, train/two: 0.7825, train/three: 0.7877, eval/one: 0.8211, eval/two: 0.7274, eval/three: 0.7031 \n",
      "Epoch: 350, train/loss: 0.1669, eval/loss: 0.5808, train/r2: 0.8107, eval/r2: 0.7896, train/one: 0.9618, train/two: 0.7050, train/three: 0.7654, eval/one: 0.8345, eval/two: 0.8277, eval/three: 0.7064 \n",
      "Epoch: 355, train/loss: 0.1921, eval/loss: 0.5721, train/r2: 0.8100, eval/r2: 0.7879, train/one: 0.9521, train/two: 0.7050, train/three: 0.7731, eval/one: 0.8369, eval/two: 0.8027, eval/three: 0.7239 \n",
      "Epoch: 360, train/loss: 0.1965, eval/loss: 0.5533, train/r2: 0.7786, eval/r2: 0.7887, train/one: 0.9557, train/two: 0.7280, train/three: 0.6521, eval/one: 0.8432, eval/two: 0.8024, eval/three: 0.7205 \n",
      "Epoch: 365, train/loss: 0.1307, eval/loss: 0.5549, train/r2: 0.8494, eval/r2: 0.7737, train/one: 0.9706, train/two: 0.7818, train/three: 0.7957, eval/one: 0.8440, eval/two: 0.7654, eval/three: 0.7118 \n",
      "Epoch: 370, train/loss: 0.2486, eval/loss: 0.5644, train/r2: 0.8155, eval/r2: 0.7627, train/one: 0.9297, train/two: 0.7387, train/three: 0.7782, eval/one: 0.8405, eval/two: 0.6992, eval/three: 0.7484 \n",
      "Epoch: 375, train/loss: 0.2047, eval/loss: 0.5816, train/r2: 0.7474, eval/r2: 0.7281, train/one: 0.9560, train/two: 0.6015, train/three: 0.6847, eval/one: 0.8369, eval/two: 0.5841, eval/three: 0.7634 \n",
      "Epoch: 380, train/loss: 0.1677, eval/loss: 0.5825, train/r2: 0.8039, eval/r2: 0.7540, train/one: 0.9623, train/two: 0.6827, train/three: 0.7668, eval/one: 0.8353, eval/two: 0.6806, eval/three: 0.7460 \n",
      "Epoch: 385, train/loss: 0.1714, eval/loss: 0.6128, train/r2: 0.8171, eval/r2: 0.7886, train/one: 0.9593, train/two: 0.7241, train/three: 0.7681, eval/one: 0.8240, eval/two: 0.8339, eval/three: 0.7079 \n",
      "Epoch: 390, train/loss: 0.1575, eval/loss: 0.5693, train/r2: 0.8122, eval/r2: 0.7962, train/one: 0.9659, train/two: 0.7643, train/three: 0.7065, eval/one: 0.8375, eval/two: 0.8355, eval/three: 0.7157 \n",
      "Epoch: 395, train/loss: 0.1412, eval/loss: 0.5156, train/r2: 0.8266, eval/r2: 0.8034, train/one: 0.9694, train/two: 0.7140, train/three: 0.7964, eval/one: 0.8541, eval/two: 0.8240, eval/three: 0.7322 \n",
      "Epoch: 400, train/loss: 0.1942, eval/loss: 0.5163, train/r2: 0.8196, eval/r2: 0.8034, train/one: 0.9494, train/two: 0.6802, train/three: 0.8291, eval/one: 0.8536, eval/two: 0.8158, eval/three: 0.7409 \n",
      "Epoch: 405, train/loss: 0.1456, eval/loss: 0.5268, train/r2: 0.8109, eval/r2: 0.7940, train/one: 0.9692, train/two: 0.6338, train/three: 0.8298, eval/one: 0.8510, eval/two: 0.7958, eval/three: 0.7353 \n",
      "Epoch: 410, train/loss: 0.1955, eval/loss: 0.5467, train/r2: 0.8244, eval/r2: 0.7873, train/one: 0.9493, train/two: 0.7680, train/three: 0.7559, eval/one: 0.8453, eval/two: 0.7922, eval/three: 0.7245 \n",
      "Epoch: 415, train/loss: 0.1921, eval/loss: 0.5554, train/r2: 0.7944, eval/r2: 0.7868, train/one: 0.9541, train/two: 0.6657, train/three: 0.7634, eval/one: 0.8427, eval/two: 0.8005, eval/three: 0.7172 \n",
      "Epoch: 420, train/loss: 0.1864, eval/loss: 0.5669, train/r2: 0.8162, eval/r2: 0.7848, train/one: 0.9536, train/two: 0.7295, train/three: 0.7655, eval/one: 0.8395, eval/two: 0.8085, eval/three: 0.7064 \n",
      "Epoch: 425, train/loss: 0.1563, eval/loss: 0.5618, train/r2: 0.7916, eval/r2: 0.7885, train/one: 0.9685, train/two: 0.6677, train/three: 0.7386, eval/one: 0.8408, eval/two: 0.8170, eval/three: 0.7077 \n",
      "Epoch: 430, train/loss: 0.1611, eval/loss: 0.5443, train/r2: 0.8282, eval/r2: 0.7945, train/one: 0.9612, train/two: 0.7026, train/three: 0.8208, eval/one: 0.8459, eval/two: 0.8212, eval/three: 0.7163 \n",
      "Epoch: 435, train/loss: 0.1378, eval/loss: 0.5414, train/r2: 0.8192, eval/r2: 0.7952, train/one: 0.9720, train/two: 0.7278, train/three: 0.7579, eval/one: 0.8465, eval/two: 0.8157, eval/three: 0.7235 \n",
      "Epoch: 440, train/loss: 0.1334, eval/loss: 0.5408, train/r2: 0.8257, eval/r2: 0.7924, train/one: 0.9725, train/two: 0.7126, train/three: 0.7918, eval/one: 0.8468, eval/two: 0.8046, eval/three: 0.7258 \n",
      "Epoch: 445, train/loss: 0.1498, eval/loss: 0.5361, train/r2: 0.8310, eval/r2: 0.7913, train/one: 0.9653, train/two: 0.7116, train/three: 0.8160, eval/one: 0.8484, eval/two: 0.7983, eval/three: 0.7273 \n",
      "Epoch: 450, train/loss: 0.1703, eval/loss: 0.5362, train/r2: 0.8383, eval/r2: 0.7928, train/one: 0.9563, train/two: 0.7300, train/three: 0.8286, eval/one: 0.8483, eval/two: 0.8048, eval/three: 0.7253 \n",
      "Epoch: 455, train/loss: 0.1465, eval/loss: 0.5353, train/r2: 0.8026, eval/r2: 0.7947, train/one: 0.9709, train/two: 0.6925, train/three: 0.7445, eval/one: 0.8484, eval/two: 0.8090, eval/three: 0.7268 \n",
      "Epoch: 460, train/loss: 0.1348, eval/loss: 0.5338, train/r2: 0.8179, eval/r2: 0.7954, train/one: 0.9733, train/two: 0.7163, train/three: 0.7640, eval/one: 0.8488, eval/two: 0.8076, eval/three: 0.7297 \n",
      "Epoch: 465, train/loss: 0.1212, eval/loss: 0.5365, train/r2: 0.8628, eval/r2: 0.7956, train/one: 0.9722, train/two: 0.7880, train/three: 0.8280, eval/one: 0.8478, eval/two: 0.8075, eval/three: 0.7315 \n",
      "Epoch: 470, train/loss: 0.1801, eval/loss: 0.5394, train/r2: 0.7982, eval/r2: 0.7961, train/one: 0.9583, train/two: 0.6742, train/three: 0.7621, eval/one: 0.8468, eval/two: 0.8088, eval/three: 0.7328 \n",
      "Epoch: 475, train/loss: 0.1838, eval/loss: 0.5403, train/r2: 0.8006, eval/r2: 0.7970, train/one: 0.9564, train/two: 0.6687, train/three: 0.7768, eval/one: 0.8464, eval/two: 0.8103, eval/three: 0.7344 \n",
      "Epoch: 480, train/loss: 0.1639, eval/loss: 0.5398, train/r2: 0.7962, eval/r2: 0.7975, train/one: 0.9648, train/two: 0.6703, train/three: 0.7535, eval/one: 0.8465, eval/two: 0.8112, eval/three: 0.7348 \n",
      "Epoch: 485, train/loss: 0.1390, eval/loss: 0.5393, train/r2: 0.8335, eval/r2: 0.7973, train/one: 0.9701, train/two: 0.7919, train/three: 0.7387, eval/one: 0.8466, eval/two: 0.8105, eval/three: 0.7348 \n",
      "Epoch: 490, train/loss: 0.1816, eval/loss: 0.5390, train/r2: 0.8379, eval/r2: 0.7971, train/one: 0.9524, train/two: 0.7627, train/three: 0.7986, eval/one: 0.8468, eval/two: 0.8098, eval/three: 0.7348 \n",
      "Epoch: 495, train/loss: 0.1604, eval/loss: 0.5389, train/r2: 0.7765, eval/r2: 0.7969, train/one: 0.9693, train/two: 0.6658, train/three: 0.6943, eval/one: 0.8468, eval/two: 0.8092, eval/three: 0.7348 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 67 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 67 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-111/metadata\n",
      "0.42764169109729666\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87434b90cc414ca481df5a3a36a7a3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 4.3617, eval/loss: 33.9555, train/r2: -0.4846, eval/r2: -3.0093, train/one: -0.5491, train/two: -0.9163, train/three: 0.0115, eval/one: -8.8157, eval/two: -0.4129, eval/three: 0.2006 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train/loss: 3.4850, eval/loss: 26.1726, train/r2: -0.4675, eval/r2: -2.4897, train/one: -0.2032, train/two: -1.1990, train/three: -0.0001, eval/one: -6.5172, eval/two: -0.5201, eval/three: -0.4319 \n",
      "Epoch: 10, train/loss: 1.4214, eval/loss: 1.7239, train/r2: -0.0968, eval/r2: -0.9567, train/one: 0.5722, train/two: -0.9909, train/three: 0.1282, eval/one: 0.7142, eval/two: -0.3615, eval/three: -3.2228 \n",
      "Epoch: 15, train/loss: 0.7338, eval/loss: 2.3894, train/r2: 0.0955, eval/r2: 0.1001, train/one: 0.8161, train/two: -1.0483, train/three: 0.5187, eval/one: 0.3853, eval/two: -0.1745, eval/three: 0.0895 \n",
      "Epoch: 20, train/loss: 0.4962, eval/loss: 3.1222, train/r2: 0.3646, eval/r2: 0.2600, train/one: 0.8834, train/two: -0.2115, train/three: 0.4221, eval/one: 0.1436, eval/two: -0.0516, eval/three: 0.6880 \n",
      "Epoch: 25, train/loss: 0.5396, eval/loss: 3.1193, train/r2: 0.0893, eval/r2: 0.2263, train/one: 0.8966, train/two: -1.0077, train/three: 0.3790, eval/one: 0.1486, eval/two: -0.1110, eval/three: 0.6414 \n",
      "Epoch: 30, train/loss: 0.3724, eval/loss: 1.5389, train/r2: 0.4004, eval/r2: 0.2165, train/one: 0.9256, train/two: -0.2904, train/three: 0.5660, eval/one: 0.6289, eval/two: 0.0186, eval/three: 0.0020 \n",
      "Epoch: 35, train/loss: 0.4047, eval/loss: 0.6080, train/r2: 0.2680, eval/r2: 0.3621, train/one: 0.9301, train/two: -0.5403, train/three: 0.4142, eval/one: 0.8939, eval/two: -0.0015, eval/three: 0.1938 \n",
      "Epoch: 40, train/loss: 0.3564, eval/loss: 0.6162, train/r2: 0.3447, eval/r2: 0.3011, train/one: 0.9382, train/two: -0.4587, train/three: 0.5546, eval/one: 0.8988, eval/two: -0.0427, eval/three: 0.0472 \n",
      "Epoch: 45, train/loss: 0.3544, eval/loss: 0.7211, train/r2: 0.3687, eval/r2: 0.2398, train/one: 0.9354, train/two: -0.4352, train/three: 0.6059, eval/one: 0.8742, eval/two: 0.0122, eval/three: -0.1669 \n",
      "Epoch: 50, train/loss: 0.3517, eval/loss: 0.7901, train/r2: 0.3982, eval/r2: 0.3691, train/one: 0.9313, train/two: -0.4365, train/three: 0.6998, eval/one: 0.8377, eval/two: 0.0457, eval/three: 0.2239 \n",
      "Epoch: 55, train/loss: 0.3139, eval/loss: 0.8382, train/r2: 0.4363, eval/r2: 0.4334, train/one: 0.9440, train/two: -0.2508, train/three: 0.6157, eval/one: 0.8154, eval/two: 0.0221, eval/three: 0.4628 \n",
      "Epoch: 60, train/loss: 0.3546, eval/loss: 0.7322, train/r2: 0.2175, eval/r2: 0.5456, train/one: 0.9535, train/two: -0.8106, train/three: 0.5096, eval/one: 0.8342, eval/two: -0.0319, eval/three: 0.8343 \n",
      "Epoch: 65, train/loss: 0.2530, eval/loss: 0.8336, train/r2: 0.4561, eval/r2: 0.4812, train/one: 0.9664, train/two: -0.1952, train/three: 0.5971, eval/one: 0.8113, eval/two: -0.0834, eval/three: 0.7158 \n",
      "Epoch: 70, train/loss: 0.2222, eval/loss: 0.8006, train/r2: 0.5419, eval/r2: 0.5251, train/one: 0.9669, train/two: -0.0575, train/three: 0.7162, eval/one: 0.8159, eval/two: 0.0020, eval/three: 0.7574 \n",
      "Epoch: 75, train/loss: 0.2813, eval/loss: 0.6882, train/r2: 0.4534, eval/r2: 0.5765, train/one: 0.9539, train/two: -0.2667, train/three: 0.6731, eval/one: 0.8437, eval/two: 0.0355, eval/three: 0.8503 \n",
      "Epoch: 80, train/loss: 0.2783, eval/loss: 0.6528, train/r2: 0.4497, eval/r2: 0.5505, train/one: 0.9550, train/two: -0.2985, train/three: 0.6925, eval/one: 0.8576, eval/two: 0.0243, eval/three: 0.7697 \n",
      "Epoch: 85, train/loss: 0.3103, eval/loss: 0.8527, train/r2: 0.3838, eval/r2: 0.5217, train/one: 0.9516, train/two: -0.3886, train/three: 0.5885, eval/one: 0.8006, eval/two: -0.0402, eval/three: 0.8047 \n",
      "Epoch: 90, train/loss: 0.2576, eval/loss: 0.6679, train/r2: 0.5081, eval/r2: 0.5264, train/one: 0.9564, train/two: -0.1554, train/three: 0.7232, eval/one: 0.8559, eval/two: 0.0181, eval/three: 0.7053 \n",
      "Epoch: 95, train/loss: 0.2875, eval/loss: 0.8580, train/r2: 0.4742, eval/r2: 0.5430, train/one: 0.9500, train/two: -0.1592, train/three: 0.6319, eval/one: 0.7964, eval/two: -0.0204, eval/three: 0.8529 \n",
      "Epoch: 100, train/loss: 0.2905, eval/loss: 0.6207, train/r2: 0.4099, eval/r2: 0.5863, train/one: 0.9552, train/two: -0.3836, train/three: 0.6581, eval/one: 0.8631, eval/two: 0.0277, eval/three: 0.8681 \n",
      "Epoch: 105, train/loss: 0.2539, eval/loss: 0.7490, train/r2: 0.5090, eval/r2: 0.5564, train/one: 0.9580, train/two: -0.1401, train/three: 0.7092, eval/one: 0.8277, eval/two: 0.0597, eval/three: 0.7817 \n",
      "Epoch: 110, train/loss: 0.2288, eval/loss: 0.6753, train/r2: 0.5851, eval/r2: 0.5416, train/one: 0.9588, train/two: 0.0420, train/three: 0.7545, eval/one: 0.8517, eval/two: 0.1380, eval/three: 0.6353 \n",
      "Epoch: 115, train/loss: 0.2551, eval/loss: 0.6629, train/r2: 0.5887, eval/r2: 0.5922, train/one: 0.9480, train/two: 0.0592, train/three: 0.7590, eval/one: 0.8493, eval/two: 0.1684, eval/three: 0.7590 \n",
      "Epoch: 120, train/loss: 0.2684, eval/loss: 0.8102, train/r2: 0.4936, eval/r2: 0.4918, train/one: 0.9554, train/two: -0.1102, train/three: 0.6355, eval/one: 0.8167, eval/two: 0.1289, eval/three: 0.5299 \n",
      "Epoch: 125, train/loss: 0.2967, eval/loss: 0.6297, train/r2: 0.5274, eval/r2: 0.5961, train/one: 0.9409, train/two: 0.0233, train/three: 0.6179, eval/one: 0.8587, eval/two: 0.2678, eval/three: 0.6619 \n",
      "Epoch: 130, train/loss: 0.2442, eval/loss: 0.7927, train/r2: 0.5759, eval/r2: 0.6315, train/one: 0.9555, train/two: 0.1013, train/three: 0.6709, eval/one: 0.8050, eval/two: 0.3047, eval/three: 0.7850 \n",
      "Epoch: 135, train/loss: 0.1758, eval/loss: 0.7004, train/r2: 0.7124, eval/r2: 0.6755, train/one: 0.9686, train/two: 0.5280, train/three: 0.6407, eval/one: 0.8276, eval/two: 0.3939, eval/three: 0.8049 \n",
      "Epoch: 140, train/loss: 0.2199, eval/loss: 0.7166, train/r2: 0.6934, eval/r2: 0.7082, train/one: 0.9525, train/two: 0.4554, train/three: 0.6724, eval/one: 0.8185, eval/two: 0.4877, eval/three: 0.8184 \n",
      "Epoch: 145, train/loss: 0.1848, eval/loss: 0.6048, train/r2: 0.7359, eval/r2: 0.7386, train/one: 0.9597, train/two: 0.4734, train/three: 0.7746, eval/one: 0.8487, eval/two: 0.5411, eval/three: 0.8260 \n",
      "Epoch: 150, train/loss: 0.2166, eval/loss: 0.7875, train/r2: 0.6898, eval/r2: 0.6924, train/one: 0.9537, train/two: 0.4187, train/three: 0.6969, eval/one: 0.7989, eval/two: 0.5075, eval/three: 0.7709 \n",
      "Epoch: 155, train/loss: 0.2068, eval/loss: 0.8410, train/r2: 0.6678, eval/r2: 0.5734, train/one: 0.9613, train/two: 0.4112, train/three: 0.6308, eval/one: 0.7974, eval/two: 0.2256, eval/three: 0.6972 \n",
      "Epoch: 160, train/loss: 0.1812, eval/loss: 1.3202, train/r2: 0.7229, eval/r2: 0.4515, train/one: 0.9636, train/two: 0.4806, train/three: 0.7246, eval/one: 0.6671, eval/two: 0.0203, eval/three: 0.6672 \n",
      "Epoch: 165, train/loss: 0.1785, eval/loss: 0.9976, train/r2: 0.7288, eval/r2: 0.4939, train/one: 0.9649, train/two: 0.5417, train/three: 0.6797, eval/one: 0.7596, eval/two: 0.1303, eval/three: 0.5916 \n",
      "Epoch: 170, train/loss: 0.1802, eval/loss: 0.7472, train/r2: 0.7091, eval/r2: 0.6128, train/one: 0.9651, train/two: 0.4202, train/three: 0.7420, eval/one: 0.8205, eval/two: 0.5952, eval/three: 0.4228 \n",
      "Epoch: 175, train/loss: 0.1745, eval/loss: 0.6444, train/r2: 0.7502, eval/r2: 0.6923, train/one: 0.9627, train/two: 0.5355, train/three: 0.7524, eval/one: 0.8422, eval/two: 0.5715, eval/three: 0.6631 \n",
      "Epoch: 180, train/loss: 0.1766, eval/loss: 0.5887, train/r2: 0.7707, eval/r2: 0.7421, train/one: 0.9605, train/two: 0.6349, train/three: 0.7166, eval/one: 0.8531, eval/two: 0.5977, eval/three: 0.7755 \n",
      "Epoch: 185, train/loss: 0.1832, eval/loss: 0.6540, train/r2: 0.7746, eval/r2: 0.6744, train/one: 0.9572, train/two: 0.6377, train/three: 0.7290, eval/one: 0.8417, eval/two: 0.4287, eval/three: 0.7529 \n",
      "Epoch: 190, train/loss: 0.1773, eval/loss: 0.5570, train/r2: 0.7356, eval/r2: 0.7015, train/one: 0.9630, train/two: 0.4849, train/three: 0.7590, eval/one: 0.8676, eval/two: 0.6034, eval/three: 0.6335 \n",
      "Epoch: 195, train/loss: 0.1684, eval/loss: 0.6510, train/r2: 0.8277, eval/r2: 0.7042, train/one: 0.9565, train/two: 0.7583, train/three: 0.7684, eval/one: 0.8388, eval/two: 0.5742, eval/three: 0.6995 \n",
      "Epoch: 200, train/loss: 0.2282, eval/loss: 0.7063, train/r2: 0.7737, eval/r2: 0.7652, train/one: 0.9370, train/two: 0.5404, train/three: 0.8437, eval/one: 0.8145, eval/two: 0.6685, eval/three: 0.8125 \n",
      "Epoch: 205, train/loss: 0.2210, eval/loss: 0.5348, train/r2: 0.6813, eval/r2: 0.7858, train/one: 0.9530, train/two: 0.4022, train/three: 0.6888, eval/one: 0.8640, eval/two: 0.7109, eval/three: 0.7824 \n",
      "Epoch: 210, train/loss: 0.1609, eval/loss: 0.6167, train/r2: 0.7618, eval/r2: 0.7484, train/one: 0.9682, train/two: 0.6305, train/three: 0.6868, eval/one: 0.8437, eval/two: 0.6761, eval/three: 0.7256 \n",
      "Epoch: 215, train/loss: 0.1785, eval/loss: 0.6287, train/r2: 0.7678, eval/r2: 0.6894, train/one: 0.9583, train/two: 0.5470, train/three: 0.7981, eval/one: 0.8474, eval/two: 0.4987, eval/three: 0.7221 \n",
      "Epoch: 220, train/loss: 0.2119, eval/loss: 0.5615, train/r2: 0.7432, eval/r2: 0.7988, train/one: 0.9491, train/two: 0.5495, train/three: 0.7311, eval/one: 0.8543, eval/two: 0.7539, eval/three: 0.7884 \n",
      "Epoch: 225, train/loss: 0.1357, eval/loss: 0.6217, train/r2: 0.8134, eval/r2: 0.7411, train/one: 0.9699, train/two: 0.6478, train/three: 0.8225, eval/one: 0.8430, eval/two: 0.7135, eval/three: 0.6669 \n",
      "Epoch: 230, train/loss: 0.1555, eval/loss: 0.6468, train/r2: 0.7708, eval/r2: 0.7283, train/one: 0.9698, train/two: 0.6767, train/three: 0.6659, eval/one: 0.8369, eval/two: 0.7058, eval/three: 0.6422 \n",
      "Epoch: 235, train/loss: 0.1644, eval/loss: 0.6643, train/r2: 0.7903, eval/r2: 0.7149, train/one: 0.9630, train/two: 0.6811, train/three: 0.7269, eval/one: 0.8336, eval/two: 0.5020, eval/three: 0.8089 \n",
      "Epoch: 240, train/loss: 0.2127, eval/loss: 0.8029, train/r2: 0.7824, eval/r2: 0.6577, train/one: 0.9431, train/two: 0.6011, train/three: 0.8028, eval/one: 0.7986, eval/two: 0.3959, eval/three: 0.7788 \n",
      "Epoch: 245, train/loss: 0.2198, eval/loss: 0.6274, train/r2: 0.7527, eval/r2: 0.7338, train/one: 0.9455, train/two: 0.6093, train/three: 0.7031, eval/one: 0.8425, eval/two: 0.5271, eval/three: 0.8318 \n",
      "Epoch: 250, train/loss: 0.1947, eval/loss: 0.5727, train/r2: 0.7301, eval/r2: 0.7795, train/one: 0.9588, train/two: 0.5741, train/three: 0.6573, eval/one: 0.8534, eval/two: 0.6564, eval/three: 0.8286 \n",
      "Epoch: 255, train/loss: 0.1670, eval/loss: 0.6351, train/r2: 0.8068, eval/r2: 0.7796, train/one: 0.9596, train/two: 0.7072, train/three: 0.7535, eval/one: 0.8344, eval/two: 0.6862, eval/three: 0.8181 \n",
      "Epoch: 260, train/loss: 0.1681, eval/loss: 0.6386, train/r2: 0.7821, eval/r2: 0.7901, train/one: 0.9624, train/two: 0.6607, train/three: 0.7233, eval/one: 0.8320, eval/two: 0.7062, eval/three: 0.8321 \n",
      "Epoch: 265, train/loss: 0.1532, eval/loss: 0.6162, train/r2: 0.7762, eval/r2: 0.7828, train/one: 0.9671, train/two: 0.5463, train/three: 0.8152, eval/one: 0.8396, eval/two: 0.7393, eval/three: 0.7696 \n",
      "Epoch: 270, train/loss: 0.2098, eval/loss: 0.5726, train/r2: 0.7953, eval/r2: 0.8049, train/one: 0.9439, train/two: 0.6904, train/three: 0.7516, eval/one: 0.8502, eval/two: 0.7472, eval/three: 0.8172 \n",
      "Epoch: 275, train/loss: 0.2247, eval/loss: 0.5993, train/r2: 0.7807, eval/r2: 0.8045, train/one: 0.9400, train/two: 0.6686, train/three: 0.7333, eval/one: 0.8421, eval/two: 0.7510, eval/three: 0.8205 \n",
      "Epoch: 280, train/loss: 0.1914, eval/loss: 0.5821, train/r2: 0.7564, eval/r2: 0.7896, train/one: 0.9548, train/two: 0.5333, train/three: 0.7812, eval/one: 0.8492, eval/two: 0.7222, eval/three: 0.7973 \n",
      "Epoch: 285, train/loss: 0.2335, eval/loss: 0.5882, train/r2: 0.7629, eval/r2: 0.7937, train/one: 0.9389, train/two: 0.6437, train/three: 0.7060, eval/one: 0.8468, eval/two: 0.7317, eval/three: 0.8025 \n",
      "Epoch: 290, train/loss: 0.1619, eval/loss: 0.6153, train/r2: 0.7972, eval/r2: 0.7872, train/one: 0.9630, train/two: 0.6903, train/three: 0.7383, eval/one: 0.8394, eval/two: 0.7199, eval/three: 0.8024 \n",
      "Epoch: 295, train/loss: 0.1742, eval/loss: 0.5973, train/r2: 0.7829, eval/r2: 0.7741, train/one: 0.9599, train/two: 0.6612, train/three: 0.7277, eval/one: 0.8465, eval/two: 0.6892, eval/three: 0.7867 \n",
      "Epoch: 300, train/loss: 0.1528, eval/loss: 0.5900, train/r2: 0.8167, eval/r2: 0.7895, train/one: 0.9643, train/two: 0.7372, train/three: 0.7486, eval/one: 0.8468, eval/two: 0.7249, eval/three: 0.7969 \n",
      "Epoch: 305, train/loss: 0.1958, eval/loss: 0.5725, train/r2: 0.7642, eval/r2: 0.7870, train/one: 0.9524, train/two: 0.5693, train/three: 0.7708, eval/one: 0.8524, eval/two: 0.6835, eval/three: 0.8252 \n",
      "Epoch: 310, train/loss: 0.2259, eval/loss: 0.5820, train/r2: 0.8060, eval/r2: 0.7710, train/one: 0.9353, train/two: 0.6792, train/three: 0.8033, eval/one: 0.8516, eval/two: 0.6527, eval/three: 0.8086 \n",
      "Epoch: 315, train/loss: 0.1602, eval/loss: 0.6661, train/r2: 0.7670, eval/r2: 0.7834, train/one: 0.9663, train/two: 0.5675, train/three: 0.7672, eval/one: 0.8244, eval/two: 0.7279, eval/three: 0.7979 \n",
      "Epoch: 320, train/loss: 0.1862, eval/loss: 0.7186, train/r2: 0.8130, eval/r2: 0.7689, train/one: 0.9499, train/two: 0.6672, train/three: 0.8220, eval/one: 0.8102, eval/two: 0.7368, eval/three: 0.7596 \n",
      "Epoch: 325, train/loss: 0.1368, eval/loss: 0.5945, train/r2: 0.7938, eval/r2: 0.7631, train/one: 0.9747, train/two: 0.7360, train/three: 0.6707, eval/one: 0.8485, eval/two: 0.7464, eval/three: 0.6944 \n",
      "Epoch: 330, train/loss: 0.1718, eval/loss: 0.5842, train/r2: 0.7794, eval/r2: 0.7500, train/one: 0.9617, train/two: 0.6750, train/three: 0.7014, eval/one: 0.8533, eval/two: 0.7016, eval/three: 0.6950 \n",
      "Epoch: 335, train/loss: 0.1688, eval/loss: 0.6437, train/r2: 0.8196, eval/r2: 0.7467, train/one: 0.9559, train/two: 0.6736, train/three: 0.8293, eval/one: 0.8356, eval/two: 0.7416, eval/three: 0.6630 \n",
      "Epoch: 340, train/loss: 0.1757, eval/loss: 0.6566, train/r2: 0.8229, eval/r2: 0.7615, train/one: 0.9535, train/two: 0.7182, train/three: 0.7969, eval/one: 0.8299, eval/two: 0.7522, eval/three: 0.7024 \n",
      "Epoch: 345, train/loss: 0.1565, eval/loss: 0.6157, train/r2: 0.8228, eval/r2: 0.7640, train/one: 0.9611, train/two: 0.7070, train/three: 0.8002, eval/one: 0.8420, eval/two: 0.7465, eval/three: 0.7034 \n",
      "Epoch: 350, train/loss: 0.1806, eval/loss: 0.6490, train/r2: 0.7863, eval/r2: 0.7504, train/one: 0.9549, train/two: 0.5774, train/three: 0.8266, eval/one: 0.8335, eval/two: 0.7459, eval/three: 0.6717 \n",
      "Epoch: 355, train/loss: 0.1638, eval/loss: 0.6118, train/r2: 0.8119, eval/r2: 0.7702, train/one: 0.9593, train/two: 0.6721, train/three: 0.8044, eval/one: 0.8424, eval/two: 0.7484, eval/three: 0.7198 \n",
      "Epoch: 360, train/loss: 0.1595, eval/loss: 0.5561, train/r2: 0.8009, eval/r2: 0.7795, train/one: 0.9634, train/two: 0.6944, train/three: 0.7449, eval/one: 0.8582, eval/two: 0.7309, eval/three: 0.7494 \n",
      "Epoch: 365, train/loss: 0.1576, eval/loss: 0.5522, train/r2: 0.8043, eval/r2: 0.7813, train/one: 0.9637, train/two: 0.6995, train/three: 0.7495, eval/one: 0.8593, eval/two: 0.6969, eval/three: 0.7876 \n",
      "Epoch: 370, train/loss: 0.1534, eval/loss: 0.5487, train/r2: 0.7844, eval/r2: 0.7894, train/one: 0.9670, train/two: 0.6143, train/three: 0.7720, eval/one: 0.8594, eval/two: 0.7070, eval/three: 0.8018 \n",
      "Epoch: 375, train/loss: 0.1530, eval/loss: 0.5544, train/r2: 0.8003, eval/r2: 0.7940, train/one: 0.9652, train/two: 0.6526, train/three: 0.7831, eval/one: 0.8571, eval/two: 0.7212, eval/three: 0.8038 \n",
      "Epoch: 380, train/loss: 0.1853, eval/loss: 0.5734, train/r2: 0.7843, eval/r2: 0.7895, train/one: 0.9551, train/two: 0.6607, train/three: 0.7370, eval/one: 0.8518, eval/two: 0.7213, eval/three: 0.7953 \n",
      "Epoch: 385, train/loss: 0.1579, eval/loss: 0.5847, train/r2: 0.8020, eval/r2: 0.7874, train/one: 0.9640, train/two: 0.7028, train/three: 0.7392, eval/one: 0.8486, eval/two: 0.7327, eval/three: 0.7809 \n",
      "Epoch: 390, train/loss: 0.1483, eval/loss: 0.6103, train/r2: 0.8238, eval/r2: 0.7894, train/one: 0.9637, train/two: 0.6815, train/three: 0.8263, eval/one: 0.8406, eval/two: 0.7405, eval/three: 0.7872 \n",
      "Epoch: 395, train/loss: 0.1706, eval/loss: 0.6268, train/r2: 0.7877, eval/r2: 0.7881, train/one: 0.9600, train/two: 0.6391, train/three: 0.7639, eval/one: 0.8357, eval/two: 0.7467, eval/three: 0.7820 \n",
      "Epoch: 400, train/loss: 0.1536, eval/loss: 0.6080, train/r2: 0.7933, eval/r2: 0.7800, train/one: 0.9652, train/two: 0.6068, train/three: 0.8079, eval/one: 0.8424, eval/two: 0.7388, eval/three: 0.7589 \n",
      "Epoch: 405, train/loss: 0.1383, eval/loss: 0.5973, train/r2: 0.7882, eval/r2: 0.7769, train/one: 0.9719, train/two: 0.5858, train/three: 0.8069, eval/one: 0.8461, eval/two: 0.7392, eval/three: 0.7455 \n",
      "Epoch: 410, train/loss: 0.1157, eval/loss: 0.6022, train/r2: 0.8577, eval/r2: 0.7692, train/one: 0.9725, train/two: 0.7528, train/three: 0.8479, eval/one: 0.8455, eval/two: 0.7388, eval/three: 0.7232 \n",
      "Epoch: 415, train/loss: 0.1264, eval/loss: 0.6247, train/r2: 0.8120, eval/r2: 0.7564, train/one: 0.9758, train/two: 0.7391, train/three: 0.7211, eval/one: 0.8402, eval/two: 0.7374, eval/three: 0.6918 \n",
      "Epoch: 420, train/loss: 0.1664, eval/loss: 0.6491, train/r2: 0.8233, eval/r2: 0.7524, train/one: 0.9577, train/two: 0.7381, train/three: 0.7743, eval/one: 0.8333, eval/two: 0.7397, eval/three: 0.6842 \n",
      "Epoch: 425, train/loss: 0.1393, eval/loss: 0.6437, train/r2: 0.8155, eval/r2: 0.7622, train/one: 0.9703, train/two: 0.7519, train/three: 0.7242, eval/one: 0.8337, eval/two: 0.7385, eval/three: 0.7142 \n",
      "Epoch: 430, train/loss: 0.1471, eval/loss: 0.6292, train/r2: 0.8155, eval/r2: 0.7746, train/one: 0.9663, train/two: 0.7138, train/three: 0.7664, eval/one: 0.8366, eval/two: 0.7365, eval/three: 0.7507 \n",
      "Epoch: 435, train/loss: 0.1850, eval/loss: 0.6133, train/r2: 0.7909, eval/r2: 0.7793, train/one: 0.9530, train/two: 0.6090, train/three: 0.8107, eval/one: 0.8409, eval/two: 0.7359, eval/three: 0.7612 \n",
      "Epoch: 440, train/loss: 0.1552, eval/loss: 0.6048, train/r2: 0.8029, eval/r2: 0.7823, train/one: 0.9634, train/two: 0.6307, train/three: 0.8147, eval/one: 0.8431, eval/two: 0.7399, eval/three: 0.7640 \n",
      "Epoch: 445, train/loss: 0.1426, eval/loss: 0.5990, train/r2: 0.8089, eval/r2: 0.7803, train/one: 0.9688, train/two: 0.6931, train/three: 0.7647, eval/one: 0.8451, eval/two: 0.7417, eval/three: 0.7540 \n",
      "Epoch: 450, train/loss: 0.1677, eval/loss: 0.5944, train/r2: 0.7498, eval/r2: 0.7768, train/one: 0.9651, train/two: 0.5174, train/three: 0.7670, eval/one: 0.8469, eval/two: 0.7408, eval/three: 0.7428 \n",
      "Epoch: 455, train/loss: 0.2209, eval/loss: 0.5951, train/r2: 0.7614, eval/r2: 0.7765, train/one: 0.9427, train/two: 0.5680, train/three: 0.7735, eval/one: 0.8468, eval/two: 0.7395, eval/three: 0.7432 \n",
      "Epoch: 460, train/loss: 0.1610, eval/loss: 0.5986, train/r2: 0.8135, eval/r2: 0.7761, train/one: 0.9606, train/two: 0.6958, train/three: 0.7842, eval/one: 0.8457, eval/two: 0.7376, eval/three: 0.7448 \n",
      "Epoch: 465, train/loss: 0.2263, eval/loss: 0.6027, train/r2: 0.7820, eval/r2: 0.7759, train/one: 0.9388, train/two: 0.6558, train/three: 0.7514, eval/one: 0.8445, eval/two: 0.7367, eval/three: 0.7466 \n",
      "Epoch: 470, train/loss: 0.1566, eval/loss: 0.6040, train/r2: 0.8187, eval/r2: 0.7763, train/one: 0.9614, train/two: 0.6884, train/three: 0.8063, eval/one: 0.8441, eval/two: 0.7372, eval/three: 0.7475 \n",
      "Epoch: 475, train/loss: 0.1709, eval/loss: 0.6021, train/r2: 0.8180, eval/r2: 0.7777, train/one: 0.9568, train/two: 0.7417, train/three: 0.7555, eval/one: 0.8445, eval/two: 0.7374, eval/three: 0.7511 \n",
      "Epoch: 480, train/loss: 0.1667, eval/loss: 0.6002, train/r2: 0.7827, eval/r2: 0.7790, train/one: 0.9615, train/two: 0.5957, train/three: 0.7909, eval/one: 0.8449, eval/two: 0.7374, eval/three: 0.7546 \n",
      "Epoch: 485, train/loss: 0.1516, eval/loss: 0.5990, train/r2: 0.8017, eval/r2: 0.7798, train/one: 0.9641, train/two: 0.5814, train/three: 0.8596, eval/one: 0.8452, eval/two: 0.7375, eval/three: 0.7566 \n",
      "Epoch: 490, train/loss: 0.1535, eval/loss: 0.5983, train/r2: 0.8365, eval/r2: 0.7802, train/one: 0.9602, train/two: 0.7198, train/three: 0.8295, eval/one: 0.8454, eval/two: 0.7376, eval/three: 0.7576 \n",
      "Epoch: 495, train/loss: 0.1563, eval/loss: 0.5981, train/r2: 0.8317, eval/r2: 0.7803, train/one: 0.9610, train/two: 0.7725, train/three: 0.7616, eval/one: 0.8454, eval/two: 0.7375, eval/three: 0.7579 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 242 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 242 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-112/metadata\n",
      "0.45465804442772045\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075b34799bb34bb9874f671e22de310c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 4.3655, eval/loss: 2.8915, train/r2: -0.2826, eval/r2: 0.1166, train/one: -0.3807, train/two: -0.6825, train/three: 0.2153, eval/one: -0.2680, eval/two: -0.0761, eval/three: 0.6940 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train/loss: 3.3783, eval/loss: 1.4284, train/r2: -0.1753, eval/r2: 0.1821, train/one: -0.0486, train/two: -0.8945, train/three: 0.4170, eval/one: 0.4334, eval/two: -0.1361, eval/three: 0.2491 \n",
      "Epoch: 10, train/loss: 1.4119, eval/loss: 23.3321, train/r2: 0.1576, eval/r2: -3.3648, train/one: 0.6063, train/two: -0.4790, train/three: 0.3455, eval/one: -9.5806, eval/two: -0.1273, eval/three: -0.3865 \n",
      "Epoch: 15, train/loss: 0.5847, eval/loss: 41.1326, train/r2: 0.2739, eval/r2: -5.7294, train/one: 0.8839, train/two: -0.4182, train/three: 0.3558, eval/one: -17.8185, eval/two: 0.0102, eval/three: 0.6202 \n",
      "Epoch: 20, train/loss: 0.6295, eval/loss: 24.4591, train/r2: 0.1935, eval/r2: -3.6483, train/one: 0.8772, train/two: -0.6391, train/three: 0.3425, eval/one: -10.0786, eval/two: -0.2322, eval/three: -0.6341 \n",
      "Epoch: 25, train/loss: 0.5628, eval/loss: 10.2992, train/r2: 0.2531, eval/r2: -2.1774, train/one: 0.8939, train/two: -0.4911, train/three: 0.3565, eval/one: -3.4436, eval/two: -0.3302, eval/three: -2.7582 \n",
      "Epoch: 30, train/loss: 0.4879, eval/loss: 2.6950, train/r2: 0.3486, eval/r2: -0.5291, train/one: 0.9079, train/two: -0.3417, train/three: 0.4795, eval/one: -0.0451, eval/two: -0.1442, eval/three: -1.3979 \n",
      "Epoch: 35, train/loss: 0.4297, eval/loss: 1.0011, train/r2: 0.3421, eval/r2: 0.1815, train/one: 0.9301, train/two: -0.3131, train/three: 0.4092, eval/one: 0.6442, eval/two: -0.0781, eval/three: -0.0216 \n",
      "Epoch: 40, train/loss: 0.4621, eval/loss: 0.6014, train/r2: 0.3816, eval/r2: 0.4983, train/one: 0.9122, train/two: -0.3273, train/three: 0.5599, eval/one: 0.7775, eval/two: -0.0509, eval/three: 0.7682 \n",
      "Epoch: 45, train/loss: 0.3426, eval/loss: 0.5552, train/r2: 0.4624, eval/r2: 0.3976, train/one: 0.9455, train/two: -0.0987, train/three: 0.5405, eval/one: 0.8200, eval/two: -0.0570, eval/three: 0.4299 \n",
      "Epoch: 50, train/loss: 0.3818, eval/loss: 0.5491, train/r2: 0.3665, eval/r2: 0.5240, train/one: 0.9409, train/two: -0.4829, train/three: 0.6414, eval/one: 0.7982, eval/two: -0.0407, eval/three: 0.8145 \n",
      "Epoch: 55, train/loss: 0.2936, eval/loss: 0.5222, train/r2: 0.4554, eval/r2: 0.5355, train/one: 0.9636, train/two: -0.1312, train/three: 0.5337, eval/one: 0.8090, eval/two: -0.0466, eval/three: 0.8441 \n",
      "Epoch: 60, train/loss: 0.3181, eval/loss: 0.5517, train/r2: 0.3888, eval/r2: 0.4564, train/one: 0.9619, train/two: -0.3619, train/three: 0.5665, eval/one: 0.8111, eval/two: -0.0175, eval/three: 0.5755 \n",
      "Epoch: 65, train/loss: 0.3307, eval/loss: 0.5369, train/r2: 0.4295, eval/r2: 0.5069, train/one: 0.9527, train/two: -0.2487, train/three: 0.5843, eval/one: 0.8081, eval/two: -0.0215, eval/three: 0.7342 \n",
      "Epoch: 70, train/loss: 0.3488, eval/loss: 0.5790, train/r2: 0.4241, eval/r2: 0.5059, train/one: 0.9448, train/two: -0.4063, train/three: 0.7337, eval/one: 0.7877, eval/two: -0.0209, eval/three: 0.7509 \n",
      "Epoch: 75, train/loss: 0.3137, eval/loss: 0.5828, train/r2: 0.3696, eval/r2: 0.4985, train/one: 0.9658, train/two: -0.4058, train/three: 0.5488, eval/one: 0.7881, eval/two: 0.0066, eval/three: 0.7007 \n",
      "Epoch: 80, train/loss: 0.3196, eval/loss: 0.5586, train/r2: 0.4299, eval/r2: 0.5725, train/one: 0.9559, train/two: -0.2983, train/three: 0.6322, eval/one: 0.7850, eval/two: -0.0019, eval/three: 0.9344 \n",
      "Epoch: 85, train/loss: 0.3873, eval/loss: 0.5367, train/r2: 0.4040, eval/r2: 0.5588, train/one: 0.9351, train/two: -0.3426, train/three: 0.6195, eval/one: 0.7989, eval/two: 0.0149, eval/three: 0.8628 \n",
      "Epoch: 90, train/loss: 0.4003, eval/loss: 0.5363, train/r2: 0.3747, eval/r2: 0.5761, train/one: 0.9351, train/two: -0.3380, train/three: 0.5271, eval/one: 0.7966, eval/two: 0.0507, eval/three: 0.8811 \n",
      "Epoch: 95, train/loss: 0.2750, eval/loss: 0.5287, train/r2: 0.5080, eval/r2: 0.5823, train/one: 0.9636, train/two: -0.0251, train/three: 0.5856, eval/one: 0.7983, eval/two: 0.0234, eval/three: 0.9251 \n",
      "Epoch: 100, train/loss: 0.3354, eval/loss: 0.9099, train/r2: 0.3860, eval/r2: 0.3809, train/one: 0.9569, train/two: -0.3111, train/three: 0.5122, eval/one: 0.6513, eval/two: 0.0024, eval/three: 0.4891 \n",
      "Epoch: 105, train/loss: 0.2707, eval/loss: 1.1188, train/r2: 0.5707, eval/r2: 0.4586, train/one: 0.9574, train/two: 0.1171, train/three: 0.6376, eval/one: 0.5342, eval/two: 0.0257, eval/three: 0.8160 \n",
      "Epoch: 110, train/loss: 0.3220, eval/loss: 0.7734, train/r2: 0.4278, eval/r2: 0.5355, train/one: 0.9575, train/two: -0.1545, train/three: 0.4804, eval/one: 0.6874, eval/two: 0.0053, eval/three: 0.9138 \n",
      "Epoch: 115, train/loss: 0.3080, eval/loss: 0.5691, train/r2: 0.5635, eval/r2: 0.5625, train/one: 0.9456, train/two: 0.1413, train/three: 0.6035, eval/one: 0.7836, eval/two: 0.0641, eval/three: 0.8398 \n",
      "Epoch: 120, train/loss: 0.2800, eval/loss: 0.5627, train/r2: 0.5332, eval/r2: 0.5625, train/one: 0.9605, train/two: 0.1502, train/three: 0.4888, eval/one: 0.7856, eval/two: 0.0205, eval/three: 0.8815 \n",
      "Epoch: 125, train/loss: 0.3095, eval/loss: 0.5001, train/r2: 0.4826, eval/r2: 0.6219, train/one: 0.9550, train/two: -0.0409, train/three: 0.5337, eval/one: 0.8081, eval/two: 0.1603, eval/three: 0.8971 \n",
      "Epoch: 130, train/loss: 0.2849, eval/loss: 0.5250, train/r2: 0.5652, eval/r2: 0.6399, train/one: 0.9530, train/two: 0.0999, train/three: 0.6428, eval/one: 0.7934, eval/two: 0.1997, eval/three: 0.9265 \n",
      "Epoch: 135, train/loss: 0.2711, eval/loss: 0.5805, train/r2: 0.6140, eval/r2: 0.6154, train/one: 0.9504, train/two: 0.1062, train/three: 0.7853, eval/one: 0.7716, eval/two: 0.2166, eval/three: 0.8579 \n",
      "Epoch: 140, train/loss: 0.1898, eval/loss: 0.5841, train/r2: 0.6910, eval/r2: 0.5767, train/one: 0.9706, train/two: 0.3294, train/three: 0.7729, eval/one: 0.7752, eval/two: 0.1316, eval/three: 0.8233 \n",
      "Epoch: 145, train/loss: 0.2289, eval/loss: 0.4823, train/r2: 0.6710, eval/r2: 0.5659, train/one: 0.9591, train/two: 0.2782, train/three: 0.7755, eval/one: 0.8311, eval/two: 0.2758, eval/three: 0.5910 \n",
      "Epoch: 150, train/loss: 0.1924, eval/loss: 0.5075, train/r2: 0.7127, eval/r2: 0.5570, train/one: 0.9692, train/two: 0.5239, train/three: 0.6449, eval/one: 0.8196, eval/two: 0.2437, eval/three: 0.6077 \n",
      "Epoch: 155, train/loss: 0.1974, eval/loss: 0.5162, train/r2: 0.7159, eval/r2: 0.5496, train/one: 0.9665, train/two: 0.4965, train/three: 0.6848, eval/one: 0.8212, eval/two: 0.4049, eval/three: 0.4227 \n",
      "Epoch: 160, train/loss: 0.2387, eval/loss: 0.6712, train/r2: 0.6994, eval/r2: 0.4369, train/one: 0.9538, train/two: 0.4594, train/three: 0.6851, eval/one: 0.7505, eval/two: -0.2401, eval/three: 0.8002 \n",
      "Epoch: 165, train/loss: 0.1996, eval/loss: 0.4980, train/r2: 0.7374, eval/r2: 0.7289, train/one: 0.9632, train/two: 0.5517, train/three: 0.6971, eval/one: 0.7971, eval/two: 0.5072, eval/three: 0.8822 \n",
      "Epoch: 170, train/loss: 0.2219, eval/loss: 0.4766, train/r2: 0.7201, eval/r2: 0.7342, train/one: 0.9580, train/two: 0.5557, train/three: 0.6466, eval/one: 0.8059, eval/two: 0.4835, eval/three: 0.9132 \n",
      "Epoch: 175, train/loss: 0.2256, eval/loss: 0.4654, train/r2: 0.7598, eval/r2: 0.6747, train/one: 0.9508, train/two: 0.5723, train/three: 0.7563, eval/one: 0.8179, eval/two: 0.2850, eval/three: 0.9212 \n",
      "Epoch: 180, train/loss: 0.1534, eval/loss: 0.4165, train/r2: 0.7712, eval/r2: 0.7636, train/one: 0.9748, train/two: 0.5786, train/three: 0.7603, eval/one: 0.8326, eval/two: 0.6044, eval/three: 0.8538 \n",
      "Epoch: 185, train/loss: 0.1635, eval/loss: 0.4508, train/r2: 0.7756, eval/r2: 0.7697, train/one: 0.9707, train/two: 0.5889, train/three: 0.7673, eval/one: 0.8135, eval/two: 0.5607, eval/three: 0.9350 \n",
      "Epoch: 190, train/loss: 0.1722, eval/loss: 0.5565, train/r2: 0.7573, eval/r2: 0.4036, train/one: 0.9707, train/two: 0.6107, train/three: 0.6905, eval/one: 0.8057, eval/two: -0.5242, eval/three: 0.9293 \n",
      "Epoch: 195, train/loss: 0.1907, eval/loss: 0.3817, train/r2: 0.7390, eval/r2: 0.7819, train/one: 0.9653, train/two: 0.4966, train/three: 0.7550, eval/one: 0.8454, eval/two: 0.5794, eval/three: 0.9208 \n",
      "Epoch: 200, train/loss: 0.2323, eval/loss: 0.4471, train/r2: 0.7098, eval/r2: 0.7913, train/one: 0.9559, train/two: 0.5579, train/three: 0.6155, eval/one: 0.8129, eval/two: 0.6320, eval/three: 0.9290 \n",
      "Epoch: 205, train/loss: 0.1674, eval/loss: 0.4374, train/r2: 0.7909, eval/r2: 0.7447, train/one: 0.9694, train/two: 0.7586, train/three: 0.6446, eval/one: 0.8261, eval/two: 0.6034, eval/three: 0.8047 \n",
      "Epoch: 210, train/loss: 0.2214, eval/loss: 0.4052, train/r2: 0.7369, eval/r2: 0.7617, train/one: 0.9558, train/two: 0.5766, train/three: 0.6784, eval/one: 0.8364, eval/two: 0.5210, eval/three: 0.9276 \n",
      "Epoch: 215, train/loss: 0.2175, eval/loss: 0.4778, train/r2: 0.7756, eval/r2: 0.7638, train/one: 0.9536, train/two: 0.7399, train/three: 0.6332, eval/one: 0.8041, eval/two: 0.6575, eval/three: 0.8296 \n",
      "Epoch: 220, train/loss: 0.1959, eval/loss: 0.5056, train/r2: 0.7976, eval/r2: 0.7835, train/one: 0.9571, train/two: 0.6894, train/three: 0.7462, eval/one: 0.7876, eval/two: 0.6967, eval/three: 0.8663 \n",
      "Epoch: 225, train/loss: 0.1981, eval/loss: 0.4062, train/r2: 0.7972, eval/r2: 0.7288, train/one: 0.9558, train/two: 0.6468, train/three: 0.7890, eval/one: 0.8417, eval/two: 0.4949, eval/three: 0.8499 \n",
      "Epoch: 230, train/loss: 0.1681, eval/loss: 0.4099, train/r2: 0.8427, eval/r2: 0.7520, train/one: 0.9615, train/two: 0.7901, train/three: 0.7764, eval/one: 0.8376, eval/two: 0.5829, eval/three: 0.8356 \n",
      "Epoch: 235, train/loss: 0.1671, eval/loss: 0.3829, train/r2: 0.7156, eval/r2: 0.8010, train/one: 0.9773, train/two: 0.4889, train/three: 0.6806, eval/one: 0.8427, eval/two: 0.6439, eval/three: 0.9164 \n",
      "Epoch: 240, train/loss: 0.1442, eval/loss: 0.4251, train/r2: 0.8164, eval/r2: 0.7365, train/one: 0.9733, train/two: 0.7350, train/three: 0.7409, eval/one: 0.8298, eval/two: 0.4530, eval/three: 0.9267 \n",
      "Epoch: 245, train/loss: 0.1438, eval/loss: 0.4234, train/r2: 0.7944, eval/r2: 0.7765, train/one: 0.9759, train/two: 0.6667, train/three: 0.7405, eval/one: 0.8261, eval/two: 0.5823, eval/three: 0.9211 \n",
      "Epoch: 250, train/loss: 0.2050, eval/loss: 0.4268, train/r2: 0.7927, eval/r2: 0.7685, train/one: 0.9542, train/two: 0.6542, train/three: 0.7698, eval/one: 0.8258, eval/two: 0.5714, eval/three: 0.9084 \n",
      "Epoch: 255, train/loss: 0.1661, eval/loss: 0.4072, train/r2: 0.7917, eval/r2: 0.7736, train/one: 0.9684, train/two: 0.6709, train/three: 0.7358, eval/one: 0.8370, eval/two: 0.6691, eval/three: 0.8147 \n",
      "Epoch: 260, train/loss: 0.1957, eval/loss: 0.4558, train/r2: 0.7833, eval/r2: 0.7924, train/one: 0.9593, train/two: 0.6813, train/three: 0.7092, eval/one: 0.8091, eval/two: 0.6573, eval/three: 0.9109 \n",
      "Epoch: 265, train/loss: 0.1478, eval/loss: 0.4411, train/r2: 0.8169, eval/r2: 0.7369, train/one: 0.9724, train/two: 0.7692, train/three: 0.7091, eval/one: 0.8231, eval/two: 0.4986, eval/three: 0.8890 \n",
      "Epoch: 270, train/loss: 0.1699, eval/loss: 0.4129, train/r2: 0.8126, eval/r2: 0.7974, train/one: 0.9645, train/two: 0.7174, train/three: 0.7558, eval/one: 0.8292, eval/two: 0.6592, eval/three: 0.9039 \n",
      "Epoch: 275, train/loss: 0.1543, eval/loss: 0.3949, train/r2: 0.8085, eval/r2: 0.8106, train/one: 0.9697, train/two: 0.6518, train/three: 0.8040, eval/one: 0.8358, eval/two: 0.6751, eval/three: 0.9210 \n",
      "Epoch: 280, train/loss: 0.1717, eval/loss: 0.3685, train/r2: 0.8554, eval/r2: 0.8263, train/one: 0.9587, train/two: 0.8195, train/three: 0.7882, eval/one: 0.8468, eval/two: 0.7204, eval/three: 0.9118 \n",
      "Epoch: 285, train/loss: 0.1964, eval/loss: 0.4000, train/r2: 0.8205, eval/r2: 0.8203, train/one: 0.9540, train/two: 0.7304, train/three: 0.7772, eval/one: 0.8324, eval/two: 0.7151, eval/three: 0.9134 \n",
      "Epoch: 290, train/loss: 0.1818, eval/loss: 0.4119, train/r2: 0.8239, eval/r2: 0.7854, train/one: 0.9589, train/two: 0.7489, train/three: 0.7640, eval/one: 0.8322, eval/two: 0.6655, eval/three: 0.8583 \n",
      "Epoch: 295, train/loss: 0.2093, eval/loss: 0.4387, train/r2: 0.7255, eval/r2: 0.7669, train/one: 0.9616, train/two: 0.5564, train/three: 0.6585, eval/one: 0.8200, eval/two: 0.5633, eval/three: 0.9173 \n",
      "Epoch: 300, train/loss: 0.2200, eval/loss: 0.4848, train/r2: 0.7578, eval/r2: 0.6929, train/one: 0.9538, train/two: 0.6260, train/three: 0.6937, eval/one: 0.8067, eval/two: 0.3572, eval/three: 0.9147 \n",
      "Epoch: 305, train/loss: 0.1704, eval/loss: 0.4172, train/r2: 0.8272, eval/r2: 0.8202, train/one: 0.9631, train/two: 0.7898, train/three: 0.7286, eval/one: 0.8236, eval/two: 0.7017, eval/three: 0.9352 \n",
      "Epoch: 310, train/loss: 0.1431, eval/loss: 0.3888, train/r2: 0.8574, eval/r2: 0.7615, train/one: 0.9679, train/two: 0.7754, train/three: 0.8290, eval/one: 0.8452, eval/two: 0.5511, eval/three: 0.8881 \n",
      "Epoch: 315, train/loss: 0.1879, eval/loss: 0.4066, train/r2: 0.8189, eval/r2: 0.8123, train/one: 0.9569, train/two: 0.7066, train/three: 0.7931, eval/one: 0.8308, eval/two: 0.7143, eval/three: 0.8917 \n",
      "Epoch: 320, train/loss: 0.1664, eval/loss: 0.3996, train/r2: 0.8030, eval/r2: 0.8114, train/one: 0.9665, train/two: 0.6651, train/three: 0.7773, eval/one: 0.8330, eval/two: 0.6644, eval/three: 0.9368 \n",
      "Epoch: 325, train/loss: 0.1624, eval/loss: 0.3899, train/r2: 0.7959, eval/r2: 0.8130, train/one: 0.9699, train/two: 0.7257, train/three: 0.6920, eval/one: 0.8376, eval/two: 0.6681, eval/three: 0.9333 \n",
      "Epoch: 330, train/loss: 0.1544, eval/loss: 0.3966, train/r2: 0.8106, eval/r2: 0.8208, train/one: 0.9702, train/two: 0.7087, train/three: 0.7530, eval/one: 0.8335, eval/two: 0.6968, eval/three: 0.9321 \n",
      "Epoch: 335, train/loss: 0.1652, eval/loss: 0.4099, train/r2: 0.7847, eval/r2: 0.8249, train/one: 0.9695, train/two: 0.6495, train/three: 0.7350, eval/one: 0.8268, eval/two: 0.7203, eval/three: 0.9275 \n",
      "Epoch: 340, train/loss: 0.1829, eval/loss: 0.3926, train/r2: 0.7987, eval/r2: 0.8110, train/one: 0.9625, train/two: 0.7519, train/three: 0.6817, eval/one: 0.8367, eval/two: 0.6714, eval/three: 0.9247 \n",
      "Epoch: 345, train/loss: 0.1789, eval/loss: 0.3808, train/r2: 0.8065, eval/r2: 0.8313, train/one: 0.9623, train/two: 0.7196, train/three: 0.7376, eval/one: 0.8399, eval/two: 0.7254, eval/three: 0.9285 \n",
      "Epoch: 350, train/loss: 0.1766, eval/loss: 0.3938, train/r2: 0.8317, eval/r2: 0.8347, train/one: 0.9593, train/two: 0.7326, train/three: 0.8032, eval/one: 0.8331, eval/two: 0.7377, eval/three: 0.9333 \n",
      "Epoch: 355, train/loss: 0.2164, eval/loss: 0.3878, train/r2: 0.7927, eval/r2: 0.8070, train/one: 0.9507, train/two: 0.6977, train/three: 0.7296, eval/one: 0.8393, eval/two: 0.6521, eval/three: 0.9296 \n",
      "Epoch: 360, train/loss: 0.1707, eval/loss: 0.3837, train/r2: 0.7846, eval/r2: 0.8208, train/one: 0.9680, train/two: 0.6780, train/three: 0.7080, eval/one: 0.8397, eval/two: 0.6929, eval/three: 0.9299 \n",
      "Epoch: 365, train/loss: 0.1781, eval/loss: 0.3882, train/r2: 0.8146, eval/r2: 0.8241, train/one: 0.9620, train/two: 0.7677, train/three: 0.7143, eval/one: 0.8372, eval/two: 0.7058, eval/three: 0.9293 \n",
      "Epoch: 370, train/loss: 0.1603, eval/loss: 0.4007, train/r2: 0.7925, eval/r2: 0.8195, train/one: 0.9699, train/two: 0.6395, train/three: 0.7680, eval/one: 0.8317, eval/two: 0.6967, eval/three: 0.9301 \n",
      "Epoch: 375, train/loss: 0.1504, eval/loss: 0.4069, train/r2: 0.8321, eval/r2: 0.8201, train/one: 0.9686, train/two: 0.7312, train/three: 0.7966, eval/one: 0.8287, eval/two: 0.7008, eval/three: 0.9307 \n",
      "Epoch: 380, train/loss: 0.1760, eval/loss: 0.4005, train/r2: 0.8109, eval/r2: 0.8216, train/one: 0.9622, train/two: 0.6946, train/three: 0.7757, eval/one: 0.8316, eval/two: 0.7045, eval/three: 0.9287 \n",
      "Epoch: 385, train/loss: 0.1779, eval/loss: 0.3959, train/r2: 0.7716, eval/r2: 0.8244, train/one: 0.9686, train/two: 0.7587, train/three: 0.5874, eval/one: 0.8335, eval/two: 0.7122, eval/three: 0.9275 \n",
      "Epoch: 390, train/loss: 0.1542, eval/loss: 0.3948, train/r2: 0.8423, eval/r2: 0.8185, train/one: 0.9660, train/two: 0.7529, train/three: 0.8080, eval/one: 0.8351, eval/two: 0.7057, eval/three: 0.9146 \n",
      "Epoch: 395, train/loss: 0.1949, eval/loss: 0.4055, train/r2: 0.8379, eval/r2: 0.8028, train/one: 0.9522, train/two: 0.7549, train/three: 0.8067, eval/one: 0.8318, eval/two: 0.6638, eval/three: 0.9127 \n",
      "Epoch: 400, train/loss: 0.1507, eval/loss: 0.3971, train/r2: 0.8187, eval/r2: 0.8119, train/one: 0.9717, train/two: 0.8073, train/three: 0.6771, eval/one: 0.8345, eval/two: 0.6790, eval/three: 0.9223 \n",
      "Epoch: 405, train/loss: 0.1972, eval/loss: 0.3917, train/r2: 0.8280, eval/r2: 0.8159, train/one: 0.9536, train/two: 0.8009, train/three: 0.7297, eval/one: 0.8365, eval/two: 0.6826, eval/three: 0.9288 \n",
      "Epoch: 410, train/loss: 0.1774, eval/loss: 0.3826, train/r2: 0.8165, eval/r2: 0.8267, train/one: 0.9612, train/two: 0.7136, train/three: 0.7747, eval/one: 0.8395, eval/two: 0.7113, eval/three: 0.9292 \n",
      "Epoch: 415, train/loss: 0.1641, eval/loss: 0.3819, train/r2: 0.8500, eval/r2: 0.8276, train/one: 0.9621, train/two: 0.8090, train/three: 0.7790, eval/one: 0.8398, eval/two: 0.7148, eval/three: 0.9282 \n",
      "Epoch: 420, train/loss: 0.1959, eval/loss: 0.3800, train/r2: 0.7741, eval/r2: 0.8307, train/one: 0.9602, train/two: 0.6485, train/three: 0.7136, eval/one: 0.8404, eval/two: 0.7230, eval/three: 0.9286 \n",
      "Epoch: 425, train/loss: 0.1907, eval/loss: 0.3791, train/r2: 0.7901, eval/r2: 0.8316, train/one: 0.9610, train/two: 0.7480, train/three: 0.6614, eval/one: 0.8407, eval/two: 0.7264, eval/three: 0.9277 \n",
      "Epoch: 430, train/loss: 0.1515, eval/loss: 0.3785, train/r2: 0.8499, eval/r2: 0.8316, train/one: 0.9669, train/two: 0.8305, train/three: 0.7524, eval/one: 0.8410, eval/two: 0.7260, eval/three: 0.9280 \n",
      "Epoch: 435, train/loss: 0.1975, eval/loss: 0.3780, train/r2: 0.8068, eval/r2: 0.8320, train/one: 0.9560, train/two: 0.7450, train/three: 0.7193, eval/one: 0.8411, eval/two: 0.7257, eval/three: 0.9290 \n",
      "Epoch: 440, train/loss: 0.2166, eval/loss: 0.3765, train/r2: 0.7269, eval/r2: 0.8315, train/one: 0.9587, train/two: 0.5479, train/three: 0.6741, eval/one: 0.8419, eval/two: 0.7219, eval/three: 0.9306 \n",
      "Epoch: 445, train/loss: 0.2123, eval/loss: 0.3746, train/r2: 0.7762, eval/r2: 0.8312, train/one: 0.9532, train/two: 0.5912, train/three: 0.7841, eval/one: 0.8428, eval/two: 0.7192, eval/three: 0.9315 \n",
      "Epoch: 450, train/loss: 0.1711, eval/loss: 0.3747, train/r2: 0.8335, eval/r2: 0.8300, train/one: 0.9610, train/two: 0.7298, train/three: 0.8098, eval/one: 0.8428, eval/two: 0.7148, eval/three: 0.9323 \n",
      "Epoch: 455, train/loss: 0.1927, eval/loss: 0.3753, train/r2: 0.7656, eval/r2: 0.8283, train/one: 0.9620, train/two: 0.6012, train/three: 0.7338, eval/one: 0.8428, eval/two: 0.7097, eval/three: 0.9325 \n",
      "Epoch: 460, train/loss: 0.1263, eval/loss: 0.3757, train/r2: 0.8555, eval/r2: 0.8272, train/one: 0.9745, train/two: 0.7970, train/three: 0.7949, eval/one: 0.8427, eval/two: 0.7074, eval/three: 0.9313 \n",
      "Epoch: 465, train/loss: 0.1434, eval/loss: 0.3763, train/r2: 0.8243, eval/r2: 0.8272, train/one: 0.9724, train/two: 0.7375, train/three: 0.7631, eval/one: 0.8425, eval/two: 0.7096, eval/three: 0.9294 \n",
      "Epoch: 470, train/loss: 0.1619, eval/loss: 0.3769, train/r2: 0.7907, eval/r2: 0.8274, train/one: 0.9702, train/two: 0.6815, train/three: 0.7204, eval/one: 0.8422, eval/two: 0.7114, eval/three: 0.9286 \n",
      "Epoch: 475, train/loss: 0.1296, eval/loss: 0.3773, train/r2: 0.8486, eval/r2: 0.8279, train/one: 0.9738, train/two: 0.7579, train/three: 0.8141, eval/one: 0.8419, eval/two: 0.7130, eval/three: 0.9288 \n",
      "Epoch: 480, train/loss: 0.1622, eval/loss: 0.3774, train/r2: 0.8048, eval/r2: 0.8282, train/one: 0.9680, train/two: 0.6871, train/three: 0.7593, eval/one: 0.8418, eval/two: 0.7136, eval/three: 0.9292 \n",
      "Epoch: 485, train/loss: 0.1895, eval/loss: 0.3774, train/r2: 0.8264, eval/r2: 0.8284, train/one: 0.9551, train/two: 0.7027, train/three: 0.8213, eval/one: 0.8418, eval/two: 0.7138, eval/three: 0.9294 \n",
      "Epoch: 490, train/loss: 0.1346, eval/loss: 0.3772, train/r2: 0.8531, eval/r2: 0.8285, train/one: 0.9723, train/two: 0.8198, train/three: 0.7672, eval/one: 0.8419, eval/two: 0.7140, eval/three: 0.9296 \n",
      "Epoch: 495, train/loss: 0.1949, eval/loss: 0.3771, train/r2: 0.7614, eval/r2: 0.8286, train/one: 0.9615, train/two: 0.5740, train/three: 0.7487, eval/one: 0.8419, eval/two: 0.7141, eval/three: 0.9296 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 65 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 65 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-113/metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-108/metadata\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "mus = [0.5431315108096543,\n",
    " 0.5373212199292232,\n",
    " 0.5625938858971476,\n",
    " 0.5611233230776563,\n",
    " 0.5471325694126035]\n",
    "\n",
    "sigmas = [249.05215723502948,\n",
    " 248.6726517269414,\n",
    " 247.84789229685813,\n",
    " 247.65868506084584,\n",
    " 248.16012425733976]\n",
    "\n",
    "mus, sigmas = [], []\n",
    "scores = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "splits = kfold.split(range(96))\n",
    "\n",
    "for fold, (train_idx, eval_idx) in enumerate(splits):\n",
    "    MODEL_NAME = f\"resnet.finetune.fold.{fold}\"\n",
    "    checkpoint_name = f\"finetune.fold.{fold}.pt\"\n",
    "    \n",
    "    train_inputs = inputs[train_idx]\n",
    "    train_targets = targets[train_idx]\n",
    "    eval_inputs = inputs[eval_idx]\n",
    "    eval_targets = targets[eval_idx]\n",
    "    \n",
    "    mu, sigma = get_stats(train_inputs, p=False, r=True)\n",
    "    train_inputs = zscore(train_inputs, mu, sigma)\n",
    "    eval_inputs = zscore(eval_inputs, mu, sigma)\n",
    "    mus.append(mu)\n",
    "    sigmas.append(sigma)\n",
    "\n",
    "    train_ds = TensorDataset(train_inputs.float(), train_targets.float())\n",
    "    eval_ds = TensorDataset(eval_inputs.float(), eval_targets.float())\n",
    "    \n",
    "    BATCH_SIZE = len(train_ds)\n",
    "    train_dl, eval_dl = return_dls(train_ds, eval_ds, BATCH_SIZE, len(eval_ds))\n",
    "    \n",
    "    model = ResNet(input_channels=2, dropout=DROPOUT).to(device)\n",
    "    if fold == 0: print(get_model_size(model))\n",
    "    \n",
    "    ckpt = torch.load(f\"pretrain.fold.{fold}.pt\", weights_only=False)\n",
    "    print(ckpt[\"score\"])\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    scheduler = get_scheduler(optimizer, train_dl, EPOCHS)\n",
    "    score = train(\n",
    "            model, \n",
    "            optimizer, \n",
    "            device,\n",
    "            scaler,\n",
    "            scheduler,\n",
    "            train_dl, \n",
    "            eval_dl,\n",
    "            EPOCHS,\n",
    "            checkpoint_name,\n",
    "            neptune_run=setup_neptune(),\n",
    "        )\n",
    "    \n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e55b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = f\"/kaggle/working/{checkpoint_name}\"\n",
    "ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "ckpt[\"epoch\"], ckpt[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "176d5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_test_data()\n",
    "get_stats(test)\n",
    "test = get_spectra_features(test)\n",
    "test = torch.tensor(test)\n",
    "test = zscore(test, mu, sigma).float()\n",
    "test.shape, test.dtype, get_stats(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(input_channels=2).to(device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = model(test.cuda())\n",
    "\n",
    "preds = preds.cpu().detach().double().numpy()\n",
    "preds.shape, get_stats(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']\n",
    "preds_df = pd.DataFrame(preds, columns=column_names)\n",
    "preds_df.insert(0, 'ID', [i+1 for i in range(len(preds_df))])\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = MODEL_NAME+\".finetune.transfer.in.pretrain.csv\"\n",
    "preds_df.to_csv(name, index=False)\n",
    "f = pd.read_csv(f\"/kaggle/working/{name}\")\n",
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
