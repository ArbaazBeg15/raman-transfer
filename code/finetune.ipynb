{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6442c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "SEED = 1000\n",
    "setup_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3fb896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, train_dl, epochs):\n",
    "    total_training_steps = len(train_dl) * epochs\n",
    "    warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "\n",
    "\n",
    "def get_stats(tensor, p=True, r=False):\n",
    "    mean, std = tensor.mean(), tensor.std()\n",
    "    min, max =  tensor.min(), tensor.max()\n",
    "    \n",
    "    #if p: print(f\"Min: {min}, Max: {max}, Mean: {mean}, Std: {std}\")\n",
    "    if p: print(f\"Mean: {mean}, Std: {std}\")\n",
    "    if r: return mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-8)\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "    \n",
    "\n",
    "def get_index(iterable):\n",
    "    return random.randint(0, len(iterable) - 1)\n",
    "\n",
    "\n",
    "def get_indices(iterable, n):\n",
    "    return random.sample(range(len(iterable)), n)\n",
    "\n",
    "\n",
    "def split(inputs, targets, seed):\n",
    "    return train_test_split(\n",
    "        inputs,\n",
    "        targets, \n",
    "        test_size=0.2,\n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    ) \n",
    "\n",
    "\n",
    "def show_waves(waves, dpi=100):\n",
    "    \"\"\"\n",
    "    waves: numpy array of shape (3, N)\n",
    "    Creates three separate figures that stretch wide.\n",
    "    \"\"\"\n",
    "    N = waves.shape[1]\n",
    "    t = np.arange(N)\n",
    "\n",
    "    # Wide aspect ratio; height modest so each window fills width\n",
    "    for i in range(waves.shape[0]):\n",
    "        fig = plt.figure(figsize=(14, 4), dpi=dpi)  # wide figure\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(t, waves[i], linewidth=1)\n",
    "        ax.set_title(f\"Wave {i+1}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()  # reduce margins to use width\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def hf_ds_download(hf_token, repo_id):\n",
    "    login(hf_token[1:])\n",
    "    return snapshot_download(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "def get_spectra_features(X, b=False):\n",
    "    \"\"\"Create multi-channel features from spectra: raw, 1st derivative, 2nd derivative.\"\"\"\n",
    "    X_processed = np.zeros_like(X)\n",
    "    # Baseline correction and SNV\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        poly = np.polyfit(np.arange(X.shape[1]), X[i], 3)\n",
    "        baseline = np.polyval(poly, np.arange(X.shape[1]))\n",
    "        corrected_spec = X[i] - baseline\n",
    "        #X_processed[i] = (corrected_spec - corrected_spec.mean()) / (corrected_spec.std() + 1e-8)\n",
    "        X_processed[i] = corrected_spec\n",
    "        \n",
    "    # Calculate derivatives\n",
    "    deriv1 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=1, axis=1)\n",
    "    deriv2 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=2, axis=1)\n",
    "\n",
    "    if b: return np.stack([X_processed, deriv1, deriv2], axis=1)\n",
    "    return np.stack([deriv1, deriv2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc57c619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'sample_submission.csv'),\n",
       " (1, 'timegate.csv'),\n",
       " (2, 'mettler_toledo.csv'),\n",
       " (3, 'kaiser.csv'),\n",
       " (4, 'anton_532.csv'),\n",
       " (5, 'transfer_plate.csv'),\n",
       " (6, '96_samples.csv'),\n",
       " (7, 'tornado.csv'),\n",
       " (8, 'tec5.csv'),\n",
       " (9, 'metrohm.csv'),\n",
       " (10, 'anton_785.csv')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3d530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_transfer_data():\n",
    "    csv_path = os.path.join(path, files[5])\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    input_cols = df.columns[1:2049]\n",
    "    target_cols = df.columns[2050:]\n",
    "\n",
    "    targets  = df[target_cols].dropna().to_numpy()\n",
    "\n",
    "    df = df[input_cols]\n",
    "    df['Unnamed: 1'] = df['Unnamed: 1'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "    df['Unnamed: 2048'] = df['Unnamed: 2048'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "\n",
    "    inputs = df.to_numpy().reshape(-1, 2, 2048)\n",
    "    inputs = inputs.mean(axis=1)\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    test = pd.read_csv(os.path.join(path, files[6]))\n",
    "\n",
    "    row1 = test.columns[1:].to_numpy().copy()\n",
    "    row1[-1] = \"5611\"\n",
    "    row1 = row1.astype(np.float64)\n",
    "\n",
    "\n",
    "    cols = test.columns[1:]\n",
    "    test = test[cols]\n",
    "    test[\" 5611]\"] = test[\" 5611]\"].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "    test = test.to_numpy()\n",
    "\n",
    "    test = np.insert(test, 0, row1, axis=0)\n",
    "    return test.reshape(-1, 2, 2048).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b26650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519584f476f44e6f9eaf3884905adeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, targets = load_transfer_data()\n",
    "inputs = get_spectra_features(inputs)\n",
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(targets)\n",
    "#train_inputs, eval_inputs, train_targets, eval_targets = split(inputs, targets, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    eval_inputs = torch.tensor(eval_inputs)\n",
    "    train_targets = torch.tensor(train_targets)\n",
    "    eval_targets = torch.tensor(eval_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    min, max, mu, sigma = get_stats(train_inputs, r=True)\n",
    "    train_inputs = zscore(train_inputs)\n",
    "    eval_inputs = zscore(eval_inputs)\n",
    "    get_stats(train_inputs), get_stats(eval_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe55238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "if False:\n",
    "    train_ds = TensorDataset(train_inputs.float(), train_targets.float())\n",
    "    eval_ds = TensorDataset(eval_inputs.float(), eval_targets.float())\n",
    "    len(train_ds), len(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9dcceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+5232)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds, train_batch_size, eval_batch_size):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "437ed80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "        if DROPOUT: neptune_run[\"h_parameters\"] = {\"dropout\": DROPOUT}\n",
    "        if DROP_PATH_RATE: neptune_run[\"h_parameters\"] = {\"drop_path_rate\": DROP_PATH_RATE}\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e978702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    \n",
    "    dim1 = r2_score(targets[:, 0], preds[:, 0])\n",
    "    dim2 = r2_score(targets[:, 1], preds[:, 1])\n",
    "    dim3 = r2_score(targets[:, 2], preds[:, 2])\n",
    "    \n",
    "    mean_r2 = (dim1 + dim2 + dim3) / 3\n",
    "    \n",
    "    return dim1, dim2, dim3, mean_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e882d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A residual block with two 1D convolutional layers.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.elu = nn.ELU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.elu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.elu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"A deeper ResNet-style 1D CNN for Raman spectra.\"\"\"\n",
    "    def __init__(self, dropout, input_channels=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.elu = nn.GELU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout), # Increased dropout for better regularization\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, stride=s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44fca775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    device,\n",
    "    scaler, \n",
    "    scheduler,\n",
    "    train_dl,\n",
    "    eval_dl,\n",
    "    epochs,\n",
    "    checkpoint_name,\n",
    "    score=-float(\"inf\"),\n",
    "    neptune_run=None,\n",
    "    p=True,\n",
    "):  \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for inputs, targets in train_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            if neptune_run is not None:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            total_loss += loss.detach().cpu()\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        all_logits = torch.cat(all_logits)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "        total_loss = total_loss / len(train_dl)\n",
    "        \n",
    "        model.eval()\n",
    "        eval_total_loss = 0.0\n",
    "        eval_all_logits = []\n",
    "        eval_all_targets = []\n",
    "\n",
    "        for inputs, targets in eval_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                #with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "\n",
    "            eval_total_loss += loss.detach().cpu()\n",
    "            eval_all_logits.append(logits.detach().cpu())\n",
    "            eval_all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        eval_all_logits = torch.cat(eval_all_logits)\n",
    "        eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "        eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "        eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "        \n",
    "        if eval_r2 > score:\n",
    "            score = eval_r2\n",
    "            data = {\"state_dict\": model.state_dict()}\n",
    "            data[\"epoch\"] = epoch \n",
    "            data[\"score\"] = score\n",
    "            torch.save(data, f\"/kaggle/working/{checkpoint_name}\")\n",
    "        \n",
    "        if neptune_run is not None:\n",
    "            neptune_run[\"train/loss\"].append(total_loss)\n",
    "            neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "            neptune_run[\"train/r2\"].append(r2)\n",
    "            neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "            neptune_run[\"train/one\"].append(one)\n",
    "            neptune_run[\"train/two\"].append(two)\n",
    "            neptune_run[\"train/three\"].append(three)\n",
    "            neptune_run[\"eval/one\"].append(eval_one)\n",
    "            neptune_run[\"eval/two\"].append(eval_two)\n",
    "            neptune_run[\"eval/three\"].append(eval_three)\n",
    "            \n",
    "        if p and epoch % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, \"\n",
    "                f\"train/loss: {total_loss:.4f}, \"\n",
    "                f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "                f\"train/r2: {r2:.4f}, \"\n",
    "                f\"eval/r2: {eval_r2:.4f}, \"\n",
    "                f\"train/one: {one:.4f}, \"\n",
    "                f\"train/two: {two:.4f}, \"\n",
    "                f\"train/three: {three:.4f}, \"\n",
    "                f\"eval/one: {eval_one:.4f}, \"\n",
    "                f\"eval/two: {eval_two:.4f}, \"\n",
    "                f\"eval/three: {eval_three:.4f} \"\n",
    "            )\n",
    "            \n",
    "    if neptune_run is not None: neptune_run.stop()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6326238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings#; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "EPOCHS = 500\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "\n",
    "DROPOUT = 0.5\n",
    "DROP_PATH_RATE = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "183441eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.981251\n",
      "None\n",
      "0.4652334739404447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3f884a13b34686adc87e382170fa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 4.5531, eval/loss: 22.6519, train/r2: -0.4333, eval/r2: -4.9345, train/one: -0.4619, train/two: -0.8636, train/three: 0.0254, eval/one: -7.9963, eval/two: -1.3240, eval/three: -5.4832 \n",
      "Epoch: 5, train/loss: 2.7835, eval/loss: 5.0359, train/r2: -0.1362, eval/r2: -1.1596, train/one: 0.1379, train/two: -0.7257, train/three: 0.1792, eval/one: -0.8797, eval/two: -1.0507, eval/three: -1.5484 \n",
      "Epoch: 10, train/loss: 1.0434, eval/loss: 9.6548, train/r2: 0.0896, eval/r2: -2.1236, train/one: 0.7336, train/two: -0.8821, train/three: 0.4173, eval/one: -2.7405, eval/two: -0.6499, eval/three: -2.9805 \n",
      "Epoch: 15, train/loss: 0.5405, eval/loss: 28.3328, train/r2: 0.1592, eval/r2: -3.7047, train/one: 0.9071, train/two: -0.7916, train/three: 0.3622, eval/one: -10.7078, eval/two: -0.0930, eval/three: -0.3133 \n",
      "Epoch: 20, train/loss: 0.5523, eval/loss: 28.6166, train/r2: 0.1059, eval/r2: -3.4282, train/one: 0.9110, train/two: -0.8020, train/three: 0.2087, eval/one: -10.8829, eval/two: -0.0615, eval/three: 0.6596 \n",
      "Epoch: 25, train/loss: 0.4894, eval/loss: 13.6550, train/r2: 0.2519, eval/r2: -1.3064, train/one: 0.9126, train/two: -0.7025, train/three: 0.5457, eval/one: -4.6472, eval/two: -0.0861, eval/three: 0.8140 \n",
      "Epoch: 30, train/loss: 0.4347, eval/loss: 8.6040, train/r2: 0.3404, eval/r2: -0.6418, train/one: 0.9226, train/two: -0.4485, train/three: 0.5473, eval/one: -2.5335, eval/two: -0.1318, eval/three: 0.7398 \n",
      "Epoch: 35, train/loss: 0.3917, eval/loss: 5.9687, train/r2: 0.3375, eval/r2: -0.4631, train/one: 0.9383, train/two: -0.4757, train/three: 0.5499, eval/one: -1.4076, eval/two: -0.5085, eval/three: 0.5266 \n",
      "Epoch: 40, train/loss: 0.3222, eval/loss: 3.0688, train/r2: 0.3698, eval/r2: -0.0578, train/one: 0.9598, train/two: -0.3910, train/three: 0.5405, eval/one: -0.2013, eval/two: -0.6774, eval/three: 0.7053 \n",
      "Epoch: 45, train/loss: 0.3902, eval/loss: 1.9578, train/r2: 0.3650, eval/r2: 0.2074, train/one: 0.9371, train/two: -0.3070, train/three: 0.4649, eval/one: 0.2480, eval/two: -0.4053, eval/three: 0.7797 \n",
      "Epoch: 50, train/loss: 0.3365, eval/loss: 1.2232, train/r2: 0.3923, eval/r2: 0.2913, train/one: 0.9521, train/two: -0.3250, train/three: 0.5497, eval/one: 0.5612, eval/two: -0.2434, eval/three: 0.5561 \n",
      "Epoch: 55, train/loss: 0.2541, eval/loss: 2.1487, train/r2: 0.4718, eval/r2: 0.2455, train/one: 0.9732, train/two: -0.0924, train/three: 0.5345, eval/one: 0.1593, eval/two: -0.2751, eval/three: 0.8523 \n",
      "Epoch: 60, train/loss: 0.2885, eval/loss: 1.3892, train/r2: 0.5395, eval/r2: 0.3486, train/one: 0.9521, train/two: 0.0353, train/three: 0.6312, eval/one: 0.4765, eval/two: -0.2852, eval/three: 0.8544 \n",
      "Epoch: 65, train/loss: 0.2811, eval/loss: 1.3432, train/r2: 0.4174, eval/r2: 0.3169, train/one: 0.9696, train/two: -0.2416, train/three: 0.5241, eval/one: 0.5017, eval/two: -0.3266, eval/three: 0.7756 \n",
      "Epoch: 70, train/loss: 0.3353, eval/loss: 1.1919, train/r2: 0.4156, eval/r2: 0.3193, train/one: 0.9507, train/two: -0.2087, train/three: 0.5047, eval/one: 0.5675, eval/two: -0.3647, eval/three: 0.7552 \n",
      "Epoch: 75, train/loss: 0.3134, eval/loss: 0.9615, train/r2: 0.4165, eval/r2: 0.4203, train/one: 0.9571, train/two: -0.3036, train/three: 0.5959, eval/one: 0.6534, eval/two: -0.2562, eval/three: 0.8639 \n",
      "Epoch: 80, train/loss: 0.3298, eval/loss: 0.8771, train/r2: 0.4660, eval/r2: 0.4215, train/one: 0.9453, train/two: -0.1810, train/three: 0.6337, eval/one: 0.6902, eval/two: -0.2697, eval/three: 0.8439 \n",
      "Epoch: 85, train/loss: 0.2560, eval/loss: 0.8553, train/r2: 0.5389, eval/r2: 0.4414, train/one: 0.9614, train/two: -0.1397, train/three: 0.7951, eval/one: 0.6974, eval/two: -0.2142, eval/three: 0.8409 \n",
      "Epoch: 90, train/loss: 0.2413, eval/loss: 0.8306, train/r2: 0.5693, eval/r2: 0.3338, train/one: 0.9650, train/two: 0.0619, train/three: 0.6809, eval/one: 0.7250, eval/two: -0.3503, eval/three: 0.6265 \n",
      "Epoch: 95, train/loss: 0.3067, eval/loss: 0.7990, train/r2: 0.5376, eval/r2: 0.4518, train/one: 0.9461, train/two: 0.0558, train/three: 0.6108, eval/one: 0.7202, eval/two: -0.2246, eval/three: 0.8598 \n",
      "Epoch: 100, train/loss: 0.2990, eval/loss: 0.9005, train/r2: 0.4440, eval/r2: 0.3787, train/one: 0.9588, train/two: -0.2512, train/three: 0.6245, eval/one: 0.6862, eval/two: -0.3385, eval/three: 0.7885 \n",
      "Epoch: 105, train/loss: 0.2657, eval/loss: 1.3525, train/r2: 0.5362, eval/r2: 0.2845, train/one: 0.9608, train/two: 0.0289, train/three: 0.6189, eval/one: 0.5038, eval/two: -0.3097, eval/three: 0.6595 \n",
      "Epoch: 110, train/loss: 0.3009, eval/loss: 0.9170, train/r2: 0.4776, eval/r2: 0.3465, train/one: 0.9546, train/two: -0.1395, train/three: 0.6177, eval/one: 0.6879, eval/two: -0.1856, eval/three: 0.5372 \n",
      "Epoch: 115, train/loss: 0.2689, eval/loss: 0.9131, train/r2: 0.5723, eval/r2: 0.4574, train/one: 0.9557, train/two: 0.1411, train/three: 0.6200, eval/one: 0.6716, eval/two: -0.0864, eval/three: 0.7870 \n",
      "Epoch: 120, train/loss: 0.2308, eval/loss: 0.9262, train/r2: 0.5727, eval/r2: 0.4038, train/one: 0.9701, train/two: 0.1760, train/three: 0.5720, eval/one: 0.6746, eval/two: -0.1303, eval/three: 0.6672 \n",
      "Epoch: 125, train/loss: 0.2489, eval/loss: 0.9116, train/r2: 0.6075, eval/r2: 0.4198, train/one: 0.9577, train/two: 0.1502, train/three: 0.7146, eval/one: 0.6788, eval/two: -0.0997, eval/three: 0.6802 \n",
      "Epoch: 130, train/loss: 0.2845, eval/loss: 0.8621, train/r2: 0.5744, eval/r2: 0.5335, train/one: 0.9493, train/two: 0.1142, train/three: 0.6598, eval/one: 0.6835, eval/two: 0.0646, eval/three: 0.8522 \n",
      "Epoch: 135, train/loss: 0.3003, eval/loss: 1.1656, train/r2: 0.6003, eval/r2: 0.4911, train/one: 0.9403, train/two: 0.1603, train/three: 0.7002, eval/one: 0.5585, eval/two: 0.1405, eval/three: 0.7742 \n",
      "Epoch: 140, train/loss: 0.2912, eval/loss: 0.7231, train/r2: 0.5980, eval/r2: 0.5923, train/one: 0.9445, train/two: 0.1963, train/three: 0.6531, eval/one: 0.7371, eval/two: 0.1961, eval/three: 0.8437 \n",
      "Epoch: 145, train/loss: 0.2166, eval/loss: 0.8822, train/r2: 0.6371, eval/r2: 0.5198, train/one: 0.9663, train/two: 0.2465, train/three: 0.6986, eval/one: 0.6814, eval/two: 0.2693, eval/three: 0.6087 \n",
      "Epoch: 150, train/loss: 0.2224, eval/loss: 0.7943, train/r2: 0.6015, eval/r2: 0.5936, train/one: 0.9684, train/two: 0.1568, train/three: 0.6795, eval/one: 0.7080, eval/two: 0.3215, eval/three: 0.7512 \n",
      "Epoch: 155, train/loss: 0.1867, eval/loss: 0.7709, train/r2: 0.7187, eval/r2: 0.5243, train/one: 0.9674, train/two: 0.4428, train/three: 0.7458, eval/one: 0.7339, eval/two: 0.4649, eval/three: 0.3741 \n",
      "Epoch: 160, train/loss: 0.2599, eval/loss: 1.0041, train/r2: 0.7321, eval/r2: 0.5840, train/one: 0.9400, train/two: 0.5309, train/three: 0.7253, eval/one: 0.6205, eval/two: 0.4914, eval/three: 0.6400 \n",
      "Epoch: 165, train/loss: 0.1948, eval/loss: 0.9086, train/r2: 0.7271, eval/r2: 0.5686, train/one: 0.9641, train/two: 0.5073, train/three: 0.7098, eval/one: 0.6670, eval/two: 0.5642, eval/three: 0.4747 \n",
      "Epoch: 170, train/loss: 0.2519, eval/loss: 0.8130, train/r2: 0.6794, eval/r2: 0.6409, train/one: 0.9500, train/two: 0.4572, train/three: 0.6308, eval/one: 0.6968, eval/two: 0.5910, eval/three: 0.6350 \n",
      "Epoch: 175, train/loss: 0.1549, eval/loss: 0.8883, train/r2: 0.7461, eval/r2: 0.6423, train/one: 0.9766, train/two: 0.5672, train/three: 0.6944, eval/one: 0.6654, eval/two: 0.6957, eval/three: 0.5659 \n",
      "Epoch: 180, train/loss: 0.1922, eval/loss: 0.9410, train/r2: 0.7751, eval/r2: 0.7398, train/one: 0.9589, train/two: 0.5949, train/three: 0.7716, eval/one: 0.6250, eval/two: 0.7252, eval/three: 0.8693 \n",
      "Epoch: 185, train/loss: 0.1773, eval/loss: 0.7920, train/r2: 0.7857, eval/r2: 0.7735, train/one: 0.9631, train/two: 0.6171, train/three: 0.7769, eval/one: 0.6861, eval/two: 0.7826, eval/three: 0.8517 \n",
      "Epoch: 190, train/loss: 0.2059, eval/loss: 0.8034, train/r2: 0.7171, eval/r2: 0.7632, train/one: 0.9620, train/two: 0.5323, train/three: 0.6569, eval/one: 0.6824, eval/two: 0.7594, eval/three: 0.8479 \n",
      "Epoch: 195, train/loss: 0.1709, eval/loss: 0.7169, train/r2: 0.7739, eval/r2: 0.7653, train/one: 0.9673, train/two: 0.6217, train/three: 0.7327, eval/one: 0.7208, eval/two: 0.7813, eval/three: 0.7939 \n",
      "Epoch: 200, train/loss: 0.2089, eval/loss: 0.7380, train/r2: 0.7240, eval/r2: 0.7971, train/one: 0.9597, train/two: 0.5221, train/three: 0.6902, eval/one: 0.7063, eval/two: 0.8121, eval/three: 0.8728 \n",
      "Epoch: 205, train/loss: 0.1905, eval/loss: 0.7718, train/r2: 0.7610, eval/r2: 0.7327, train/one: 0.9626, train/two: 0.6507, train/three: 0.6697, eval/one: 0.6997, eval/two: 0.6539, eval/three: 0.8446 \n",
      "Epoch: 210, train/loss: 0.2011, eval/loss: 0.7181, train/r2: 0.7717, eval/r2: 0.7882, train/one: 0.9570, train/two: 0.6482, train/three: 0.7099, eval/one: 0.7163, eval/two: 0.7925, eval/three: 0.8557 \n",
      "Epoch: 215, train/loss: 0.1906, eval/loss: 0.6932, train/r2: 0.7675, eval/r2: 0.7904, train/one: 0.9616, train/two: 0.6560, train/three: 0.6850, eval/one: 0.7271, eval/two: 0.8003, eval/three: 0.8439 \n",
      "Epoch: 220, train/loss: 0.2199, eval/loss: 0.7547, train/r2: 0.7374, eval/r2: 0.7108, train/one: 0.9551, train/two: 0.6216, train/three: 0.6357, eval/one: 0.7092, eval/two: 0.5581, eval/three: 0.8651 \n",
      "Epoch: 225, train/loss: 0.2027, eval/loss: 0.8673, train/r2: 0.7836, eval/r2: 0.7027, train/one: 0.9546, train/two: 0.6469, train/three: 0.7494, eval/one: 0.6616, eval/two: 0.5909, eval/three: 0.8555 \n",
      "Epoch: 230, train/loss: 0.1764, eval/loss: 0.7768, train/r2: 0.7510, eval/r2: 0.7727, train/one: 0.9680, train/two: 0.5682, train/three: 0.7168, eval/one: 0.6929, eval/two: 0.7805, eval/three: 0.8447 \n",
      "Epoch: 235, train/loss: 0.2190, eval/loss: 0.8280, train/r2: 0.7784, eval/r2: 0.6853, train/one: 0.9493, train/two: 0.6397, train/three: 0.7461, eval/one: 0.6809, eval/two: 0.5332, eval/three: 0.8419 \n",
      "Epoch: 240, train/loss: 0.1968, eval/loss: 0.7662, train/r2: 0.7340, eval/r2: 0.6546, train/one: 0.9634, train/two: 0.5744, train/three: 0.6641, eval/one: 0.7130, eval/two: 0.4925, eval/three: 0.7583 \n",
      "Epoch: 245, train/loss: 0.1603, eval/loss: 0.8562, train/r2: 0.8202, eval/r2: 0.4829, train/one: 0.9657, train/two: 0.7390, train/three: 0.7559, eval/one: 0.7016, eval/two: 0.3632, eval/three: 0.3840 \n",
      "Epoch: 250, train/loss: 0.2238, eval/loss: 0.8133, train/r2: 0.7707, eval/r2: 0.7476, train/one: 0.9484, train/two: 0.6124, train/three: 0.7512, eval/one: 0.6818, eval/two: 0.8061, eval/three: 0.7548 \n",
      "Epoch: 255, train/loss: 0.2448, eval/loss: 0.9124, train/r2: 0.7437, eval/r2: 0.7581, train/one: 0.9450, train/two: 0.6142, train/three: 0.6718, eval/one: 0.6367, eval/two: 0.8385, eval/three: 0.7991 \n",
      "Epoch: 260, train/loss: 0.1949, eval/loss: 0.8462, train/r2: 0.7823, eval/r2: 0.7506, train/one: 0.9582, train/two: 0.6837, train/three: 0.7050, eval/one: 0.6673, eval/two: 0.8349, eval/three: 0.7496 \n",
      "Epoch: 265, train/loss: 0.1412, eval/loss: 0.9808, train/r2: 0.7832, eval/r2: 0.7132, train/one: 0.9767, train/two: 0.6230, train/three: 0.7500, eval/one: 0.6119, eval/two: 0.7069, eval/three: 0.8208 \n",
      "Epoch: 270, train/loss: 0.1951, eval/loss: 0.7846, train/r2: 0.7841, eval/r2: 0.7438, train/one: 0.9570, train/two: 0.6265, train/three: 0.7687, eval/one: 0.6946, eval/two: 0.7791, eval/three: 0.7577 \n",
      "Epoch: 275, train/loss: 0.1981, eval/loss: 0.7614, train/r2: 0.7671, eval/r2: 0.7794, train/one: 0.9592, train/two: 0.6726, train/three: 0.6694, eval/one: 0.7002, eval/two: 0.8626, eval/three: 0.7755 \n",
      "Epoch: 280, train/loss: 0.1992, eval/loss: 0.6860, train/r2: 0.7982, eval/r2: 0.8115, train/one: 0.9543, train/two: 0.6938, train/three: 0.7466, eval/one: 0.7277, eval/two: 0.8591, eval/three: 0.8478 \n",
      "Epoch: 285, train/loss: 0.2159, eval/loss: 0.7545, train/r2: 0.7865, eval/r2: 0.8079, train/one: 0.9505, train/two: 0.7265, train/three: 0.6823, eval/one: 0.6980, eval/two: 0.8590, eval/three: 0.8667 \n",
      "Epoch: 290, train/loss: 0.2318, eval/loss: 0.7620, train/r2: 0.7741, eval/r2: 0.7978, train/one: 0.9458, train/two: 0.6698, train/three: 0.7065, eval/one: 0.6959, eval/two: 0.8288, eval/three: 0.8688 \n",
      "Epoch: 295, train/loss: 0.1731, eval/loss: 0.7219, train/r2: 0.8196, eval/r2: 0.7929, train/one: 0.9608, train/two: 0.7181, train/three: 0.7799, eval/one: 0.7148, eval/two: 0.8446, eval/three: 0.8192 \n",
      "Epoch: 300, train/loss: 0.1675, eval/loss: 0.7418, train/r2: 0.7742, eval/r2: 0.7833, train/one: 0.9693, train/two: 0.6755, train/three: 0.6778, eval/one: 0.7081, eval/two: 0.8614, eval/three: 0.7803 \n",
      "Epoch: 305, train/loss: 0.1620, eval/loss: 0.7070, train/r2: 0.7934, eval/r2: 0.7812, train/one: 0.9673, train/two: 0.6108, train/three: 0.8021, eval/one: 0.7242, eval/two: 0.8750, eval/three: 0.7444 \n",
      "Epoch: 310, train/loss: 0.1842, eval/loss: 0.7377, train/r2: 0.7942, eval/r2: 0.7714, train/one: 0.9607, train/two: 0.7133, train/three: 0.7085, eval/one: 0.7105, eval/two: 0.7817, eval/three: 0.8221 \n",
      "Epoch: 315, train/loss: 0.2041, eval/loss: 0.7879, train/r2: 0.8253, eval/r2: 0.6974, train/one: 0.9500, train/two: 0.8064, train/three: 0.7195, eval/one: 0.6988, eval/two: 0.6430, eval/three: 0.7504 \n",
      "Epoch: 320, train/loss: 0.1897, eval/loss: 0.8146, train/r2: 0.7538, eval/r2: 0.7034, train/one: 0.9630, train/two: 0.5828, train/three: 0.7157, eval/one: 0.6858, eval/two: 0.6439, eval/three: 0.7804 \n",
      "Epoch: 325, train/loss: 0.2031, eval/loss: 0.8436, train/r2: 0.8240, eval/r2: 0.7511, train/one: 0.9497, train/two: 0.7538, train/three: 0.7686, eval/one: 0.6673, eval/two: 0.7847, eval/three: 0.8014 \n",
      "Epoch: 330, train/loss: 0.1889, eval/loss: 0.8217, train/r2: 0.7768, eval/r2: 0.7429, train/one: 0.9616, train/two: 0.7104, train/three: 0.6585, eval/one: 0.6792, eval/two: 0.8155, eval/three: 0.7340 \n",
      "Epoch: 335, train/loss: 0.1843, eval/loss: 0.8257, train/r2: 0.7649, eval/r2: 0.7241, train/one: 0.9641, train/two: 0.6438, train/three: 0.6868, eval/one: 0.6785, eval/two: 0.7050, eval/three: 0.7890 \n",
      "Epoch: 340, train/loss: 0.1685, eval/loss: 0.7843, train/r2: 0.7621, eval/r2: 0.7477, train/one: 0.9696, train/two: 0.5948, train/three: 0.7217, eval/one: 0.6930, eval/two: 0.7291, eval/three: 0.8208 \n",
      "Epoch: 345, train/loss: 0.1384, eval/loss: 0.7239, train/r2: 0.8068, eval/r2: 0.7669, train/one: 0.9758, train/two: 0.7414, train/three: 0.7033, eval/one: 0.7176, eval/two: 0.7927, eval/three: 0.7903 \n",
      "Epoch: 350, train/loss: 0.1542, eval/loss: 0.7438, train/r2: 0.7984, eval/r2: 0.7949, train/one: 0.9700, train/two: 0.6501, train/three: 0.7750, eval/one: 0.7051, eval/two: 0.8588, eval/three: 0.8209 \n",
      "Epoch: 355, train/loss: 0.1482, eval/loss: 0.7955, train/r2: 0.8362, eval/r2: 0.7910, train/one: 0.9678, train/two: 0.7520, train/three: 0.7888, eval/one: 0.6825, eval/two: 0.8406, eval/three: 0.8498 \n",
      "Epoch: 360, train/loss: 0.2342, eval/loss: 0.7731, train/r2: 0.8134, eval/r2: 0.8084, train/one: 0.9399, train/two: 0.7398, train/three: 0.7604, eval/one: 0.6895, eval/two: 0.8520, eval/three: 0.8836 \n",
      "Epoch: 365, train/loss: 0.1766, eval/loss: 0.7561, train/r2: 0.7581, eval/r2: 0.8153, train/one: 0.9682, train/two: 0.6612, train/three: 0.6448, eval/one: 0.6961, eval/two: 0.8658, eval/three: 0.8842 \n",
      "Epoch: 370, train/loss: 0.1332, eval/loss: 0.7174, train/r2: 0.8450, eval/r2: 0.8195, train/one: 0.9717, train/two: 0.7400, train/three: 0.8233, eval/one: 0.7128, eval/two: 0.8802, eval/three: 0.8655 \n",
      "Epoch: 375, train/loss: 0.1611, eval/loss: 0.6952, train/r2: 0.8183, eval/r2: 0.8138, train/one: 0.9656, train/two: 0.7300, train/three: 0.7593, eval/one: 0.7236, eval/two: 0.8788, eval/three: 0.8389 \n",
      "Epoch: 380, train/loss: 0.2351, eval/loss: 0.6817, train/r2: 0.7753, eval/r2: 0.8146, train/one: 0.9442, train/two: 0.6530, train/three: 0.7287, eval/one: 0.7292, eval/two: 0.8660, eval/three: 0.8485 \n",
      "Epoch: 385, train/loss: 0.1934, eval/loss: 0.7153, train/r2: 0.7767, eval/r2: 0.8097, train/one: 0.9592, train/two: 0.6555, train/three: 0.7154, eval/one: 0.7153, eval/two: 0.8729, eval/three: 0.8409 \n",
      "Epoch: 390, train/loss: 0.1736, eval/loss: 0.7537, train/r2: 0.7830, eval/r2: 0.8088, train/one: 0.9652, train/two: 0.6427, train/three: 0.7410, eval/one: 0.6989, eval/two: 0.8928, eval/three: 0.8347 \n",
      "Epoch: 395, train/loss: 0.1803, eval/loss: 0.7682, train/r2: 0.8498, eval/r2: 0.8092, train/one: 0.9547, train/two: 0.7963, train/three: 0.7985, eval/one: 0.6925, eval/two: 0.8957, eval/three: 0.8394 \n",
      "Epoch: 400, train/loss: 0.1857, eval/loss: 0.7189, train/r2: 0.8049, eval/r2: 0.8111, train/one: 0.9592, train/two: 0.7640, train/three: 0.6914, eval/one: 0.7136, eval/two: 0.8811, eval/three: 0.8385 \n",
      "Epoch: 405, train/loss: 0.1756, eval/loss: 0.7000, train/r2: 0.7986, eval/r2: 0.8005, train/one: 0.9622, train/two: 0.6526, train/three: 0.7809, eval/one: 0.7235, eval/two: 0.8605, eval/three: 0.8175 \n",
      "Epoch: 410, train/loss: 0.1808, eval/loss: 0.6940, train/r2: 0.8198, eval/r2: 0.7973, train/one: 0.9591, train/two: 0.7895, train/three: 0.7110, eval/one: 0.7262, eval/two: 0.8338, eval/three: 0.8320 \n",
      "Epoch: 415, train/loss: 0.2313, eval/loss: 0.7076, train/r2: 0.7779, eval/r2: 0.7975, train/one: 0.9455, train/two: 0.6765, train/three: 0.7116, eval/one: 0.7202, eval/two: 0.8409, eval/three: 0.8314 \n",
      "Epoch: 420, train/loss: 0.1775, eval/loss: 0.7133, train/r2: 0.8079, eval/r2: 0.7949, train/one: 0.9609, train/two: 0.7108, train/three: 0.7522, eval/one: 0.7183, eval/two: 0.8437, eval/three: 0.8226 \n",
      "Epoch: 425, train/loss: 0.1742, eval/loss: 0.7291, train/r2: 0.8327, eval/r2: 0.8023, train/one: 0.9592, train/two: 0.7688, train/three: 0.7701, eval/one: 0.7104, eval/two: 0.8644, eval/three: 0.8322 \n",
      "Epoch: 430, train/loss: 0.1656, eval/loss: 0.7439, train/r2: 0.8177, eval/r2: 0.8041, train/one: 0.9639, train/two: 0.7208, train/three: 0.7683, eval/one: 0.7036, eval/two: 0.8716, eval/three: 0.8369 \n",
      "Epoch: 435, train/loss: 0.1664, eval/loss: 0.7501, train/r2: 0.8307, eval/r2: 0.8053, train/one: 0.9618, train/two: 0.7314, train/three: 0.7988, eval/one: 0.7007, eval/two: 0.8736, eval/three: 0.8417 \n",
      "Epoch: 440, train/loss: 0.1744, eval/loss: 0.7441, train/r2: 0.8010, eval/r2: 0.8043, train/one: 0.9628, train/two: 0.6899, train/three: 0.7503, eval/one: 0.7035, eval/two: 0.8675, eval/three: 0.8418 \n",
      "Epoch: 445, train/loss: 0.2039, eval/loss: 0.7445, train/r2: 0.7679, eval/r2: 0.7992, train/one: 0.9570, train/two: 0.6744, train/three: 0.6724, eval/one: 0.7040, eval/two: 0.8595, eval/three: 0.8341 \n",
      "Epoch: 450, train/loss: 0.1590, eval/loss: 0.7435, train/r2: 0.7695, eval/r2: 0.7956, train/one: 0.9719, train/two: 0.5982, train/three: 0.7385, eval/one: 0.7051, eval/two: 0.8577, eval/three: 0.8242 \n",
      "Epoch: 455, train/loss: 0.1698, eval/loss: 0.7396, train/r2: 0.8232, eval/r2: 0.7907, train/one: 0.9620, train/two: 0.7533, train/three: 0.7543, eval/one: 0.7076, eval/two: 0.8509, eval/three: 0.8137 \n",
      "Epoch: 460, train/loss: 0.1584, eval/loss: 0.7361, train/r2: 0.7888, eval/r2: 0.7891, train/one: 0.9699, train/two: 0.6448, train/three: 0.7516, eval/one: 0.7094, eval/two: 0.8519, eval/three: 0.8059 \n",
      "Epoch: 465, train/loss: 0.1850, eval/loss: 0.7383, train/r2: 0.7944, eval/r2: 0.7912, train/one: 0.9594, train/two: 0.6509, train/three: 0.7731, eval/one: 0.7082, eval/two: 0.8574, eval/three: 0.8081 \n",
      "Epoch: 470, train/loss: 0.1634, eval/loss: 0.7376, train/r2: 0.8170, eval/r2: 0.7924, train/one: 0.9640, train/two: 0.6707, train/three: 0.8163, eval/one: 0.7083, eval/two: 0.8581, eval/three: 0.8107 \n",
      "Epoch: 475, train/loss: 0.1532, eval/loss: 0.7377, train/r2: 0.8463, eval/r2: 0.7941, train/one: 0.9644, train/two: 0.7505, train/three: 0.8239, eval/one: 0.7079, eval/two: 0.8592, eval/three: 0.8153 \n",
      "Epoch: 480, train/loss: 0.1820, eval/loss: 0.7383, train/r2: 0.7972, eval/r2: 0.7954, train/one: 0.9600, train/two: 0.6430, train/three: 0.7885, eval/one: 0.7075, eval/two: 0.8598, eval/three: 0.8191 \n",
      "Epoch: 485, train/loss: 0.1990, eval/loss: 0.7389, train/r2: 0.7637, eval/r2: 0.7969, train/one: 0.9596, train/two: 0.6839, train/three: 0.6477, eval/one: 0.7069, eval/two: 0.8606, eval/three: 0.8231 \n",
      "Epoch: 490, train/loss: 0.1651, eval/loss: 0.7394, train/r2: 0.7532, eval/r2: 0.7976, train/one: 0.9713, train/two: 0.5348, train/three: 0.7534, eval/one: 0.7066, eval/two: 0.8605, eval/three: 0.8257 \n",
      "Epoch: 495, train/loss: 0.1623, eval/loss: 0.7397, train/r2: 0.8351, eval/r2: 0.7979, train/one: 0.9624, train/two: 0.7193, train/three: 0.8236, eval/one: 0.7064, eval/two: 0.8600, eval/three: 0.8273 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 110 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 110 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-120/metadata\n",
      "0.48822867684523436\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-121\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06427e5a008042f2bd7291930c1db64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 4.7801, eval/loss: 28.8829, train/r2: -0.5837, eval/r2: -31.6682, train/one: -0.5603, train/two: -1.0489, train/three: -0.1420, eval/one: -6.6087, eval/two: -0.0139, eval/three: -88.3819 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train/loss: 3.5225, eval/loss: 6.0692, train/r2: -0.2915, eval/r2: -9.6195, train/one: -0.1352, train/two: -0.8485, train/three: 0.1092, eval/one: -0.1963, eval/two: -0.0469, eval/three: -28.6152 \n",
      "Epoch: 10, train/loss: 1.2741, eval/loss: 19.6431, train/r2: 0.0374, eval/r2: -6.0820, train/one: 0.6520, train/two: -0.8093, train/three: 0.2696, eval/one: -5.7810, eval/two: -0.0575, eval/three: -12.4076 \n",
      "Epoch: 15, train/loss: 0.5009, eval/loss: 28.5529, train/r2: 0.2746, eval/r2: -5.3486, train/one: 0.9084, train/two: -0.4504, train/three: 0.3659, eval/one: -9.2780, eval/two: -0.0583, eval/three: -6.7095 \n",
      "Epoch: 20, train/loss: 0.5736, eval/loss: 19.3934, train/r2: 0.2775, eval/r2: -3.3658, train/one: 0.8790, train/two: -0.5199, train/three: 0.4734, eval/one: -5.9724, eval/two: -0.3712, eval/three: -3.7539 \n",
      "Epoch: 25, train/loss: 0.5006, eval/loss: 15.0217, train/r2: 0.2015, eval/r2: -1.5536, train/one: 0.9179, train/two: -0.6134, train/three: 0.3000, eval/one: -4.4877, eval/two: -0.2731, eval/three: 0.0999 \n",
      "Epoch: 30, train/loss: 0.4989, eval/loss: 7.3324, train/r2: 0.2118, eval/r2: -0.3834, train/one: 0.9147, train/two: -0.6988, train/three: 0.4195, eval/one: -1.6624, eval/two: -0.0106, eval/three: 0.5230 \n",
      "Epoch: 35, train/loss: 0.3800, eval/loss: 1.7749, train/r2: 0.3173, eval/r2: -0.0807, train/one: 0.9458, train/two: -0.4693, train/three: 0.4753, eval/one: 0.4381, eval/two: -0.0068, eval/three: -0.6733 \n",
      "Epoch: 40, train/loss: 0.3383, eval/loss: 0.5945, train/r2: 0.3752, eval/r2: 0.5528, train/one: 0.9498, train/two: -0.5262, train/three: 0.7021, eval/one: 0.8237, eval/two: -0.0033, eval/three: 0.8380 \n",
      "Epoch: 45, train/loss: 0.3436, eval/loss: 0.3772, train/r2: 0.4066, eval/r2: 0.5625, train/one: 0.9495, train/two: -0.2040, train/three: 0.4744, eval/one: 0.9060, eval/two: 0.0271, eval/three: 0.7545 \n",
      "Epoch: 50, train/loss: 0.3790, eval/loss: 0.5307, train/r2: 0.4251, eval/r2: 0.5923, train/one: 0.9338, train/two: -0.1649, train/three: 0.5066, eval/one: 0.8439, eval/two: 0.0199, eval/three: 0.9133 \n",
      "Epoch: 55, train/loss: 0.3635, eval/loss: 0.7496, train/r2: 0.3142, eval/r2: 0.5718, train/one: 0.9502, train/two: -0.5709, train/three: 0.5633, eval/one: 0.7620, eval/two: 0.0340, eval/three: 0.9194 \n",
      "Epoch: 60, train/loss: 0.3156, eval/loss: 0.3547, train/r2: 0.4997, eval/r2: 0.6270, train/one: 0.9466, train/two: -0.0623, train/three: 0.6147, eval/one: 0.9077, eval/two: 0.0445, eval/three: 0.9287 \n",
      "Epoch: 65, train/loss: 0.3207, eval/loss: 0.3634, train/r2: 0.3789, eval/r2: 0.6016, train/one: 0.9584, train/two: -0.4095, train/three: 0.5880, eval/one: 0.9071, eval/two: 0.0329, eval/three: 0.8649 \n",
      "Epoch: 70, train/loss: 0.3037, eval/loss: 0.3300, train/r2: 0.4634, eval/r2: 0.6019, train/one: 0.9538, train/two: -0.2303, train/three: 0.6668, eval/one: 0.9199, eval/two: 0.0251, eval/three: 0.8607 \n",
      "Epoch: 75, train/loss: 0.3123, eval/loss: 0.3550, train/r2: 0.2605, eval/r2: 0.6182, train/one: 0.9733, train/two: -0.8253, train/three: 0.6334, eval/one: 0.9086, eval/two: 0.0212, eval/three: 0.9248 \n",
      "Epoch: 80, train/loss: 0.3547, eval/loss: 0.4446, train/r2: 0.4025, eval/r2: 0.5833, train/one: 0.9432, train/two: -0.3315, train/three: 0.5960, eval/one: 0.8778, eval/two: 0.0333, eval/three: 0.8387 \n",
      "Epoch: 85, train/loss: 0.3399, eval/loss: 0.6055, train/r2: 0.3792, eval/r2: 0.5381, train/one: 0.9518, train/two: -0.3778, train/three: 0.5636, eval/one: 0.8208, eval/two: 0.0673, eval/three: 0.7262 \n",
      "Epoch: 90, train/loss: 0.2882, eval/loss: 0.3266, train/r2: 0.5196, eval/r2: 0.6197, train/one: 0.9537, train/two: -0.0478, train/three: 0.6528, eval/one: 0.9192, eval/two: 0.0414, eval/three: 0.8986 \n",
      "Epoch: 95, train/loss: 0.2566, eval/loss: 0.7408, train/r2: 0.5286, eval/r2: 0.5347, train/one: 0.9626, train/two: -0.1076, train/three: 0.7306, eval/one: 0.7694, eval/two: 0.0234, eval/three: 0.8115 \n",
      "Epoch: 100, train/loss: 0.2602, eval/loss: 0.3594, train/r2: 0.5068, eval/r2: 0.6163, train/one: 0.9626, train/two: -0.2226, train/three: 0.7804, eval/one: 0.9068, eval/two: 0.0961, eval/three: 0.8461 \n",
      "Epoch: 105, train/loss: 0.3070, eval/loss: 0.2941, train/r2: 0.4363, eval/r2: 0.6385, train/one: 0.9557, train/two: -0.3057, train/three: 0.6588, eval/one: 0.9294, eval/two: 0.1197, eval/three: 0.8663 \n",
      "Epoch: 110, train/loss: 0.3346, eval/loss: 0.6019, train/r2: 0.3635, eval/r2: 0.5838, train/one: 0.9537, train/two: -0.5080, train/three: 0.6448, eval/one: 0.8171, eval/two: 0.1032, eval/three: 0.8311 \n",
      "Epoch: 115, train/loss: 0.3243, eval/loss: 0.5956, train/r2: 0.5354, eval/r2: 0.5776, train/one: 0.9388, train/two: 0.0197, train/three: 0.6477, eval/one: 0.8201, eval/two: 0.1329, eval/three: 0.7797 \n",
      "Epoch: 120, train/loss: 0.2713, eval/loss: 0.4914, train/r2: 0.5550, eval/r2: 0.4729, train/one: 0.9563, train/two: 0.0669, train/three: 0.6419, eval/one: 0.8710, eval/two: 0.1768, eval/three: 0.3710 \n",
      "Epoch: 125, train/loss: 0.3078, eval/loss: 0.5343, train/r2: 0.4675, eval/r2: 0.6636, train/one: 0.9513, train/two: -0.2373, train/three: 0.6884, eval/one: 0.8340, eval/two: 0.2421, eval/three: 0.9146 \n",
      "Epoch: 130, train/loss: 0.2606, eval/loss: 0.3517, train/r2: 0.5048, eval/r2: 0.6588, train/one: 0.9644, train/two: -0.1505, train/three: 0.7005, eval/one: 0.9049, eval/two: 0.1688, eval/three: 0.9028 \n",
      "Epoch: 135, train/loss: 0.2764, eval/loss: 0.5560, train/r2: 0.5812, eval/r2: 0.6307, train/one: 0.9527, train/two: 0.1997, train/three: 0.5912, eval/one: 0.8292, eval/two: 0.2396, eval/three: 0.8233 \n",
      "Epoch: 140, train/loss: 0.2555, eval/loss: 0.6244, train/r2: 0.5944, eval/r2: 0.6213, train/one: 0.9584, train/two: 0.2071, train/three: 0.6177, eval/one: 0.8038, eval/two: 0.2725, eval/three: 0.7875 \n",
      "Epoch: 145, train/loss: 0.2893, eval/loss: 0.3021, train/r2: 0.6236, eval/r2: 0.6905, train/one: 0.9406, train/two: 0.2115, train/three: 0.7188, eval/one: 0.9197, eval/two: 0.3554, eval/three: 0.7964 \n",
      "Epoch: 150, train/loss: 0.1942, eval/loss: 0.3626, train/r2: 0.7094, eval/r2: 0.6757, train/one: 0.9664, train/two: 0.4492, train/three: 0.7124, eval/one: 0.8983, eval/two: 0.3155, eval/three: 0.8133 \n",
      "Epoch: 155, train/loss: 0.2040, eval/loss: 0.5709, train/r2: 0.6846, eval/r2: 0.6831, train/one: 0.9656, train/two: 0.3808, train/three: 0.7075, eval/one: 0.8173, eval/two: 0.3669, eval/three: 0.8649 \n",
      "Epoch: 160, train/loss: 0.2602, eval/loss: 0.3446, train/r2: 0.6706, eval/r2: 0.7265, train/one: 0.9480, train/two: 0.4245, train/three: 0.6394, eval/one: 0.8995, eval/two: 0.3941, eval/three: 0.8860 \n",
      "Epoch: 165, train/loss: 0.2506, eval/loss: 0.3179, train/r2: 0.7345, eval/r2: 0.7487, train/one: 0.9419, train/two: 0.4998, train/three: 0.7619, eval/one: 0.9063, eval/two: 0.6242, eval/three: 0.7156 \n",
      "Epoch: 170, train/loss: 0.2235, eval/loss: 0.3484, train/r2: 0.7286, eval/r2: 0.6275, train/one: 0.9538, train/two: 0.5316, train/three: 0.7005, eval/one: 0.9097, eval/two: 0.1156, eval/three: 0.8572 \n",
      "Epoch: 175, train/loss: 0.2034, eval/loss: 0.4251, train/r2: 0.7215, eval/r2: 0.7480, train/one: 0.9617, train/two: 0.4897, train/three: 0.7131, eval/one: 0.8653, eval/two: 0.6184, eval/three: 0.7604 \n",
      "Epoch: 180, train/loss: 0.2103, eval/loss: 0.2636, train/r2: 0.7537, eval/r2: 0.8117, train/one: 0.9549, train/two: 0.5594, train/three: 0.7468, eval/one: 0.9204, eval/two: 0.6534, eval/three: 0.8613 \n",
      "Epoch: 185, train/loss: 0.1930, eval/loss: 0.3365, train/r2: 0.7310, eval/r2: 0.6874, train/one: 0.9645, train/two: 0.5150, train/three: 0.7134, eval/one: 0.9072, eval/two: 0.2838, eval/three: 0.8712 \n",
      "Epoch: 190, train/loss: 0.1744, eval/loss: 0.3087, train/r2: 0.7784, eval/r2: 0.7377, train/one: 0.9649, train/two: 0.5986, train/three: 0.7718, eval/one: 0.9119, eval/two: 0.4229, eval/three: 0.8785 \n",
      "Epoch: 195, train/loss: 0.2401, eval/loss: 0.3115, train/r2: 0.7339, eval/r2: 0.8395, train/one: 0.9455, train/two: 0.4823, train/three: 0.7740, eval/one: 0.8986, eval/two: 0.7571, eval/three: 0.8627 \n",
      "Epoch: 200, train/loss: 0.2104, eval/loss: 0.6573, train/r2: 0.7436, eval/r2: 0.7817, train/one: 0.9556, train/two: 0.5102, train/three: 0.7650, eval/one: 0.7719, eval/two: 0.7866, eval/three: 0.7866 \n",
      "Epoch: 205, train/loss: 0.1988, eval/loss: 0.7806, train/r2: 0.7261, eval/r2: 0.7669, train/one: 0.9636, train/two: 0.5357, train/three: 0.6790, eval/one: 0.7265, eval/two: 0.7047, eval/three: 0.8696 \n",
      "Epoch: 210, train/loss: 0.2989, eval/loss: 0.4366, train/r2: 0.7192, eval/r2: 0.8045, train/one: 0.9277, train/two: 0.5543, train/three: 0.6757, eval/one: 0.8545, eval/two: 0.7096, eval/three: 0.8495 \n",
      "Epoch: 215, train/loss: 0.1683, eval/loss: 0.3293, train/r2: 0.7600, eval/r2: 0.7812, train/one: 0.9706, train/two: 0.6020, train/three: 0.7075, eval/one: 0.8986, eval/two: 0.6084, eval/three: 0.8367 \n",
      "Epoch: 220, train/loss: 0.1882, eval/loss: 0.2656, train/r2: 0.7566, eval/r2: 0.8471, train/one: 0.9634, train/two: 0.5907, train/three: 0.7158, eval/one: 0.9154, eval/two: 0.7361, eval/three: 0.8899 \n",
      "Epoch: 225, train/loss: 0.1812, eval/loss: 0.2363, train/r2: 0.7520, eval/r2: 0.8660, train/one: 0.9669, train/two: 0.5917, train/three: 0.6975, eval/one: 0.9244, eval/two: 0.8059, eval/three: 0.8676 \n",
      "Epoch: 230, train/loss: 0.2047, eval/loss: 0.2764, train/r2: 0.7688, eval/r2: 0.8275, train/one: 0.9564, train/two: 0.6506, train/three: 0.6995, eval/one: 0.9136, eval/two: 0.6850, eval/three: 0.8838 \n",
      "Epoch: 235, train/loss: 0.2221, eval/loss: 0.3502, train/r2: 0.7621, eval/r2: 0.8461, train/one: 0.9520, train/two: 0.6887, train/three: 0.6456, eval/one: 0.8830, eval/two: 0.7670, eval/three: 0.8882 \n",
      "Epoch: 240, train/loss: 0.1837, eval/loss: 0.3017, train/r2: 0.7464, eval/r2: 0.8330, train/one: 0.9670, train/two: 0.5911, train/three: 0.6812, eval/one: 0.9031, eval/two: 0.7413, eval/three: 0.8546 \n",
      "Epoch: 245, train/loss: 0.2293, eval/loss: 0.3255, train/r2: 0.7748, eval/r2: 0.8073, train/one: 0.9448, train/two: 0.5941, train/three: 0.7854, eval/one: 0.8968, eval/two: 0.7004, eval/three: 0.8246 \n",
      "Epoch: 250, train/loss: 0.1905, eval/loss: 0.2769, train/r2: 0.7931, eval/r2: 0.7997, train/one: 0.9583, train/two: 0.6906, train/three: 0.7303, eval/one: 0.9167, eval/two: 0.6132, eval/three: 0.8692 \n",
      "Epoch: 255, train/loss: 0.1670, eval/loss: 0.3233, train/r2: 0.8085, eval/r2: 0.8080, train/one: 0.9657, train/two: 0.7468, train/three: 0.7129, eval/one: 0.8975, eval/two: 0.7448, eval/three: 0.7816 \n",
      "Epoch: 260, train/loss: 0.2110, eval/loss: 0.4260, train/r2: 0.7609, eval/r2: 0.8373, train/one: 0.9545, train/two: 0.6094, train/three: 0.7189, eval/one: 0.8547, eval/two: 0.8011, eval/three: 0.8561 \n",
      "Epoch: 265, train/loss: 0.1887, eval/loss: 0.2947, train/r2: 0.7926, eval/r2: 0.8440, train/one: 0.9589, train/two: 0.6812, train/three: 0.7377, eval/one: 0.9044, eval/two: 0.7909, eval/three: 0.8365 \n",
      "Epoch: 270, train/loss: 0.2144, eval/loss: 0.3827, train/r2: 0.7301, eval/r2: 0.8496, train/one: 0.9579, train/two: 0.5731, train/three: 0.6593, eval/one: 0.8699, eval/two: 0.8270, eval/three: 0.8518 \n",
      "Epoch: 275, train/loss: 0.1885, eval/loss: 0.3286, train/r2: 0.7741, eval/r2: 0.8564, train/one: 0.9599, train/two: 0.5749, train/three: 0.7875, eval/one: 0.8898, eval/two: 0.8414, eval/three: 0.8378 \n",
      "Epoch: 280, train/loss: 0.2150, eval/loss: 0.2379, train/r2: 0.7235, eval/r2: 0.8784, train/one: 0.9580, train/two: 0.5346, train/three: 0.6780, eval/one: 0.9224, eval/two: 0.8257, eval/three: 0.8870 \n",
      "Epoch: 285, train/loss: 0.2056, eval/loss: 0.2712, train/r2: 0.7583, eval/r2: 0.8725, train/one: 0.9573, train/two: 0.6192, train/three: 0.6984, eval/one: 0.9102, eval/two: 0.8382, eval/three: 0.8691 \n",
      "Epoch: 290, train/loss: 0.1825, eval/loss: 0.2580, train/r2: 0.7621, eval/r2: 0.8689, train/one: 0.9646, train/two: 0.5920, train/three: 0.7296, eval/one: 0.9157, eval/two: 0.8223, eval/three: 0.8688 \n",
      "Epoch: 295, train/loss: 0.2171, eval/loss: 0.3029, train/r2: 0.7912, eval/r2: 0.7402, train/one: 0.9484, train/two: 0.6781, train/three: 0.7472, eval/one: 0.9137, eval/two: 0.4523, eval/three: 0.8546 \n",
      "Epoch: 300, train/loss: 0.2187, eval/loss: 0.2677, train/r2: 0.7682, eval/r2: 0.8421, train/one: 0.9489, train/two: 0.5451, train/three: 0.8105, eval/one: 0.9152, eval/two: 0.7323, eval/three: 0.8788 \n",
      "Epoch: 305, train/loss: 0.1806, eval/loss: 0.2518, train/r2: 0.7572, eval/r2: 0.8752, train/one: 0.9663, train/two: 0.5960, train/three: 0.7091, eval/one: 0.9174, eval/two: 0.8270, eval/three: 0.8812 \n",
      "Epoch: 310, train/loss: 0.1875, eval/loss: 0.3096, train/r2: 0.7992, eval/r2: 0.8631, train/one: 0.9579, train/two: 0.6719, train/three: 0.7678, eval/one: 0.8964, eval/two: 0.8444, eval/three: 0.8484 \n",
      "Epoch: 315, train/loss: 0.1995, eval/loss: 0.3271, train/r2: 0.7260, eval/r2: 0.7957, train/one: 0.9639, train/two: 0.5557, train/three: 0.6586, eval/one: 0.8968, eval/two: 0.8494, eval/three: 0.6407 \n",
      "Epoch: 320, train/loss: 0.1305, eval/loss: 0.2733, train/r2: 0.8450, eval/r2: 0.8572, train/one: 0.9730, train/two: 0.7517, train/three: 0.8105, eval/one: 0.9110, eval/two: 0.8497, eval/three: 0.8110 \n",
      "Epoch: 325, train/loss: 0.1731, eval/loss: 0.2459, train/r2: 0.7451, eval/r2: 0.8767, train/one: 0.9713, train/two: 0.5984, train/three: 0.6656, eval/one: 0.9194, eval/two: 0.8368, eval/three: 0.8740 \n",
      "Epoch: 330, train/loss: 0.1677, eval/loss: 0.2634, train/r2: 0.7639, eval/r2: 0.8627, train/one: 0.9699, train/two: 0.5942, train/three: 0.7275, eval/one: 0.9142, eval/two: 0.8318, eval/three: 0.8422 \n",
      "Epoch: 335, train/loss: 0.2020, eval/loss: 0.3592, train/r2: 0.7906, eval/r2: 0.8670, train/one: 0.9536, train/two: 0.6541, train/three: 0.7641, eval/one: 0.8770, eval/two: 0.8484, eval/three: 0.8757 \n",
      "Epoch: 340, train/loss: 0.2004, eval/loss: 0.3606, train/r2: 0.7423, eval/r2: 0.8564, train/one: 0.9601, train/two: 0.5312, train/three: 0.7356, eval/one: 0.8776, eval/two: 0.8426, eval/three: 0.8489 \n",
      "Epoch: 345, train/loss: 0.1847, eval/loss: 0.2776, train/r2: 0.7875, eval/r2: 0.8740, train/one: 0.9607, train/two: 0.6571, train/three: 0.7447, eval/one: 0.9075, eval/two: 0.8487, eval/three: 0.8658 \n",
      "Epoch: 350, train/loss: 0.2245, eval/loss: 0.2681, train/r2: 0.7791, eval/r2: 0.8720, train/one: 0.9487, train/two: 0.7209, train/three: 0.6678, eval/one: 0.9114, eval/two: 0.8426, eval/three: 0.8620 \n",
      "Epoch: 355, train/loss: 0.1671, eval/loss: 0.3054, train/r2: 0.8087, eval/r2: 0.8467, train/one: 0.9641, train/two: 0.6805, train/three: 0.7815, eval/one: 0.8999, eval/two: 0.8077, eval/three: 0.8325 \n",
      "Epoch: 360, train/loss: 0.1630, eval/loss: 0.2737, train/r2: 0.8226, eval/r2: 0.8749, train/one: 0.9649, train/two: 0.7558, train/three: 0.7472, eval/one: 0.9089, eval/two: 0.8473, eval/three: 0.8684 \n",
      "Epoch: 365, train/loss: 0.1710, eval/loss: 0.2562, train/r2: 0.8235, eval/r2: 0.8765, train/one: 0.9602, train/two: 0.6884, train/three: 0.8220, eval/one: 0.9155, eval/two: 0.8368, eval/three: 0.8771 \n",
      "Epoch: 370, train/loss: 0.1563, eval/loss: 0.2770, train/r2: 0.7915, eval/r2: 0.8731, train/one: 0.9711, train/two: 0.6744, train/three: 0.7289, eval/one: 0.9079, eval/two: 0.8313, eval/three: 0.8799 \n",
      "Epoch: 375, train/loss: 0.1767, eval/loss: 0.3510, train/r2: 0.8011, eval/r2: 0.8644, train/one: 0.9626, train/two: 0.7141, train/three: 0.7266, eval/one: 0.8805, eval/two: 0.8314, eval/three: 0.8815 \n",
      "Epoch: 380, train/loss: 0.1933, eval/loss: 0.3417, train/r2: 0.7818, eval/r2: 0.8656, train/one: 0.9583, train/two: 0.6480, train/three: 0.7392, eval/one: 0.8839, eval/two: 0.8272, eval/three: 0.8857 \n",
      "Epoch: 385, train/loss: 0.1793, eval/loss: 0.2918, train/r2: 0.7919, eval/r2: 0.8683, train/one: 0.9630, train/two: 0.6999, train/three: 0.7129, eval/one: 0.9028, eval/two: 0.8248, eval/three: 0.8773 \n",
      "Epoch: 390, train/loss: 0.1918, eval/loss: 0.2698, train/r2: 0.7701, eval/r2: 0.8721, train/one: 0.9601, train/two: 0.6097, train/three: 0.7404, eval/one: 0.9108, eval/two: 0.8315, eval/three: 0.8741 \n",
      "Epoch: 395, train/loss: 0.2121, eval/loss: 0.2642, train/r2: 0.7791, eval/r2: 0.8804, train/one: 0.9512, train/two: 0.6231, train/three: 0.7629, eval/one: 0.9120, eval/two: 0.8515, eval/three: 0.8778 \n",
      "Epoch: 400, train/loss: 0.1521, eval/loss: 0.2781, train/r2: 0.8110, eval/r2: 0.8804, train/one: 0.9686, train/two: 0.6487, train/three: 0.8158, eval/one: 0.9066, eval/two: 0.8542, eval/three: 0.8803 \n",
      "Epoch: 405, train/loss: 0.1292, eval/loss: 0.3197, train/r2: 0.8112, eval/r2: 0.8753, train/one: 0.9793, train/two: 0.7406, train/three: 0.7136, eval/one: 0.8912, eval/two: 0.8553, eval/three: 0.8794 \n",
      "Epoch: 410, train/loss: 0.1640, eval/loss: 0.3332, train/r2: 0.7876, eval/r2: 0.8691, train/one: 0.9681, train/two: 0.6382, train/three: 0.7565, eval/one: 0.8867, eval/two: 0.8571, eval/three: 0.8636 \n",
      "Epoch: 415, train/loss: 0.1816, eval/loss: 0.3292, train/r2: 0.7769, eval/r2: 0.8670, train/one: 0.9624, train/two: 0.5942, train/three: 0.7740, eval/one: 0.8885, eval/two: 0.8490, eval/three: 0.8636 \n",
      "Epoch: 420, train/loss: 0.1727, eval/loss: 0.2884, train/r2: 0.7663, eval/r2: 0.8722, train/one: 0.9678, train/two: 0.6016, train/three: 0.7295, eval/one: 0.9036, eval/two: 0.8406, eval/three: 0.8724 \n",
      "Epoch: 425, train/loss: 0.2000, eval/loss: 0.2656, train/r2: 0.8241, eval/r2: 0.8774, train/one: 0.9498, train/two: 0.7174, train/three: 0.8050, eval/one: 0.9118, eval/two: 0.8487, eval/three: 0.8717 \n",
      "Epoch: 430, train/loss: 0.1551, eval/loss: 0.2629, train/r2: 0.8256, eval/r2: 0.8788, train/one: 0.9663, train/two: 0.7126, train/three: 0.7980, eval/one: 0.9126, eval/two: 0.8522, eval/three: 0.8714 \n",
      "Epoch: 435, train/loss: 0.1872, eval/loss: 0.2650, train/r2: 0.8041, eval/r2: 0.8771, train/one: 0.9588, train/two: 0.7422, train/three: 0.7112, eval/one: 0.9120, eval/two: 0.8474, eval/three: 0.8720 \n",
      "Epoch: 440, train/loss: 0.2161, eval/loss: 0.2794, train/r2: 0.7792, eval/r2: 0.8786, train/one: 0.9516, train/two: 0.7100, train/three: 0.6760, eval/one: 0.9063, eval/two: 0.8526, eval/three: 0.8770 \n",
      "Epoch: 445, train/loss: 0.2180, eval/loss: 0.2919, train/r2: 0.7830, eval/r2: 0.8772, train/one: 0.9508, train/two: 0.7329, train/three: 0.6653, eval/one: 0.9017, eval/two: 0.8531, eval/three: 0.8768 \n",
      "Epoch: 450, train/loss: 0.1523, eval/loss: 0.2900, train/r2: 0.7990, eval/r2: 0.8771, train/one: 0.9701, train/two: 0.6235, train/three: 0.8032, eval/one: 0.9024, eval/two: 0.8533, eval/three: 0.8757 \n",
      "Epoch: 455, train/loss: 0.2101, eval/loss: 0.2922, train/r2: 0.8029, eval/r2: 0.8765, train/one: 0.9490, train/two: 0.6811, train/three: 0.7786, eval/one: 0.9016, eval/two: 0.8529, eval/three: 0.8750 \n",
      "Epoch: 460, train/loss: 0.1675, eval/loss: 0.2922, train/r2: 0.8302, eval/r2: 0.8763, train/one: 0.9607, train/two: 0.7055, train/three: 0.8244, eval/one: 0.9017, eval/two: 0.8519, eval/three: 0.8752 \n",
      "Epoch: 465, train/loss: 0.1535, eval/loss: 0.2887, train/r2: 0.7626, eval/r2: 0.8765, train/one: 0.9742, train/two: 0.5397, train/three: 0.7740, eval/one: 0.9030, eval/two: 0.8509, eval/three: 0.8757 \n",
      "Epoch: 470, train/loss: 0.1829, eval/loss: 0.2881, train/r2: 0.7490, eval/r2: 0.8771, train/one: 0.9667, train/two: 0.5845, train/three: 0.6960, eval/one: 0.9031, eval/two: 0.8524, eval/three: 0.8758 \n",
      "Epoch: 475, train/loss: 0.1823, eval/loss: 0.2910, train/r2: 0.7887, eval/r2: 0.8769, train/one: 0.9616, train/two: 0.6665, train/three: 0.7380, eval/one: 0.9020, eval/two: 0.8529, eval/three: 0.8758 \n",
      "Epoch: 480, train/loss: 0.2123, eval/loss: 0.2916, train/r2: 0.7833, eval/r2: 0.8766, train/one: 0.9499, train/two: 0.6030, train/three: 0.7969, eval/one: 0.9019, eval/two: 0.8520, eval/three: 0.8759 \n",
      "Epoch: 485, train/loss: 0.1883, eval/loss: 0.2910, train/r2: 0.7980, eval/r2: 0.8765, train/one: 0.9590, train/two: 0.7229, train/three: 0.7122, eval/one: 0.9021, eval/two: 0.8516, eval/three: 0.8759 \n",
      "Epoch: 490, train/loss: 0.1825, eval/loss: 0.2904, train/r2: 0.8201, eval/r2: 0.8766, train/one: 0.9560, train/two: 0.6682, train/three: 0.8361, eval/one: 0.9023, eval/two: 0.8516, eval/three: 0.8759 \n",
      "Epoch: 495, train/loss: 0.1877, eval/loss: 0.2899, train/r2: 0.7729, eval/r2: 0.8767, train/one: 0.9619, train/two: 0.6436, train/three: 0.7134, eval/one: 0.9025, eval/two: 0.8518, eval/three: 0.8758 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 114 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 114 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-121/metadata\n",
      "0.44160493415121715\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401adc57d636496bb119b0d13a9b3a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 4.4139, eval/loss: 6.8783, train/r2: -0.4419, eval/r2: -2.2932, train/one: -0.5104, train/two: -0.9817, train/three: 0.1664, eval/one: -0.7798, eval/two: -0.0914, eval/three: -6.0083 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train/loss: 2.8410, eval/loss: 11.9052, train/r2: -0.3080, eval/r2: -4.4540, train/one: 0.0788, train/two: -1.0844, train/three: 0.0817, eval/one: -2.1063, eval/two: -0.3465, eval/three: -10.9091 \n",
      "Epoch: 10, train/loss: 1.2460, eval/loss: 25.2687, train/r2: -0.0066, eval/r2: -5.3407, train/one: 0.6528, train/two: -0.9366, train/three: 0.2638, eval/one: -6.5002, eval/two: -0.4722, eval/three: -9.0496 \n",
      "Epoch: 15, train/loss: 0.5739, eval/loss: 14.8570, train/r2: 0.1403, eval/r2: -2.7159, train/one: 0.8932, train/two: -0.7378, train/three: 0.2653, eval/one: -3.4182, eval/two: -0.0423, eval/three: -4.6872 \n",
      "Epoch: 20, train/loss: 0.4491, eval/loss: 5.0094, train/r2: 0.3516, eval/r2: -0.4445, train/one: 0.9123, train/two: -0.3781, train/three: 0.5206, eval/one: -0.4810, eval/two: -0.0964, eval/three: -0.7561 \n",
      "Epoch: 25, train/loss: 0.4362, eval/loss: 3.5605, train/r2: 0.4042, eval/r2: -0.2174, train/one: 0.9110, train/two: -0.2070, train/three: 0.5086, eval/one: -0.0432, eval/two: -0.5000, eval/three: -0.1090 \n",
      "Epoch: 30, train/loss: 0.4408, eval/loss: 0.9841, train/r2: 0.3384, eval/r2: 0.3069, train/one: 0.9173, train/two: -0.4084, train/three: 0.5064, eval/one: 0.7465, eval/two: -0.2734, eval/three: 0.4474 \n",
      "Epoch: 35, train/loss: 0.4096, eval/loss: 0.8008, train/r2: 0.3369, eval/r2: 0.4172, train/one: 0.9303, train/two: -0.3567, train/three: 0.4372, eval/one: 0.7978, eval/two: -0.0165, eval/three: 0.4703 \n",
      "Epoch: 40, train/loss: 0.3437, eval/loss: 0.4827, train/r2: 0.4668, eval/r2: 0.5340, train/one: 0.9381, train/two: -0.1096, train/three: 0.5719, eval/one: 0.8890, eval/two: 0.1014, eval/three: 0.6117 \n",
      "Epoch: 45, train/loss: 0.3670, eval/loss: 0.4482, train/r2: 0.3956, eval/r2: 0.5921, train/one: 0.9386, train/two: -0.2613, train/three: 0.5093, eval/one: 0.8915, eval/two: 0.1382, eval/three: 0.7466 \n",
      "Epoch: 50, train/loss: 0.3189, eval/loss: 0.4206, train/r2: 0.4272, eval/r2: 0.6131, train/one: 0.9518, train/two: -0.2929, train/three: 0.6227, eval/one: 0.8968, eval/two: 0.1328, eval/three: 0.8097 \n",
      "Epoch: 55, train/loss: 0.3466, eval/loss: 0.5156, train/r2: 0.3579, eval/r2: 0.5884, train/one: 0.9515, train/two: -0.3523, train/three: 0.4744, eval/one: 0.8684, eval/two: 0.1156, eval/three: 0.7812 \n",
      "Epoch: 60, train/loss: 0.3284, eval/loss: 0.4618, train/r2: 0.4810, eval/r2: 0.5913, train/one: 0.9417, train/two: -0.1216, train/three: 0.6230, eval/one: 0.8852, eval/two: 0.0868, eval/three: 0.8019 \n",
      "Epoch: 65, train/loss: 0.2822, eval/loss: 0.5707, train/r2: 0.4648, eval/r2: 0.5590, train/one: 0.9611, train/two: -0.2153, train/three: 0.6486, eval/one: 0.8561, eval/two: 0.1464, eval/three: 0.6745 \n",
      "Epoch: 70, train/loss: 0.3218, eval/loss: 0.4691, train/r2: 0.4145, eval/r2: 0.5365, train/one: 0.9521, train/two: -0.3520, train/three: 0.6435, eval/one: 0.8953, eval/two: 0.1627, eval/three: 0.5516 \n",
      "Epoch: 75, train/loss: 0.3222, eval/loss: 0.5129, train/r2: 0.4559, eval/r2: 0.5399, train/one: 0.9469, train/two: -0.2198, train/three: 0.6407, eval/one: 0.8789, eval/two: 0.1361, eval/three: 0.6048 \n",
      "Epoch: 80, train/loss: 0.3013, eval/loss: 0.4150, train/r2: 0.4313, eval/r2: 0.5940, train/one: 0.9587, train/two: -0.2370, train/three: 0.5721, eval/one: 0.9001, eval/two: 0.0721, eval/three: 0.8099 \n",
      "Epoch: 85, train/loss: 0.3446, eval/loss: 0.4169, train/r2: 0.4131, eval/r2: 0.5893, train/one: 0.9431, train/two: -0.3760, train/three: 0.6724, eval/one: 0.9004, eval/two: 0.0757, eval/three: 0.7918 \n",
      "Epoch: 90, train/loss: 0.3526, eval/loss: 0.4932, train/r2: 0.4192, eval/r2: 0.5416, train/one: 0.9400, train/two: -0.2963, train/three: 0.6140, eval/one: 0.8827, eval/two: 0.0626, eval/three: 0.6797 \n",
      "Epoch: 95, train/loss: 0.2618, eval/loss: 0.5338, train/r2: 0.5404, eval/r2: 0.5259, train/one: 0.9601, train/two: 0.0463, train/three: 0.6147, eval/one: 0.8737, eval/two: 0.1186, eval/three: 0.5853 \n",
      "Epoch: 100, train/loss: 0.2675, eval/loss: 0.5589, train/r2: 0.5706, eval/r2: 0.5435, train/one: 0.9533, train/two: 0.0635, train/three: 0.6951, eval/one: 0.8642, eval/two: 0.1834, eval/three: 0.5830 \n",
      "Epoch: 105, train/loss: 0.2798, eval/loss: 0.4930, train/r2: 0.4982, eval/r2: 0.5164, train/one: 0.9568, train/two: -0.2044, train/three: 0.7423, eval/one: 0.8886, eval/two: 0.0973, eval/three: 0.5632 \n",
      "Epoch: 110, train/loss: 0.2971, eval/loss: 0.5929, train/r2: 0.4893, eval/r2: 0.5429, train/one: 0.9533, train/two: -0.0584, train/three: 0.5729, eval/one: 0.8481, eval/two: 0.0522, eval/three: 0.7282 \n",
      "Epoch: 115, train/loss: 0.3218, eval/loss: 0.4481, train/r2: 0.5349, eval/r2: 0.5809, train/one: 0.9373, train/two: 0.0159, train/three: 0.6514, eval/one: 0.8943, eval/two: 0.1607, eval/three: 0.6876 \n",
      "Epoch: 120, train/loss: 0.3093, eval/loss: 0.5101, train/r2: 0.5648, eval/r2: 0.5687, train/one: 0.9378, train/two: 0.0519, train/three: 0.7047, eval/one: 0.8774, eval/two: 0.2157, eval/three: 0.6131 \n",
      "Epoch: 125, train/loss: 0.2662, eval/loss: 0.5344, train/r2: 0.6075, eval/r2: 0.6053, train/one: 0.9486, train/two: 0.1248, train/three: 0.7490, eval/one: 0.8634, eval/two: 0.2431, eval/three: 0.7092 \n",
      "Epoch: 130, train/loss: 0.2623, eval/loss: 0.4162, train/r2: 0.6274, eval/r2: 0.4949, train/one: 0.9486, train/two: 0.2574, train/three: 0.6762, eval/one: 0.9094, eval/two: -0.1671, eval/three: 0.7425 \n",
      "Epoch: 135, train/loss: 0.1995, eval/loss: 0.4276, train/r2: 0.6067, eval/r2: 0.6382, train/one: 0.9756, train/two: 0.2006, train/three: 0.6440, eval/one: 0.9024, eval/two: 0.4880, eval/three: 0.5242 \n",
      "Epoch: 140, train/loss: 0.2672, eval/loss: 0.5372, train/r2: 0.6482, eval/r2: 0.7125, train/one: 0.9448, train/two: 0.3784, train/three: 0.6214, eval/one: 0.8540, eval/two: 0.5579, eval/three: 0.7255 \n",
      "Epoch: 145, train/loss: 0.2496, eval/loss: 0.4556, train/r2: 0.7033, eval/r2: 0.6420, train/one: 0.9437, train/two: 0.4515, train/three: 0.7146, eval/one: 0.8980, eval/two: 0.6536, eval/three: 0.3743 \n",
      "Epoch: 150, train/loss: 0.2324, eval/loss: 0.4375, train/r2: 0.7133, eval/r2: 0.4672, train/one: 0.9492, train/two: 0.4886, train/three: 0.7021, eval/one: 0.9046, eval/two: -0.2394, eval/three: 0.7364 \n",
      "Epoch: 155, train/loss: 0.1821, eval/loss: 0.6140, train/r2: 0.7470, eval/r2: 0.0713, train/one: 0.9644, train/two: 0.5696, train/three: 0.7070, eval/one: 0.8839, eval/two: -1.1728, eval/three: 0.5027 \n",
      "Epoch: 160, train/loss: 0.2106, eval/loss: 0.5579, train/r2: 0.7716, eval/r2: 0.3954, train/one: 0.9505, train/two: 0.6586, train/three: 0.7056, eval/one: 0.8720, eval/two: -0.3716, eval/three: 0.6858 \n",
      "Epoch: 165, train/loss: 0.1775, eval/loss: 0.4187, train/r2: 0.7425, eval/r2: 0.7304, train/one: 0.9663, train/two: 0.5204, train/three: 0.7408, eval/one: 0.8955, eval/two: 0.6833, eval/three: 0.6125 \n",
      "Epoch: 170, train/loss: 0.1551, eval/loss: 0.4060, train/r2: 0.8141, eval/r2: 0.7950, train/one: 0.9657, train/two: 0.6916, train/three: 0.7848, eval/one: 0.8891, eval/two: 0.7133, eval/three: 0.7826 \n",
      "Epoch: 175, train/loss: 0.1733, eval/loss: 0.3961, train/r2: 0.8043, eval/r2: 0.7409, train/one: 0.9601, train/two: 0.6836, train/three: 0.7693, eval/one: 0.8993, eval/two: 0.6290, eval/three: 0.6942 \n",
      "Epoch: 180, train/loss: 0.2033, eval/loss: 0.4684, train/r2: 0.7500, eval/r2: 0.6449, train/one: 0.9559, train/two: 0.5852, train/three: 0.7089, eval/one: 0.8940, eval/two: 0.6788, eval/three: 0.3618 \n",
      "Epoch: 185, train/loss: 0.2210, eval/loss: 0.4090, train/r2: 0.7844, eval/r2: 0.5438, train/one: 0.9449, train/two: 0.7009, train/three: 0.7075, eval/one: 0.9074, eval/two: -0.0382, eval/three: 0.7621 \n",
      "Epoch: 190, train/loss: 0.2202, eval/loss: 0.5095, train/r2: 0.7882, eval/r2: 0.3306, train/one: 0.9444, train/two: 0.6868, train/three: 0.7333, eval/one: 0.8900, eval/two: -0.6670, eval/three: 0.7689 \n",
      "Epoch: 195, train/loss: 0.1969, eval/loss: 0.3750, train/r2: 0.7737, eval/r2: 0.8222, train/one: 0.9548, train/two: 0.6005, train/three: 0.7659, eval/one: 0.8983, eval/two: 0.8180, eval/three: 0.7504 \n",
      "Epoch: 200, train/loss: 0.2424, eval/loss: 0.4016, train/r2: 0.7795, eval/r2: 0.8123, train/one: 0.9363, train/two: 0.6160, train/three: 0.7860, eval/one: 0.8888, eval/two: 0.7543, eval/three: 0.7938 \n",
      "Epoch: 205, train/loss: 0.1538, eval/loss: 0.3974, train/r2: 0.7920, eval/r2: 0.7767, train/one: 0.9696, train/two: 0.6786, train/three: 0.7279, eval/one: 0.8946, eval/two: 0.6938, eval/three: 0.7416 \n",
      "Epoch: 210, train/loss: 0.2124, eval/loss: 0.4012, train/r2: 0.7565, eval/r2: 0.7398, train/one: 0.9523, train/two: 0.6672, train/three: 0.6501, eval/one: 0.8981, eval/two: 0.6364, eval/three: 0.6849 \n",
      "Epoch: 215, train/loss: 0.2246, eval/loss: 0.3714, train/r2: 0.7646, eval/r2: 0.7392, train/one: 0.9446, train/two: 0.5317, train/three: 0.8176, eval/one: 0.9149, eval/two: 0.8232, eval/three: 0.4796 \n",
      "Epoch: 220, train/loss: 0.1663, eval/loss: 0.4416, train/r2: 0.8355, eval/r2: 0.6722, train/one: 0.9589, train/two: 0.7708, train/three: 0.7767, eval/one: 0.8975, eval/two: 0.6611, eval/three: 0.4580 \n",
      "Epoch: 225, train/loss: 0.1691, eval/loss: 0.5005, train/r2: 0.8187, eval/r2: 0.6006, train/one: 0.9602, train/two: 0.7514, train/three: 0.7443, eval/one: 0.8868, eval/two: 0.5549, eval/three: 0.3602 \n",
      "Epoch: 230, train/loss: 0.1932, eval/loss: 0.3812, train/r2: 0.7895, eval/r2: 0.7835, train/one: 0.9541, train/two: 0.6388, train/three: 0.7757, eval/one: 0.9019, eval/two: 0.7798, eval/three: 0.6687 \n",
      "Epoch: 235, train/loss: 0.2173, eval/loss: 0.4011, train/r2: 0.7815, eval/r2: 0.7925, train/one: 0.9458, train/two: 0.6154, train/three: 0.7834, eval/one: 0.8931, eval/two: 0.7668, eval/three: 0.7175 \n",
      "Epoch: 240, train/loss: 0.1609, eval/loss: 0.4087, train/r2: 0.7980, eval/r2: 0.6488, train/one: 0.9666, train/two: 0.7397, train/three: 0.6878, eval/one: 0.9009, eval/two: 0.3178, eval/three: 0.7277 \n",
      "Epoch: 245, train/loss: 0.2310, eval/loss: 0.5008, train/r2: 0.7939, eval/r2: 0.7389, train/one: 0.9389, train/two: 0.6495, train/three: 0.7932, eval/one: 0.8660, eval/two: 0.6853, eval/three: 0.6653 \n",
      "Epoch: 250, train/loss: 0.1956, eval/loss: 0.3347, train/r2: 0.7563, eval/r2: 0.8460, train/one: 0.9575, train/two: 0.5554, train/three: 0.7561, eval/one: 0.9083, eval/two: 0.8357, eval/three: 0.7941 \n",
      "Epoch: 255, train/loss: 0.1875, eval/loss: 0.3084, train/r2: 0.8004, eval/r2: 0.8471, train/one: 0.9547, train/two: 0.6466, train/three: 0.8000, eval/one: 0.9166, eval/two: 0.8222, eval/three: 0.8025 \n",
      "Epoch: 260, train/loss: 0.2069, eval/loss: 0.3399, train/r2: 0.7450, eval/r2: 0.7765, train/one: 0.9555, train/two: 0.6015, train/three: 0.6780, eval/one: 0.9119, eval/two: 0.6265, eval/three: 0.7910 \n",
      "Epoch: 265, train/loss: 0.1804, eval/loss: 0.3437, train/r2: 0.8073, eval/r2: 0.8075, train/one: 0.9572, train/two: 0.7164, train/three: 0.7481, eval/one: 0.9076, eval/two: 0.7029, eval/three: 0.8120 \n",
      "Epoch: 270, train/loss: 0.1622, eval/loss: 0.3781, train/r2: 0.7379, eval/r2: 0.7924, train/one: 0.9727, train/two: 0.4982, train/three: 0.7429, eval/one: 0.8967, eval/two: 0.6468, eval/three: 0.8338 \n",
      "Epoch: 275, train/loss: 0.2105, eval/loss: 0.3749, train/r2: 0.8077, eval/r2: 0.7891, train/one: 0.9457, train/two: 0.7371, train/three: 0.7405, eval/one: 0.9016, eval/two: 0.7389, eval/three: 0.7267 \n",
      "Epoch: 280, train/loss: 0.1747, eval/loss: 0.3221, train/r2: 0.7722, eval/r2: 0.7739, train/one: 0.9642, train/two: 0.6470, train/three: 0.7054, eval/one: 0.9189, eval/two: 0.6391, eval/three: 0.7638 \n",
      "Epoch: 285, train/loss: 0.2357, eval/loss: 0.3281, train/r2: 0.7764, eval/r2: 0.7988, train/one: 0.9394, train/two: 0.6130, train/three: 0.7767, eval/one: 0.9144, eval/two: 0.6988, eval/three: 0.7833 \n",
      "Epoch: 290, train/loss: 0.1528, eval/loss: 0.3838, train/r2: 0.7963, eval/r2: 0.7755, train/one: 0.9686, train/two: 0.6248, train/three: 0.7954, eval/one: 0.9012, eval/two: 0.7429, eval/three: 0.6825 \n",
      "Epoch: 295, train/loss: 0.1545, eval/loss: 0.3942, train/r2: 0.7756, eval/r2: 0.7540, train/one: 0.9717, train/two: 0.6564, train/three: 0.6986, eval/one: 0.8994, eval/two: 0.6811, eval/three: 0.6816 \n",
      "Epoch: 300, train/loss: 0.1685, eval/loss: 0.3780, train/r2: 0.8192, eval/r2: 0.7748, train/one: 0.9602, train/two: 0.7411, train/three: 0.7563, eval/one: 0.9014, eval/two: 0.6882, eval/three: 0.7348 \n",
      "Epoch: 305, train/loss: 0.2232, eval/loss: 0.3719, train/r2: 0.8172, eval/r2: 0.7465, train/one: 0.9397, train/two: 0.7716, train/three: 0.7403, eval/one: 0.9043, eval/two: 0.5659, eval/three: 0.7692 \n",
      "Epoch: 310, train/loss: 0.1707, eval/loss: 0.3764, train/r2: 0.7640, eval/r2: 0.7911, train/one: 0.9665, train/two: 0.6038, train/three: 0.7218, eval/one: 0.9006, eval/two: 0.7339, eval/three: 0.7389 \n",
      "Epoch: 315, train/loss: 0.1339, eval/loss: 0.3878, train/r2: 0.8255, eval/r2: 0.7863, train/one: 0.9721, train/two: 0.6961, train/three: 0.8082, eval/one: 0.8994, eval/two: 0.7853, eval/three: 0.6741 \n",
      "Epoch: 320, train/loss: 0.1582, eval/loss: 0.3947, train/r2: 0.8276, eval/r2: 0.7815, train/one: 0.9629, train/two: 0.7385, train/three: 0.7815, eval/one: 0.8962, eval/two: 0.7361, eval/three: 0.7123 \n",
      "Epoch: 325, train/loss: 0.1538, eval/loss: 0.3883, train/r2: 0.8245, eval/r2: 0.7804, train/one: 0.9650, train/two: 0.7346, train/three: 0.7737, eval/one: 0.8999, eval/two: 0.7733, eval/three: 0.6680 \n",
      "Epoch: 330, train/loss: 0.1738, eval/loss: 0.3858, train/r2: 0.8292, eval/r2: 0.7761, train/one: 0.9569, train/two: 0.7626, train/three: 0.7681, eval/one: 0.9018, eval/two: 0.7819, eval/three: 0.6447 \n",
      "Epoch: 335, train/loss: 0.2032, eval/loss: 0.3864, train/r2: 0.7993, eval/r2: 0.7641, train/one: 0.9492, train/two: 0.6800, train/three: 0.7686, eval/one: 0.9013, eval/two: 0.7117, eval/three: 0.6794 \n",
      "Epoch: 340, train/loss: 0.1779, eval/loss: 0.3959, train/r2: 0.7998, eval/r2: 0.7896, train/one: 0.9592, train/two: 0.7008, train/three: 0.7394, eval/one: 0.8929, eval/two: 0.6980, eval/three: 0.7779 \n",
      "Epoch: 345, train/loss: 0.1437, eval/loss: 0.3504, train/r2: 0.7968, eval/r2: 0.7841, train/one: 0.9721, train/two: 0.6253, train/three: 0.7932, eval/one: 0.9087, eval/two: 0.6761, eval/three: 0.7676 \n",
      "Epoch: 350, train/loss: 0.1914, eval/loss: 0.3316, train/r2: 0.8215, eval/r2: 0.8149, train/one: 0.9514, train/two: 0.7730, train/three: 0.7402, eval/one: 0.9126, eval/two: 0.7641, eval/three: 0.7681 \n",
      "Epoch: 355, train/loss: 0.1726, eval/loss: 0.3592, train/r2: 0.7901, eval/r2: 0.8126, train/one: 0.9629, train/two: 0.7083, train/three: 0.6992, eval/one: 0.9056, eval/two: 0.8212, eval/three: 0.7111 \n",
      "Epoch: 360, train/loss: 0.1785, eval/loss: 0.4097, train/r2: 0.7985, eval/r2: 0.7865, train/one: 0.9590, train/two: 0.6886, train/three: 0.7480, eval/one: 0.8919, eval/two: 0.7857, eval/three: 0.6818 \n",
      "Epoch: 365, train/loss: 0.1659, eval/loss: 0.4339, train/r2: 0.7885, eval/r2: 0.7639, train/one: 0.9660, train/two: 0.7212, train/three: 0.6782, eval/one: 0.8863, eval/two: 0.7442, eval/three: 0.6610 \n",
      "Epoch: 370, train/loss: 0.1763, eval/loss: 0.4062, train/r2: 0.7814, eval/r2: 0.7748, train/one: 0.9623, train/two: 0.6604, train/three: 0.7215, eval/one: 0.8936, eval/two: 0.7394, eval/three: 0.6915 \n",
      "Epoch: 375, train/loss: 0.1198, eval/loss: 0.3808, train/r2: 0.8523, eval/r2: 0.8169, train/one: 0.9738, train/two: 0.7320, train/three: 0.8512, eval/one: 0.8967, eval/two: 0.7990, eval/three: 0.7550 \n",
      "Epoch: 380, train/loss: 0.1953, eval/loss: 0.3801, train/r2: 0.8070, eval/r2: 0.8306, train/one: 0.9512, train/two: 0.6906, train/three: 0.7794, eval/one: 0.8952, eval/two: 0.8221, eval/three: 0.7744 \n",
      "Epoch: 385, train/loss: 0.1789, eval/loss: 0.3562, train/r2: 0.8423, eval/r2: 0.8252, train/one: 0.9533, train/two: 0.8027, train/three: 0.7709, eval/one: 0.9051, eval/two: 0.8427, eval/three: 0.7277 \n",
      "Epoch: 390, train/loss: 0.1590, eval/loss: 0.3569, train/r2: 0.8036, eval/r2: 0.8033, train/one: 0.9657, train/two: 0.6825, train/three: 0.7627, eval/one: 0.9082, eval/two: 0.8249, eval/three: 0.6769 \n",
      "Epoch: 395, train/loss: 0.1672, eval/loss: 0.3660, train/r2: 0.8316, eval/r2: 0.7824, train/one: 0.9589, train/two: 0.7523, train/three: 0.7837, eval/one: 0.9077, eval/two: 0.7911, eval/three: 0.6484 \n",
      "Epoch: 400, train/loss: 0.1958, eval/loss: 0.3638, train/r2: 0.7898, eval/r2: 0.7755, train/one: 0.9544, train/two: 0.7458, train/three: 0.6693, eval/one: 0.9094, eval/two: 0.7813, eval/three: 0.6358 \n",
      "Epoch: 405, train/loss: 0.1696, eval/loss: 0.3510, train/r2: 0.7618, eval/r2: 0.7922, train/one: 0.9681, train/two: 0.6623, train/three: 0.6551, eval/one: 0.9105, eval/two: 0.7769, eval/three: 0.6892 \n",
      "Epoch: 410, train/loss: 0.1880, eval/loss: 0.3582, train/r2: 0.8270, eval/r2: 0.8030, train/one: 0.9517, train/two: 0.7640, train/three: 0.7652, eval/one: 0.9062, eval/two: 0.7797, eval/three: 0.7231 \n",
      "Epoch: 415, train/loss: 0.1939, eval/loss: 0.3625, train/r2: 0.8123, eval/r2: 0.8086, train/one: 0.9516, train/two: 0.7528, train/three: 0.7324, eval/one: 0.9038, eval/two: 0.7803, eval/three: 0.7416 \n",
      "Epoch: 420, train/loss: 0.1923, eval/loss: 0.3653, train/r2: 0.7754, eval/r2: 0.8148, train/one: 0.9565, train/two: 0.6202, train/three: 0.7495, eval/one: 0.9018, eval/two: 0.7836, eval/three: 0.7590 \n",
      "Epoch: 425, train/loss: 0.1476, eval/loss: 0.3679, train/r2: 0.8090, eval/r2: 0.8170, train/one: 0.9686, train/two: 0.6271, train/three: 0.8314, eval/one: 0.9007, eval/two: 0.7889, eval/three: 0.7613 \n",
      "Epoch: 430, train/loss: 0.1792, eval/loss: 0.3679, train/r2: 0.8089, eval/r2: 0.8151, train/one: 0.9576, train/two: 0.7297, train/three: 0.7395, eval/one: 0.9012, eval/two: 0.7940, eval/three: 0.7500 \n",
      "Epoch: 435, train/loss: 0.1826, eval/loss: 0.3695, train/r2: 0.8268, eval/r2: 0.8101, train/one: 0.9536, train/two: 0.7481, train/three: 0.7788, eval/one: 0.9015, eval/two: 0.7929, eval/three: 0.7359 \n",
      "Epoch: 440, train/loss: 0.1273, eval/loss: 0.3679, train/r2: 0.8394, eval/r2: 0.8025, train/one: 0.9731, train/two: 0.7434, train/three: 0.8018, eval/one: 0.9033, eval/two: 0.7870, eval/three: 0.7172 \n",
      "Epoch: 445, train/loss: 0.1737, eval/loss: 0.3644, train/r2: 0.8149, eval/r2: 0.8013, train/one: 0.9595, train/two: 0.7875, train/three: 0.6978, eval/one: 0.9041, eval/two: 0.7712, eval/three: 0.7286 \n",
      "Epoch: 450, train/loss: 0.1346, eval/loss: 0.3640, train/r2: 0.8762, eval/r2: 0.7978, train/one: 0.9657, train/two: 0.8559, train/three: 0.8071, eval/one: 0.9042, eval/two: 0.7504, eval/three: 0.7388 \n",
      "Epoch: 455, train/loss: 0.1323, eval/loss: 0.3652, train/r2: 0.8587, eval/r2: 0.7949, train/one: 0.9685, train/two: 0.7807, train/three: 0.8269, eval/one: 0.9038, eval/two: 0.7371, eval/three: 0.7439 \n",
      "Epoch: 460, train/loss: 0.1671, eval/loss: 0.3663, train/r2: 0.8017, eval/r2: 0.7953, train/one: 0.9632, train/two: 0.7046, train/three: 0.7373, eval/one: 0.9034, eval/two: 0.7400, eval/three: 0.7426 \n",
      "Epoch: 465, train/loss: 0.2225, eval/loss: 0.3668, train/r2: 0.8212, eval/r2: 0.7928, train/one: 0.9381, train/two: 0.6778, train/three: 0.8478, eval/one: 0.9035, eval/two: 0.7323, eval/three: 0.7426 \n",
      "Epoch: 470, train/loss: 0.1682, eval/loss: 0.3675, train/r2: 0.8492, eval/r2: 0.7908, train/one: 0.9562, train/two: 0.7922, train/three: 0.7994, eval/one: 0.9035, eval/two: 0.7285, eval/three: 0.7404 \n",
      "Epoch: 475, train/loss: 0.1549, eval/loss: 0.3676, train/r2: 0.8239, eval/r2: 0.7906, train/one: 0.9647, train/two: 0.7333, train/three: 0.7737, eval/one: 0.9035, eval/two: 0.7296, eval/three: 0.7389 \n",
      "Epoch: 480, train/loss: 0.1739, eval/loss: 0.3675, train/r2: 0.8153, eval/r2: 0.7917, train/one: 0.9593, train/two: 0.7845, train/three: 0.7022, eval/one: 0.9035, eval/two: 0.7339, eval/three: 0.7377 \n",
      "Epoch: 485, train/loss: 0.1534, eval/loss: 0.3677, train/r2: 0.8387, eval/r2: 0.7929, train/one: 0.9631, train/two: 0.7476, train/three: 0.8052, eval/one: 0.9034, eval/two: 0.7388, eval/three: 0.7366 \n",
      "Epoch: 490, train/loss: 0.2208, eval/loss: 0.3676, train/r2: 0.7652, eval/r2: 0.7937, train/one: 0.9473, train/two: 0.6411, train/three: 0.7070, eval/one: 0.9033, eval/two: 0.7415, eval/three: 0.7363 \n",
      "Epoch: 495, train/loss: 0.1563, eval/loss: 0.3675, train/r2: 0.8551, eval/r2: 0.7942, train/one: 0.9597, train/two: 0.7803, train/three: 0.8252, eval/one: 0.9034, eval/two: 0.7434, eval/three: 0.7360 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 70 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 70 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-122/metadata\n",
      "0.4198664363229064\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851a73716987498eba8defe491de5bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 4.8362, eval/loss: 31.5051, train/r2: -0.6857, eval/r2: -8.2103, train/one: -0.7177, train/two: -1.4553, train/three: 0.1160, eval/one: -7.4479, eval/two: -0.8996, eval/three: -16.2835 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train/loss: 3.6157, eval/loss: 23.1478, train/r2: -0.3746, eval/r2: -5.6669, train/one: -0.2672, train/two: -0.9646, train/three: 0.1079, eval/one: -5.2186, eval/two: -0.6060, eval/three: -11.1762 \n",
      "Epoch: 10, train/loss: 1.5467, eval/loss: 3.1050, train/r2: -0.0752, eval/r2: -2.4013, train/one: 0.5133, train/two: -1.2048, train/three: 0.4660, eval/one: 0.4681, eval/two: -0.1518, eval/three: -7.5204 \n",
      "Epoch: 15, train/loss: 0.6451, eval/loss: 2.6699, train/r2: 0.0347, eval/r2: -0.4448, train/one: 0.8620, train/two: -1.0718, train/three: 0.3138, eval/one: 0.3653, eval/two: -0.0823, eval/three: -1.6173 \n",
      "Epoch: 20, train/loss: 0.5286, eval/loss: 1.8052, train/r2: 0.1872, eval/r2: 0.0277, train/one: 0.8889, train/two: -0.7816, train/three: 0.4542, eval/one: 0.5710, eval/two: -0.1096, eval/three: -0.3784 \n",
      "Epoch: 25, train/loss: 0.5334, eval/loss: 1.4349, train/r2: 0.1506, eval/r2: 0.3540, train/one: 0.8940, train/two: -0.7475, train/three: 0.3051, eval/one: 0.6443, eval/two: -0.1679, eval/three: 0.5856 \n",
      "Epoch: 30, train/loss: 0.3419, eval/loss: 0.5183, train/r2: 0.3457, eval/r2: 0.5379, train/one: 0.9456, train/two: -0.3773, train/three: 0.4690, eval/one: 0.9001, eval/two: -0.0759, eval/three: 0.7896 \n",
      "Epoch: 35, train/loss: 0.3549, eval/loss: 0.8752, train/r2: 0.3109, eval/r2: 0.5191, train/one: 0.9432, train/two: -0.5283, train/three: 0.5177, eval/one: 0.7941, eval/two: -0.0400, eval/three: 0.8031 \n",
      "Epoch: 40, train/loss: 0.3467, eval/loss: 0.7914, train/r2: 0.3436, eval/r2: 0.4949, train/one: 0.9414, train/two: -0.4992, train/three: 0.5885, eval/one: 0.8225, eval/two: -0.1138, eval/three: 0.7760 \n",
      "Epoch: 45, train/loss: 0.3694, eval/loss: 0.7506, train/r2: 0.3623, eval/r2: 0.5018, train/one: 0.9331, train/two: -0.3046, train/three: 0.4583, eval/one: 0.8340, eval/two: -0.0944, eval/three: 0.7659 \n",
      "Epoch: 50, train/loss: 0.3746, eval/loss: 0.6353, train/r2: 0.2508, eval/r2: 0.5527, train/one: 0.9406, train/two: -0.7666, train/three: 0.5783, eval/one: 0.8627, eval/two: -0.0111, eval/three: 0.8064 \n",
      "Epoch: 55, train/loss: 0.3234, eval/loss: 0.7757, train/r2: 0.4134, eval/r2: 0.4800, train/one: 0.9419, train/two: -0.3550, train/three: 0.6534, eval/one: 0.8288, eval/two: 0.0181, eval/three: 0.5930 \n",
      "Epoch: 60, train/loss: 0.2839, eval/loss: 0.6544, train/r2: 0.4432, eval/r2: 0.5403, train/one: 0.9542, train/two: -0.2863, train/three: 0.6617, eval/one: 0.8583, eval/two: 0.0362, eval/three: 0.7265 \n",
      "Epoch: 65, train/loss: 0.2497, eval/loss: 0.6584, train/r2: 0.5000, eval/r2: 0.5712, train/one: 0.9636, train/two: -0.0282, train/three: 0.5647, eval/one: 0.8534, eval/two: 0.0820, eval/three: 0.7782 \n",
      "Epoch: 70, train/loss: 0.2609, eval/loss: 0.6887, train/r2: 0.4877, eval/r2: 0.4394, train/one: 0.9605, train/two: -0.0603, train/three: 0.5630, eval/one: 0.8599, eval/two: 0.0908, eval/three: 0.3675 \n",
      "Epoch: 75, train/loss: 0.3269, eval/loss: 0.6344, train/r2: 0.4507, eval/r2: 0.5261, train/one: 0.9401, train/two: -0.0649, train/three: 0.4770, eval/one: 0.8660, eval/two: 0.0977, eval/three: 0.6146 \n",
      "Epoch: 80, train/loss: 0.2709, eval/loss: 0.6647, train/r2: 0.4631, eval/r2: 0.5610, train/one: 0.9564, train/two: -0.2669, train/three: 0.7000, eval/one: 0.8526, eval/two: 0.0872, eval/three: 0.7432 \n",
      "Epoch: 85, train/loss: 0.2943, eval/loss: 0.6564, train/r2: 0.3917, eval/r2: 0.5711, train/one: 0.9544, train/two: -0.4958, train/three: 0.7166, eval/one: 0.8539, eval/two: 0.0960, eval/three: 0.7634 \n",
      "Epoch: 90, train/loss: 0.3128, eval/loss: 0.9127, train/r2: 0.3138, eval/r2: 0.4805, train/one: 0.9590, train/two: -0.5606, train/three: 0.5431, eval/one: 0.7870, eval/two: 0.1200, eval/three: 0.5344 \n",
      "Epoch: 95, train/loss: 0.2780, eval/loss: 0.8781, train/r2: 0.4137, eval/r2: 0.5134, train/one: 0.9602, train/two: -0.3517, train/three: 0.6327, eval/one: 0.7936, eval/two: 0.1192, eval/three: 0.6274 \n",
      "Epoch: 100, train/loss: 0.3212, eval/loss: 0.6224, train/r2: 0.4321, eval/r2: 0.5967, train/one: 0.9437, train/two: -0.1562, train/three: 0.5087, eval/one: 0.8612, eval/two: 0.0978, eval/three: 0.8311 \n",
      "Epoch: 105, train/loss: 0.3168, eval/loss: 0.7297, train/r2: 0.4489, eval/r2: 0.5223, train/one: 0.9439, train/two: -0.0942, train/three: 0.4971, eval/one: 0.8374, eval/two: 0.1488, eval/three: 0.5808 \n",
      "Epoch: 110, train/loss: 0.2777, eval/loss: 0.7131, train/r2: 0.4183, eval/r2: 0.5182, train/one: 0.9607, train/two: -0.3012, train/three: 0.5953, eval/one: 0.8429, eval/two: 0.1715, eval/three: 0.5402 \n",
      "Epoch: 115, train/loss: 0.2960, eval/loss: 0.7399, train/r2: 0.5008, eval/r2: 0.5537, train/one: 0.9437, train/two: -0.0794, train/three: 0.6381, eval/one: 0.8306, eval/two: 0.1203, eval/three: 0.7102 \n",
      "Epoch: 120, train/loss: 0.2639, eval/loss: 0.6876, train/r2: 0.5398, eval/r2: 0.5832, train/one: 0.9522, train/two: 0.0288, train/three: 0.6385, eval/one: 0.8430, eval/two: 0.1443, eval/three: 0.7623 \n",
      "Epoch: 125, train/loss: 0.2568, eval/loss: 0.8564, train/r2: 0.5748, eval/r2: 0.5007, train/one: 0.9501, train/two: 0.0785, train/three: 0.6959, eval/one: 0.8015, eval/two: 0.2494, eval/three: 0.4512 \n",
      "Epoch: 130, train/loss: 0.2350, eval/loss: 0.7104, train/r2: 0.5128, eval/r2: 0.5834, train/one: 0.9667, train/two: -0.0596, train/three: 0.6311, eval/one: 0.8359, eval/two: 0.2328, eval/three: 0.6815 \n",
      "Epoch: 135, train/loss: 0.2214, eval/loss: 0.6032, train/r2: 0.6477, eval/r2: 0.6221, train/one: 0.9576, train/two: 0.3531, train/three: 0.6325, eval/one: 0.8636, eval/two: 0.2806, eval/three: 0.7221 \n",
      "Epoch: 140, train/loss: 0.2426, eval/loss: 0.6656, train/r2: 0.6227, eval/r2: 0.5006, train/one: 0.9515, train/two: 0.2624, train/three: 0.6542, eval/one: 0.8593, eval/two: 0.2885, eval/three: 0.3541 \n",
      "Epoch: 145, train/loss: 0.2619, eval/loss: 0.5437, train/r2: 0.5367, eval/r2: 0.6770, train/one: 0.9529, train/two: -0.0039, train/three: 0.6612, eval/one: 0.8750, eval/two: 0.3242, eval/three: 0.8317 \n",
      "Epoch: 150, train/loss: 0.2405, eval/loss: 0.5831, train/r2: 0.5232, eval/r2: 0.5313, train/one: 0.9629, train/two: -0.0509, train/three: 0.6577, eval/one: 0.8806, eval/two: 0.2682, eval/three: 0.4452 \n",
      "Epoch: 155, train/loss: 0.3051, eval/loss: 0.5826, train/r2: 0.6206, eval/r2: 0.6929, train/one: 0.9262, train/two: 0.2502, train/three: 0.6853, eval/one: 0.8611, eval/two: 0.4182, eval/three: 0.7993 \n",
      "Epoch: 160, train/loss: 0.2565, eval/loss: 0.6867, train/r2: 0.6930, eval/r2: 0.6678, train/one: 0.9372, train/two: 0.4271, train/three: 0.7147, eval/one: 0.8326, eval/two: 0.3719, eval/three: 0.7988 \n",
      "Epoch: 165, train/loss: 0.2392, eval/loss: 0.7096, train/r2: 0.6518, eval/r2: 0.7034, train/one: 0.9486, train/two: 0.3022, train/three: 0.7047, eval/one: 0.8213, eval/two: 0.4497, eval/three: 0.8393 \n",
      "Epoch: 170, train/loss: 0.1884, eval/loss: 0.5686, train/r2: 0.7028, eval/r2: 0.6188, train/one: 0.9627, train/two: 0.4105, train/three: 0.7352, eval/one: 0.8748, eval/two: 0.1413, eval/three: 0.8402 \n",
      "Epoch: 175, train/loss: 0.2007, eval/loss: 0.8754, train/r2: 0.6957, eval/r2: 0.6792, train/one: 0.9605, train/two: 0.4853, train/three: 0.6414, eval/one: 0.7739, eval/two: 0.4486, eval/three: 0.8151 \n",
      "Epoch: 180, train/loss: 0.2268, eval/loss: 0.6870, train/r2: 0.6810, eval/r2: 0.7292, train/one: 0.9513, train/two: 0.4296, train/three: 0.6621, eval/one: 0.8248, eval/two: 0.5927, eval/three: 0.7701 \n",
      "Epoch: 185, train/loss: 0.1675, eval/loss: 0.8414, train/r2: 0.7564, eval/r2: 0.7245, train/one: 0.9658, train/two: 0.5958, train/three: 0.7077, eval/one: 0.7785, eval/two: 0.6155, eval/three: 0.7795 \n",
      "Epoch: 190, train/loss: 0.1706, eval/loss: 0.6063, train/r2: 0.7936, eval/r2: 0.5839, train/one: 0.9597, train/two: 0.6722, train/three: 0.7490, eval/one: 0.8669, eval/two: 0.4547, eval/three: 0.4302 \n",
      "Epoch: 195, train/loss: 0.1508, eval/loss: 0.6878, train/r2: 0.7868, eval/r2: 0.3138, train/one: 0.9681, train/two: 0.6340, train/three: 0.7582, eval/one: 0.8743, eval/two: 0.6320, eval/three: -0.5650 \n",
      "Epoch: 200, train/loss: 0.2007, eval/loss: 0.7087, train/r2: 0.7307, eval/r2: 0.3858, train/one: 0.9552, train/two: 0.5206, train/three: 0.7164, eval/one: 0.8598, eval/two: 0.3934, eval/three: -0.0958 \n",
      "Epoch: 205, train/loss: 0.1720, eval/loss: 0.5556, train/r2: 0.8196, eval/r2: 0.6586, train/one: 0.9575, train/two: 0.8058, train/three: 0.6956, eval/one: 0.8734, eval/two: 0.4179, eval/three: 0.6844 \n",
      "Epoch: 210, train/loss: 0.1961, eval/loss: 0.5709, train/r2: 0.7231, eval/r2: 0.7734, train/one: 0.9595, train/two: 0.5743, train/three: 0.6356, eval/one: 0.8546, eval/two: 0.6486, eval/three: 0.8171 \n",
      "Epoch: 215, train/loss: 0.2190, eval/loss: 0.5269, train/r2: 0.7204, eval/r2: 0.7845, train/one: 0.9494, train/two: 0.5140, train/three: 0.6977, eval/one: 0.8666, eval/two: 0.6633, eval/three: 0.8237 \n",
      "Epoch: 220, train/loss: 0.2100, eval/loss: 0.6251, train/r2: 0.7462, eval/r2: 0.7873, train/one: 0.9491, train/two: 0.5374, train/three: 0.7523, eval/one: 0.8365, eval/two: 0.6909, eval/three: 0.8346 \n",
      "Epoch: 225, train/loss: 0.1897, eval/loss: 0.7280, train/r2: 0.7377, eval/r2: 0.7813, train/one: 0.9594, train/two: 0.5665, train/three: 0.6871, eval/one: 0.8060, eval/two: 0.6809, eval/three: 0.8570 \n",
      "Epoch: 230, train/loss: 0.2310, eval/loss: 0.7196, train/r2: 0.7033, eval/r2: 0.7965, train/one: 0.9472, train/two: 0.4999, train/three: 0.6627, eval/one: 0.8067, eval/two: 0.6834, eval/three: 0.8994 \n",
      "Epoch: 235, train/loss: 0.1784, eval/loss: 0.6771, train/r2: 0.7815, eval/r2: 0.7134, train/one: 0.9579, train/two: 0.6392, train/three: 0.7474, eval/one: 0.8296, eval/two: 0.6941, eval/three: 0.6164 \n",
      "Epoch: 240, train/loss: 0.1713, eval/loss: 0.5128, train/r2: 0.7318, eval/r2: 0.7969, train/one: 0.9681, train/two: 0.5802, train/three: 0.6470, eval/one: 0.8694, eval/two: 0.6759, eval/three: 0.8453 \n",
      "Epoch: 245, train/loss: 0.1900, eval/loss: 0.4784, train/r2: 0.7689, eval/r2: 0.7974, train/one: 0.9567, train/two: 0.6986, train/three: 0.6514, eval/one: 0.8798, eval/two: 0.6356, eval/three: 0.8768 \n",
      "Epoch: 250, train/loss: 0.1568, eval/loss: 0.4921, train/r2: 0.8010, eval/r2: 0.7941, train/one: 0.9641, train/two: 0.6762, train/three: 0.7629, eval/one: 0.8760, eval/two: 0.6650, eval/three: 0.8414 \n",
      "Epoch: 255, train/loss: 0.1713, eval/loss: 0.4651, train/r2: 0.7395, eval/r2: 0.7998, train/one: 0.9670, train/two: 0.5904, train/three: 0.6610, eval/one: 0.8834, eval/two: 0.7227, eval/three: 0.7934 \n",
      "Epoch: 260, train/loss: 0.1823, eval/loss: 0.5435, train/r2: 0.7575, eval/r2: 0.6721, train/one: 0.9596, train/two: 0.5939, train/three: 0.7190, eval/one: 0.8750, eval/two: 0.6798, eval/three: 0.4615 \n",
      "Epoch: 265, train/loss: 0.1952, eval/loss: 0.6076, train/r2: 0.7592, eval/r2: 0.6836, train/one: 0.9558, train/two: 0.6773, train/three: 0.6446, eval/one: 0.8542, eval/two: 0.6829, eval/three: 0.5136 \n",
      "Epoch: 270, train/loss: 0.1810, eval/loss: 0.4918, train/r2: 0.7452, eval/r2: 0.8285, train/one: 0.9609, train/two: 0.5337, train/three: 0.7411, eval/one: 0.8719, eval/two: 0.7287, eval/three: 0.8849 \n",
      "Epoch: 275, train/loss: 0.1449, eval/loss: 0.5454, train/r2: 0.8141, eval/r2: 0.7585, train/one: 0.9670, train/two: 0.6949, train/three: 0.7804, eval/one: 0.8641, eval/two: 0.6604, eval/three: 0.7511 \n",
      "Epoch: 280, train/loss: 0.1859, eval/loss: 0.5909, train/r2: 0.7547, eval/r2: 0.7315, train/one: 0.9566, train/two: 0.5020, train/three: 0.8055, eval/one: 0.8536, eval/two: 0.6225, eval/three: 0.7183 \n",
      "Epoch: 285, train/loss: 0.2006, eval/loss: 0.5488, train/r2: 0.7546, eval/r2: 0.8063, train/one: 0.9523, train/two: 0.5782, train/three: 0.7334, eval/one: 0.8573, eval/two: 0.6865, eval/three: 0.8752 \n",
      "Epoch: 290, train/loss: 0.1684, eval/loss: 0.6214, train/r2: 0.8071, eval/r2: 0.7810, train/one: 0.9589, train/two: 0.7046, train/three: 0.7577, eval/one: 0.8383, eval/two: 0.7100, eval/three: 0.7948 \n",
      "Epoch: 295, train/loss: 0.1606, eval/loss: 0.4749, train/r2: 0.7783, eval/r2: 0.8241, train/one: 0.9667, train/two: 0.6875, train/three: 0.6806, eval/one: 0.8775, eval/two: 0.7221, eval/three: 0.8728 \n",
      "Epoch: 300, train/loss: 0.1586, eval/loss: 0.4913, train/r2: 0.7850, eval/r2: 0.8120, train/one: 0.9649, train/two: 0.6155, train/three: 0.7745, eval/one: 0.8741, eval/two: 0.6890, eval/three: 0.8730 \n",
      "Epoch: 305, train/loss: 0.1770, eval/loss: 0.5212, train/r2: 0.7795, eval/r2: 0.7088, train/one: 0.9586, train/two: 0.6272, train/three: 0.7528, eval/one: 0.8777, eval/two: 0.5325, eval/three: 0.7162 \n",
      "Epoch: 310, train/loss: 0.1374, eval/loss: 0.5604, train/r2: 0.7997, eval/r2: 0.6209, train/one: 0.9733, train/two: 0.7287, train/three: 0.6973, eval/one: 0.8763, eval/two: 0.5248, eval/three: 0.4617 \n",
      "Epoch: 315, train/loss: 0.1842, eval/loss: 0.5838, train/r2: 0.7634, eval/r2: 0.6955, train/one: 0.9601, train/two: 0.7039, train/three: 0.6261, eval/one: 0.8600, eval/two: 0.6629, eval/three: 0.5636 \n",
      "Epoch: 320, train/loss: 0.1477, eval/loss: 0.5547, train/r2: 0.7879, eval/r2: 0.7170, train/one: 0.9695, train/two: 0.6506, train/three: 0.7437, eval/one: 0.8663, eval/two: 0.6443, eval/three: 0.6405 \n",
      "Epoch: 325, train/loss: 0.1546, eval/loss: 0.5494, train/r2: 0.7943, eval/r2: 0.6739, train/one: 0.9654, train/two: 0.6438, train/three: 0.7736, eval/one: 0.8733, eval/two: 0.5416, eval/three: 0.6068 \n",
      "Epoch: 330, train/loss: 0.1713, eval/loss: 0.5592, train/r2: 0.8195, eval/r2: 0.7075, train/one: 0.9563, train/two: 0.7365, train/three: 0.7656, eval/one: 0.8662, eval/two: 0.5680, eval/three: 0.6884 \n",
      "Epoch: 335, train/loss: 0.1517, eval/loss: 0.5204, train/r2: 0.8076, eval/r2: 0.7612, train/one: 0.9654, train/two: 0.6966, train/three: 0.7608, eval/one: 0.8714, eval/two: 0.6804, eval/three: 0.7319 \n",
      "Epoch: 340, train/loss: 0.1942, eval/loss: 0.4786, train/r2: 0.7774, eval/r2: 0.7620, train/one: 0.9522, train/two: 0.6358, train/three: 0.7443, eval/one: 0.8839, eval/two: 0.6893, eval/three: 0.7126 \n",
      "Epoch: 345, train/loss: 0.1575, eval/loss: 0.4600, train/r2: 0.7646, eval/r2: 0.7857, train/one: 0.9681, train/two: 0.5801, train/three: 0.7455, eval/one: 0.8868, eval/two: 0.6682, eval/three: 0.8020 \n",
      "Epoch: 350, train/loss: 0.1654, eval/loss: 0.4628, train/r2: 0.7944, eval/r2: 0.8027, train/one: 0.9621, train/two: 0.6921, train/three: 0.7289, eval/one: 0.8839, eval/two: 0.6811, eval/three: 0.8433 \n",
      "Epoch: 355, train/loss: 0.1646, eval/loss: 0.4775, train/r2: 0.7863, eval/r2: 0.8099, train/one: 0.9645, train/two: 0.7237, train/three: 0.6708, eval/one: 0.8785, eval/two: 0.7136, eval/three: 0.8376 \n",
      "Epoch: 360, train/loss: 0.1790, eval/loss: 0.5294, train/r2: 0.7827, eval/r2: 0.7718, train/one: 0.9568, train/two: 0.6069, train/three: 0.7844, eval/one: 0.8673, eval/two: 0.7023, eval/three: 0.7457 \n",
      "Epoch: 365, train/loss: 0.1645, eval/loss: 0.5503, train/r2: 0.7779, eval/r2: 0.7322, train/one: 0.9652, train/two: 0.6889, train/three: 0.6796, eval/one: 0.8658, eval/two: 0.6806, eval/three: 0.6503 \n",
      "Epoch: 370, train/loss: 0.1240, eval/loss: 0.5501, train/r2: 0.7946, eval/r2: 0.7165, train/one: 0.9779, train/two: 0.6489, train/three: 0.7570, eval/one: 0.8677, eval/two: 0.6863, eval/three: 0.5955 \n",
      "Epoch: 375, train/loss: 0.1728, eval/loss: 0.5353, train/r2: 0.7908, eval/r2: 0.7433, train/one: 0.9592, train/two: 0.6670, train/three: 0.7464, eval/one: 0.8689, eval/two: 0.7046, eval/three: 0.6564 \n",
      "Epoch: 380, train/loss: 0.1776, eval/loss: 0.5138, train/r2: 0.7724, eval/r2: 0.7738, train/one: 0.9590, train/two: 0.6004, train/three: 0.7579, eval/one: 0.8718, eval/two: 0.7016, eval/three: 0.7480 \n",
      "Epoch: 385, train/loss: 0.1724, eval/loss: 0.5126, train/r2: 0.7775, eval/r2: 0.7933, train/one: 0.9615, train/two: 0.6619, train/three: 0.7090, eval/one: 0.8698, eval/two: 0.7032, eval/three: 0.8070 \n",
      "Epoch: 390, train/loss: 0.1889, eval/loss: 0.5183, train/r2: 0.7417, eval/r2: 0.8011, train/one: 0.9580, train/two: 0.5176, train/three: 0.7494, eval/one: 0.8672, eval/two: 0.7052, eval/three: 0.8307 \n",
      "Epoch: 395, train/loss: 0.1541, eval/loss: 0.5208, train/r2: 0.7912, eval/r2: 0.7926, train/one: 0.9661, train/two: 0.6384, train/three: 0.7691, eval/one: 0.8674, eval/two: 0.7100, eval/three: 0.8006 \n",
      "Epoch: 400, train/loss: 0.1909, eval/loss: 0.5017, train/r2: 0.8193, eval/r2: 0.7927, train/one: 0.9480, train/two: 0.7188, train/three: 0.7910, eval/one: 0.8732, eval/two: 0.7205, eval/three: 0.7846 \n",
      "Epoch: 405, train/loss: 0.1822, eval/loss: 0.5058, train/r2: 0.7716, eval/r2: 0.7870, train/one: 0.9592, train/two: 0.6927, train/three: 0.6630, eval/one: 0.8726, eval/two: 0.7285, eval/three: 0.7600 \n",
      "Epoch: 410, train/loss: 0.1512, eval/loss: 0.5083, train/r2: 0.7594, eval/r2: 0.7851, train/one: 0.9731, train/two: 0.6571, train/three: 0.6479, eval/one: 0.8721, eval/two: 0.7287, eval/three: 0.7546 \n",
      "Epoch: 415, train/loss: 0.1838, eval/loss: 0.5107, train/r2: 0.7254, eval/r2: 0.7836, train/one: 0.9637, train/two: 0.5592, train/three: 0.6531, eval/one: 0.8715, eval/two: 0.7281, eval/three: 0.7512 \n",
      "Epoch: 420, train/loss: 0.1704, eval/loss: 0.5245, train/r2: 0.7977, eval/r2: 0.7761, train/one: 0.9586, train/two: 0.6499, train/three: 0.7846, eval/one: 0.8682, eval/two: 0.7271, eval/three: 0.7330 \n",
      "Epoch: 425, train/loss: 0.1705, eval/loss: 0.5292, train/r2: 0.8006, eval/r2: 0.7741, train/one: 0.9596, train/two: 0.7255, train/three: 0.7166, eval/one: 0.8670, eval/two: 0.7269, eval/three: 0.7284 \n",
      "Epoch: 430, train/loss: 0.1383, eval/loss: 0.5185, train/r2: 0.8260, eval/r2: 0.7828, train/one: 0.9685, train/two: 0.7352, train/three: 0.7744, eval/one: 0.8693, eval/two: 0.7291, eval/three: 0.7499 \n",
      "Epoch: 435, train/loss: 0.2069, eval/loss: 0.5104, train/r2: 0.7470, eval/r2: 0.7825, train/one: 0.9508, train/two: 0.5656, train/three: 0.7245, eval/one: 0.8717, eval/two: 0.7298, eval/three: 0.7460 \n",
      "Epoch: 440, train/loss: 0.1938, eval/loss: 0.5026, train/r2: 0.7992, eval/r2: 0.7830, train/one: 0.9497, train/two: 0.6887, train/three: 0.7591, eval/one: 0.8741, eval/two: 0.7271, eval/three: 0.7478 \n",
      "Epoch: 445, train/loss: 0.1279, eval/loss: 0.5011, train/r2: 0.8480, eval/r2: 0.7798, train/one: 0.9692, train/two: 0.7490, train/three: 0.8259, eval/one: 0.8749, eval/two: 0.7247, eval/three: 0.7399 \n",
      "Epoch: 450, train/loss: 0.1600, eval/loss: 0.5019, train/r2: 0.7537, eval/r2: 0.7821, train/one: 0.9680, train/two: 0.5354, train/three: 0.7576, eval/one: 0.8744, eval/two: 0.7253, eval/three: 0.7466 \n",
      "Epoch: 455, train/loss: 0.1635, eval/loss: 0.5021, train/r2: 0.7239, eval/r2: 0.7863, train/one: 0.9730, train/two: 0.5964, train/three: 0.6021, eval/one: 0.8738, eval/two: 0.7277, eval/three: 0.7573 \n",
      "Epoch: 460, train/loss: 0.2085, eval/loss: 0.5048, train/r2: 0.7452, eval/r2: 0.7863, train/one: 0.9510, train/two: 0.5911, train/three: 0.6936, eval/one: 0.8730, eval/two: 0.7281, eval/three: 0.7578 \n",
      "Epoch: 465, train/loss: 0.1768, eval/loss: 0.5071, train/r2: 0.8062, eval/r2: 0.7858, train/one: 0.9556, train/two: 0.7009, train/three: 0.7619, eval/one: 0.8724, eval/two: 0.7279, eval/three: 0.7571 \n",
      "Epoch: 470, train/loss: 0.1895, eval/loss: 0.5070, train/r2: 0.7511, eval/r2: 0.7876, train/one: 0.9584, train/two: 0.6246, train/three: 0.6703, eval/one: 0.8722, eval/two: 0.7279, eval/three: 0.7627 \n",
      "Epoch: 475, train/loss: 0.1773, eval/loss: 0.5052, train/r2: 0.8050, eval/r2: 0.7898, train/one: 0.9562, train/two: 0.7281, train/three: 0.7308, eval/one: 0.8724, eval/two: 0.7275, eval/three: 0.7695 \n",
      "Epoch: 480, train/loss: 0.1635, eval/loss: 0.5044, train/r2: 0.7419, eval/r2: 0.7917, train/one: 0.9708, train/two: 0.6399, train/three: 0.6149, eval/one: 0.8725, eval/two: 0.7272, eval/three: 0.7754 \n",
      "Epoch: 485, train/loss: 0.1825, eval/loss: 0.5034, train/r2: 0.8056, eval/r2: 0.7931, train/one: 0.9532, train/two: 0.6905, train/three: 0.7730, eval/one: 0.8726, eval/two: 0.7273, eval/three: 0.7796 \n",
      "Epoch: 490, train/loss: 0.1771, eval/loss: 0.5026, train/r2: 0.8018, eval/r2: 0.7941, train/one: 0.9567, train/two: 0.7238, train/three: 0.7248, eval/one: 0.8727, eval/two: 0.7273, eval/three: 0.7823 \n",
      "Epoch: 495, train/loss: 0.1881, eval/loss: 0.5021, train/r2: 0.7528, eval/r2: 0.7950, train/one: 0.9578, train/two: 0.5857, train/three: 0.7149, eval/one: 0.8728, eval/two: 0.7273, eval/three: 0.7849 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 69 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 69 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-123/metadata\n",
      "0.4508551418603717\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b682083f1e460abc58f49ab7e0387c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 4.3080, eval/loss: 2.5972, train/r2: -0.3399, eval/r2: 0.0609, train/one: -0.3521, train/two: -0.7407, train/three: 0.0731, eval/one: -0.1155, eval/two: -0.1719, eval/three: 0.4702 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train/loss: 3.0140, eval/loss: 2.6630, train/r2: -0.3003, eval/r2: -1.6374, train/one: 0.0985, train/two: -1.0398, train/three: 0.0403, eval/one: 0.1947, eval/two: -0.0173, eval/three: -5.0895 \n",
      "Epoch: 10, train/loss: 1.2329, eval/loss: 4.6628, train/r2: -0.0124, eval/r2: -2.0637, train/one: 0.6934, train/two: -0.7015, train/three: -0.0292, eval/one: -0.6993, eval/two: -0.0223, eval/three: -5.4693 \n",
      "Epoch: 15, train/loss: 0.6900, eval/loss: 10.6777, train/r2: 0.1139, eval/r2: -1.4296, train/one: 0.8681, train/two: -0.6387, train/three: 0.1124, eval/one: -3.7725, eval/two: -0.1301, eval/three: -0.3862 \n",
      "Epoch: 20, train/loss: 0.6273, eval/loss: 20.9476, train/r2: 0.2124, eval/r2: -2.8044, train/one: 0.8762, train/two: -0.5665, train/three: 0.3275, eval/one: -8.5345, eval/two: -0.4762, eval/three: 0.5976 \n",
      "Epoch: 25, train/loss: 0.5707, eval/loss: 12.9677, train/r2: 0.3054, eval/r2: -1.6482, train/one: 0.8855, train/two: -0.3195, train/three: 0.3502, eval/one: -4.8612, eval/two: -0.5562, eval/three: 0.4728 \n",
      "Epoch: 30, train/loss: 0.5201, eval/loss: 4.6221, train/r2: 0.3307, eval/r2: -0.1529, train/one: 0.8991, train/two: -0.3521, train/three: 0.4451, eval/one: -1.0633, eval/two: -0.1478, eval/three: 0.7523 \n",
      "Epoch: 35, train/loss: 0.4857, eval/loss: 1.5556, train/r2: 0.2970, eval/r2: 0.3785, train/one: 0.9180, train/two: -0.2509, train/three: 0.2239, eval/one: 0.3363, eval/two: 0.0253, eval/three: 0.7740 \n",
      "Epoch: 40, train/loss: 0.3743, eval/loss: 0.9567, train/r2: 0.4185, eval/r2: 0.4445, train/one: 0.9391, train/two: -0.2343, train/three: 0.5505, eval/one: 0.6171, eval/two: 0.0525, eval/three: 0.6639 \n",
      "Epoch: 45, train/loss: 0.4564, eval/loss: 0.9481, train/r2: 0.2873, eval/r2: 0.4793, train/one: 0.9302, train/two: -0.2354, train/three: 0.1670, eval/one: 0.6143, eval/two: 0.0504, eval/three: 0.7732 \n",
      "Epoch: 50, train/loss: 0.3551, eval/loss: 0.9996, train/r2: 0.3634, eval/r2: 0.4394, train/one: 0.9520, train/two: -0.4042, train/three: 0.5424, eval/one: 0.5975, eval/two: 0.0690, eval/three: 0.6518 \n",
      "Epoch: 55, train/loss: 0.3321, eval/loss: 0.9780, train/r2: 0.4037, eval/r2: 0.4797, train/one: 0.9554, train/two: -0.3053, train/three: 0.5612, eval/one: 0.6005, eval/two: 0.0848, eval/three: 0.7539 \n",
      "Epoch: 60, train/loss: 0.3432, eval/loss: 0.9387, train/r2: 0.4782, eval/r2: 0.5105, train/one: 0.9427, train/two: -0.1071, train/three: 0.5991, eval/one: 0.6137, eval/two: 0.0897, eval/three: 0.8280 \n",
      "Epoch: 65, train/loss: 0.2824, eval/loss: 0.8945, train/r2: 0.4600, eval/r2: 0.5204, train/one: 0.9657, train/two: -0.2133, train/three: 0.6275, eval/one: 0.6331, eval/two: 0.0803, eval/three: 0.8476 \n",
      "Epoch: 70, train/loss: 0.2995, eval/loss: 0.6839, train/r2: 0.4884, eval/r2: 0.5581, train/one: 0.9568, train/two: -0.1017, train/three: 0.6099, eval/one: 0.7280, eval/two: 0.0544, eval/three: 0.8919 \n",
      "Epoch: 75, train/loss: 0.2720, eval/loss: 0.6113, train/r2: 0.4640, eval/r2: 0.5677, train/one: 0.9709, train/two: -0.0656, train/three: 0.4868, eval/one: 0.7618, eval/two: 0.0627, eval/three: 0.8786 \n",
      "Epoch: 80, train/loss: 0.2810, eval/loss: 0.7282, train/r2: 0.5443, eval/r2: 0.5449, train/one: 0.9559, train/two: -0.0240, train/three: 0.7009, eval/one: 0.7100, eval/two: 0.0932, eval/three: 0.8314 \n",
      "Epoch: 85, train/loss: 0.3267, eval/loss: 0.7454, train/r2: 0.4774, eval/r2: 0.5600, train/one: 0.9483, train/two: -0.1382, train/three: 0.6222, eval/one: 0.6991, eval/two: 0.1143, eval/three: 0.8666 \n",
      "Epoch: 90, train/loss: 0.2953, eval/loss: 0.5589, train/r2: 0.5116, eval/r2: 0.5943, train/one: 0.9554, train/two: -0.0524, train/three: 0.6318, eval/one: 0.7835, eval/two: 0.1118, eval/three: 0.8877 \n",
      "Epoch: 95, train/loss: 0.3006, eval/loss: 0.6133, train/r2: 0.4550, eval/r2: 0.5837, train/one: 0.9620, train/two: -0.0769, train/three: 0.4800, eval/one: 0.7600, eval/two: 0.1482, eval/three: 0.8430 \n",
      "Epoch: 100, train/loss: 0.2979, eval/loss: 0.6244, train/r2: 0.5514, eval/r2: 0.5697, train/one: 0.9511, train/two: 0.1381, train/three: 0.5650, eval/one: 0.7571, eval/two: 0.1391, eval/three: 0.8131 \n",
      "Epoch: 105, train/loss: 0.2758, eval/loss: 0.5882, train/r2: 0.5021, eval/r2: 0.5697, train/one: 0.9639, train/two: -0.0470, train/three: 0.5894, eval/one: 0.7746, eval/two: 0.1295, eval/three: 0.8051 \n",
      "Epoch: 110, train/loss: 0.3030, eval/loss: 0.9753, train/r2: 0.5327, eval/r2: 0.5417, train/one: 0.9518, train/two: 0.1155, train/three: 0.5309, eval/one: 0.5917, eval/two: 0.1688, eval/three: 0.8646 \n",
      "Epoch: 115, train/loss: 0.2265, eval/loss: 0.5071, train/r2: 0.6249, eval/r2: 0.6240, train/one: 0.9664, train/two: 0.2299, train/three: 0.6784, eval/one: 0.8047, eval/two: 0.1775, eval/three: 0.8896 \n",
      "Epoch: 120, train/loss: 0.2722, eval/loss: 0.7263, train/r2: 0.6695, eval/r2: 0.6101, train/one: 0.9455, train/two: 0.3845, train/three: 0.6787, eval/one: 0.7019, eval/two: 0.2426, eval/three: 0.8856 \n",
      "Epoch: 125, train/loss: 0.2380, eval/loss: 0.5169, train/r2: 0.6162, eval/r2: 0.6259, train/one: 0.9639, train/two: 0.2436, train/three: 0.6409, eval/one: 0.8035, eval/two: 0.3276, eval/three: 0.7466 \n",
      "Epoch: 130, train/loss: 0.2536, eval/loss: 0.5447, train/r2: 0.6196, eval/r2: 0.5999, train/one: 0.9583, train/two: 0.2777, train/three: 0.6229, eval/one: 0.7963, eval/two: 0.3722, eval/three: 0.6312 \n",
      "Epoch: 135, train/loss: 0.1611, eval/loss: 0.5643, train/r2: 0.7159, eval/r2: 0.6661, train/one: 0.9801, train/two: 0.5405, train/three: 0.6270, eval/one: 0.7749, eval/two: 0.4236, eval/three: 0.8000 \n",
      "Epoch: 140, train/loss: 0.2020, eval/loss: 0.7486, train/r2: 0.7208, eval/r2: 0.6399, train/one: 0.9644, train/two: 0.5150, train/three: 0.6829, eval/one: 0.6900, eval/two: 0.4268, eval/three: 0.8028 \n",
      "Epoch: 145, train/loss: 0.2184, eval/loss: 0.6433, train/r2: 0.7388, eval/r2: 0.7116, train/one: 0.9569, train/two: 0.5926, train/three: 0.6670, eval/one: 0.7316, eval/two: 0.5887, eval/three: 0.8146 \n",
      "Epoch: 150, train/loss: 0.2356, eval/loss: 1.0187, train/r2: 0.7028, eval/r2: 0.6445, train/one: 0.9554, train/two: 0.5323, train/three: 0.6207, eval/one: 0.5615, eval/two: 0.5994, eval/three: 0.7727 \n",
      "Epoch: 155, train/loss: 0.2536, eval/loss: 0.4377, train/r2: 0.7183, eval/r2: 0.7236, train/one: 0.9479, train/two: 0.6175, train/three: 0.5895, eval/one: 0.8291, eval/two: 0.5607, eval/three: 0.7811 \n",
      "Epoch: 160, train/loss: 0.2732, eval/loss: 0.5467, train/r2: 0.7374, eval/r2: 0.7589, train/one: 0.9377, train/two: 0.6039, train/three: 0.6706, eval/one: 0.7708, eval/two: 0.6400, eval/three: 0.8660 \n",
      "Epoch: 165, train/loss: 0.2399, eval/loss: 0.5236, train/r2: 0.7228, eval/r2: 0.6623, train/one: 0.9501, train/two: 0.4821, train/three: 0.7361, eval/one: 0.7949, eval/two: 0.3985, eval/three: 0.7936 \n",
      "Epoch: 170, train/loss: 0.2270, eval/loss: 0.4936, train/r2: 0.7069, eval/r2: 0.7803, train/one: 0.9589, train/two: 0.6005, train/three: 0.5614, eval/one: 0.7941, eval/two: 0.6999, eval/three: 0.8470 \n",
      "Epoch: 175, train/loss: 0.1913, eval/loss: 0.4572, train/r2: 0.7442, eval/r2: 0.7860, train/one: 0.9676, train/two: 0.7202, train/three: 0.5448, eval/one: 0.8110, eval/two: 0.7079, eval/three: 0.8391 \n",
      "Epoch: 180, train/loss: 0.1633, eval/loss: 0.5352, train/r2: 0.7937, eval/r2: 0.7185, train/one: 0.9685, train/two: 0.6326, train/three: 0.7801, eval/one: 0.7810, eval/two: 0.5080, eval/three: 0.8665 \n",
      "Epoch: 185, train/loss: 0.1794, eval/loss: 0.4678, train/r2: 0.7466, eval/r2: 0.7402, train/one: 0.9704, train/two: 0.6484, train/three: 0.6210, eval/one: 0.8101, eval/two: 0.5256, eval/three: 0.8850 \n",
      "Epoch: 190, train/loss: 0.2028, eval/loss: 0.7654, train/r2: 0.7907, eval/r2: 0.5271, train/one: 0.9552, train/two: 0.6516, train/three: 0.7653, eval/one: 0.6934, eval/two: 0.0177, eval/three: 0.8701 \n",
      "Epoch: 195, train/loss: 0.2503, eval/loss: 0.5034, train/r2: 0.6851, eval/r2: 0.6881, train/one: 0.9530, train/two: 0.5307, train/three: 0.5716, eval/one: 0.7999, eval/two: 0.4071, eval/three: 0.8574 \n",
      "Epoch: 200, train/loss: 0.2403, eval/loss: 0.5504, train/r2: 0.7996, eval/r2: 0.7361, train/one: 0.9414, train/two: 0.7117, train/three: 0.7458, eval/one: 0.7720, eval/two: 0.5831, eval/three: 0.8532 \n",
      "Epoch: 205, train/loss: 0.1953, eval/loss: 0.4938, train/r2: 0.7576, eval/r2: 0.7731, train/one: 0.9630, train/two: 0.6506, train/three: 0.6591, eval/one: 0.7940, eval/two: 0.6450, eval/three: 0.8803 \n",
      "Epoch: 210, train/loss: 0.1791, eval/loss: 0.7713, train/r2: 0.7326, eval/r2: 0.6726, train/one: 0.9706, train/two: 0.5093, train/three: 0.7178, eval/one: 0.6748, eval/two: 0.5171, eval/three: 0.8259 \n",
      "Epoch: 215, train/loss: 0.1981, eval/loss: 0.4645, train/r2: 0.7781, eval/r2: 0.8167, train/one: 0.9591, train/two: 0.6699, train/three: 0.7053, eval/one: 0.8036, eval/two: 0.7918, eval/three: 0.8548 \n",
      "Epoch: 220, train/loss: 0.1971, eval/loss: 0.5755, train/r2: 0.7821, eval/r2: 0.8065, train/one: 0.9599, train/two: 0.7439, train/three: 0.6426, eval/one: 0.7516, eval/two: 0.8036, eval/three: 0.8643 \n",
      "Epoch: 225, train/loss: 0.2161, eval/loss: 0.5422, train/r2: 0.7483, eval/r2: 0.7786, train/one: 0.9580, train/two: 0.7142, train/three: 0.5726, eval/one: 0.7710, eval/two: 0.7098, eval/three: 0.8552 \n",
      "Epoch: 230, train/loss: 0.2342, eval/loss: 0.4277, train/r2: 0.7391, eval/r2: 0.7945, train/one: 0.9500, train/two: 0.5101, train/three: 0.7571, eval/one: 0.8260, eval/two: 0.7919, eval/three: 0.7655 \n",
      "Epoch: 235, train/loss: 0.1456, eval/loss: 0.3933, train/r2: 0.8129, eval/r2: 0.8303, train/one: 0.9720, train/two: 0.6387, train/three: 0.8281, eval/one: 0.8352, eval/two: 0.7717, eval/three: 0.8841 \n",
      "Epoch: 240, train/loss: 0.1962, eval/loss: 0.4720, train/r2: 0.7716, eval/r2: 0.6502, train/one: 0.9597, train/two: 0.5904, train/three: 0.7648, eval/one: 0.8215, eval/two: 0.3576, eval/three: 0.7716 \n",
      "Epoch: 245, train/loss: 0.1555, eval/loss: 0.4579, train/r2: 0.7860, eval/r2: 0.8023, train/one: 0.9731, train/two: 0.6717, train/three: 0.7131, eval/one: 0.8086, eval/two: 0.7504, eval/three: 0.8480 \n",
      "Epoch: 250, train/loss: 0.2155, eval/loss: 0.4202, train/r2: 0.7443, eval/r2: 0.8188, train/one: 0.9570, train/two: 0.5916, train/three: 0.6843, eval/one: 0.8258, eval/two: 0.8266, eval/three: 0.8042 \n",
      "Epoch: 255, train/loss: 0.1816, eval/loss: 0.4608, train/r2: 0.7745, eval/r2: 0.8242, train/one: 0.9663, train/two: 0.7194, train/three: 0.6378, eval/one: 0.8040, eval/two: 0.7974, eval/three: 0.8712 \n",
      "Epoch: 260, train/loss: 0.1758, eval/loss: 0.5479, train/r2: 0.8256, eval/r2: 0.7487, train/one: 0.9606, train/two: 0.7318, train/three: 0.7843, eval/one: 0.7711, eval/two: 0.5943, eval/three: 0.8806 \n",
      "Epoch: 265, train/loss: 0.2006, eval/loss: 0.4573, train/r2: 0.7501, eval/r2: 0.7508, train/one: 0.9627, train/two: 0.6801, train/three: 0.6074, eval/one: 0.8150, eval/two: 0.5960, eval/three: 0.8413 \n",
      "Epoch: 270, train/loss: 0.1912, eval/loss: 0.4179, train/r2: 0.7867, eval/r2: 0.7764, train/one: 0.9611, train/two: 0.7327, train/three: 0.6661, eval/one: 0.8326, eval/two: 0.7245, eval/three: 0.7722 \n",
      "Epoch: 275, train/loss: 0.1883, eval/loss: 0.4495, train/r2: 0.7809, eval/r2: 0.8139, train/one: 0.9626, train/two: 0.6972, train/three: 0.6828, eval/one: 0.8119, eval/two: 0.8073, eval/three: 0.8223 \n",
      "Epoch: 280, train/loss: 0.1407, eval/loss: 0.4241, train/r2: 0.8025, eval/r2: 0.8247, train/one: 0.9764, train/two: 0.7136, train/three: 0.7174, eval/one: 0.8221, eval/two: 0.8048, eval/three: 0.8472 \n",
      "Epoch: 285, train/loss: 0.1945, eval/loss: 0.4176, train/r2: 0.7477, eval/r2: 0.7782, train/one: 0.9635, train/two: 0.5577, train/three: 0.7220, eval/one: 0.8301, eval/two: 0.6384, eval/three: 0.8659 \n",
      "Epoch: 290, train/loss: 0.1710, eval/loss: 0.4202, train/r2: 0.7983, eval/r2: 0.7659, train/one: 0.9660, train/two: 0.6954, train/three: 0.7335, eval/one: 0.8303, eval/two: 0.5999, eval/three: 0.8674 \n",
      "Epoch: 295, train/loss: 0.2086, eval/loss: 0.4396, train/r2: 0.7956, eval/r2: 0.7433, train/one: 0.9529, train/two: 0.6874, train/three: 0.7464, eval/one: 0.8233, eval/two: 0.5255, eval/three: 0.8812 \n",
      "Epoch: 300, train/loss: 0.2225, eval/loss: 0.4432, train/r2: 0.7365, eval/r2: 0.7239, train/one: 0.9572, train/two: 0.6955, train/three: 0.5568, eval/one: 0.8248, eval/two: 0.5061, eval/three: 0.8408 \n",
      "Epoch: 305, train/loss: 0.1675, eval/loss: 0.4567, train/r2: 0.7781, eval/r2: 0.7538, train/one: 0.9700, train/two: 0.6670, train/three: 0.6973, eval/one: 0.8140, eval/two: 0.5697, eval/three: 0.8776 \n",
      "Epoch: 310, train/loss: 0.2274, eval/loss: 0.4343, train/r2: 0.7522, eval/r2: 0.7603, train/one: 0.9533, train/two: 0.7124, train/three: 0.5908, eval/one: 0.8236, eval/two: 0.5684, eval/three: 0.8889 \n",
      "Epoch: 315, train/loss: 0.1826, eval/loss: 0.5642, train/r2: 0.8027, eval/r2: 0.7993, train/one: 0.9612, train/two: 0.6958, train/three: 0.7511, eval/one: 0.7576, eval/two: 0.7679, eval/three: 0.8726 \n",
      "Epoch: 320, train/loss: 0.1718, eval/loss: 0.4278, train/r2: 0.7843, eval/r2: 0.8245, train/one: 0.9682, train/two: 0.7201, train/three: 0.6644, eval/one: 0.8201, eval/two: 0.7978, eval/three: 0.8556 \n",
      "Epoch: 325, train/loss: 0.2268, eval/loss: 0.4708, train/r2: 0.7867, eval/r2: 0.8147, train/one: 0.9484, train/two: 0.7308, train/three: 0.6808, eval/one: 0.8005, eval/two: 0.7776, eval/three: 0.8658 \n",
      "Epoch: 330, train/loss: 0.1643, eval/loss: 0.4395, train/r2: 0.8109, eval/r2: 0.8280, train/one: 0.9666, train/two: 0.7094, train/three: 0.7566, eval/one: 0.8138, eval/two: 0.8007, eval/three: 0.8695 \n",
      "Epoch: 335, train/loss: 0.1751, eval/loss: 0.4225, train/r2: 0.7853, eval/r2: 0.8269, train/one: 0.9666, train/two: 0.6965, train/three: 0.6930, eval/one: 0.8221, eval/two: 0.7903, eval/three: 0.8683 \n",
      "Epoch: 340, train/loss: 0.1552, eval/loss: 0.4735, train/r2: 0.7989, eval/r2: 0.7903, train/one: 0.9714, train/two: 0.6845, train/three: 0.7407, eval/one: 0.8021, eval/two: 0.7040, eval/three: 0.8647 \n",
      "Epoch: 345, train/loss: 0.1890, eval/loss: 0.4117, train/r2: 0.7917, eval/r2: 0.8265, train/one: 0.9616, train/two: 0.7608, train/three: 0.6527, eval/one: 0.8271, eval/two: 0.7788, eval/three: 0.8735 \n",
      "Epoch: 350, train/loss: 0.1787, eval/loss: 0.4202, train/r2: 0.7950, eval/r2: 0.8320, train/one: 0.9632, train/two: 0.6547, train/three: 0.7670, eval/one: 0.8223, eval/two: 0.7930, eval/three: 0.8808 \n",
      "Epoch: 355, train/loss: 0.1691, eval/loss: 0.4257, train/r2: 0.8036, eval/r2: 0.8251, train/one: 0.9659, train/two: 0.6999, train/three: 0.7450, eval/one: 0.8207, eval/two: 0.7821, eval/three: 0.8724 \n",
      "Epoch: 360, train/loss: 0.1365, eval/loss: 0.4070, train/r2: 0.8205, eval/r2: 0.8340, train/one: 0.9752, train/two: 0.7224, train/three: 0.7639, eval/one: 0.8282, eval/two: 0.7899, eval/three: 0.8839 \n",
      "Epoch: 365, train/loss: 0.1809, eval/loss: 0.3998, train/r2: 0.8146, eval/r2: 0.8382, train/one: 0.9605, train/two: 0.7315, train/three: 0.7517, eval/one: 0.8312, eval/two: 0.8000, eval/three: 0.8833 \n",
      "Epoch: 370, train/loss: 0.1533, eval/loss: 0.3967, train/r2: 0.8184, eval/r2: 0.8410, train/one: 0.9696, train/two: 0.7252, train/three: 0.7604, eval/one: 0.8323, eval/two: 0.8053, eval/three: 0.8853 \n",
      "Epoch: 375, train/loss: 0.1621, eval/loss: 0.4121, train/r2: 0.8101, eval/r2: 0.8374, train/one: 0.9669, train/two: 0.6681, train/three: 0.7953, eval/one: 0.8254, eval/two: 0.8025, eval/three: 0.8844 \n",
      "Epoch: 380, train/loss: 0.1650, eval/loss: 0.4340, train/r2: 0.7885, eval/r2: 0.8178, train/one: 0.9688, train/two: 0.6370, train/three: 0.7596, eval/one: 0.8175, eval/two: 0.7635, eval/three: 0.8725 \n",
      "Epoch: 385, train/loss: 0.1674, eval/loss: 0.4134, train/r2: 0.7847, eval/r2: 0.8055, train/one: 0.9697, train/two: 0.7170, train/three: 0.6674, eval/one: 0.8288, eval/two: 0.7151, eval/three: 0.8727 \n",
      "Epoch: 390, train/loss: 0.2015, eval/loss: 0.4260, train/r2: 0.7995, eval/r2: 0.8073, train/one: 0.9552, train/two: 0.7111, train/three: 0.7323, eval/one: 0.8227, eval/two: 0.7308, eval/three: 0.8685 \n",
      "Epoch: 395, train/loss: 0.1813, eval/loss: 0.4253, train/r2: 0.8244, eval/r2: 0.8170, train/one: 0.9592, train/two: 0.7568, train/three: 0.7573, eval/one: 0.8218, eval/two: 0.7569, eval/three: 0.8723 \n",
      "Epoch: 400, train/loss: 0.1600, eval/loss: 0.4360, train/r2: 0.7995, eval/r2: 0.8220, train/one: 0.9706, train/two: 0.7569, train/three: 0.6709, eval/one: 0.8161, eval/two: 0.7759, eval/three: 0.8739 \n",
      "Epoch: 405, train/loss: 0.2018, eval/loss: 0.4710, train/r2: 0.8114, eval/r2: 0.8164, train/one: 0.9533, train/two: 0.7134, train/three: 0.7676, eval/one: 0.8000, eval/two: 0.7729, eval/three: 0.8763 \n",
      "Epoch: 410, train/loss: 0.1621, eval/loss: 0.4572, train/r2: 0.8282, eval/r2: 0.8172, train/one: 0.9655, train/two: 0.7613, train/three: 0.7577, eval/one: 0.8063, eval/two: 0.7656, eval/three: 0.8797 \n",
      "Epoch: 415, train/loss: 0.1924, eval/loss: 0.4306, train/r2: 0.8185, eval/r2: 0.8244, train/one: 0.9565, train/two: 0.7807, train/three: 0.7185, eval/one: 0.8182, eval/two: 0.7749, eval/three: 0.8799 \n",
      "Epoch: 420, train/loss: 0.1813, eval/loss: 0.4174, train/r2: 0.8174, eval/r2: 0.8283, train/one: 0.9606, train/two: 0.7793, train/three: 0.7123, eval/one: 0.8240, eval/two: 0.7812, eval/three: 0.8798 \n",
      "Epoch: 425, train/loss: 0.1506, eval/loss: 0.4048, train/r2: 0.7887, eval/r2: 0.8302, train/one: 0.9740, train/two: 0.6427, train/three: 0.7494, eval/one: 0.8299, eval/two: 0.7821, eval/three: 0.8787 \n",
      "Epoch: 430, train/loss: 0.1782, eval/loss: 0.4047, train/r2: 0.7890, eval/r2: 0.8291, train/one: 0.9643, train/two: 0.6596, train/three: 0.7431, eval/one: 0.8300, eval/two: 0.7793, eval/three: 0.8780 \n",
      "Epoch: 435, train/loss: 0.1734, eval/loss: 0.4027, train/r2: 0.7928, eval/r2: 0.8310, train/one: 0.9664, train/two: 0.7188, train/three: 0.6931, eval/one: 0.8308, eval/two: 0.7841, eval/three: 0.8782 \n",
      "Epoch: 440, train/loss: 0.1506, eval/loss: 0.4031, train/r2: 0.8022, eval/r2: 0.8318, train/one: 0.9732, train/two: 0.7337, train/three: 0.6997, eval/one: 0.8305, eval/two: 0.7870, eval/three: 0.8780 \n",
      "Epoch: 445, train/loss: 0.2007, eval/loss: 0.4082, train/r2: 0.7810, eval/r2: 0.8323, train/one: 0.9580, train/two: 0.6859, train/three: 0.6993, eval/one: 0.8280, eval/two: 0.7921, eval/three: 0.8767 \n",
      "Epoch: 450, train/loss: 0.1588, eval/loss: 0.4117, train/r2: 0.7951, eval/r2: 0.8325, train/one: 0.9714, train/two: 0.7317, train/three: 0.6823, eval/one: 0.8264, eval/two: 0.7955, eval/three: 0.8755 \n",
      "Epoch: 455, train/loss: 0.1694, eval/loss: 0.4091, train/r2: 0.7915, eval/r2: 0.8331, train/one: 0.9679, train/two: 0.7125, train/three: 0.6942, eval/one: 0.8276, eval/two: 0.7958, eval/three: 0.8758 \n",
      "Epoch: 460, train/loss: 0.1462, eval/loss: 0.4052, train/r2: 0.7806, eval/r2: 0.8345, train/one: 0.9765, train/two: 0.6194, train/three: 0.7460, eval/one: 0.8292, eval/two: 0.7975, eval/three: 0.8769 \n",
      "Epoch: 465, train/loss: 0.1267, eval/loss: 0.4029, train/r2: 0.8361, eval/r2: 0.8361, train/one: 0.9768, train/two: 0.7570, train/three: 0.7745, eval/one: 0.8301, eval/two: 0.8001, eval/three: 0.8780 \n",
      "Epoch: 470, train/loss: 0.1426, eval/loss: 0.4057, train/r2: 0.7987, eval/r2: 0.8352, train/one: 0.9764, train/two: 0.7170, train/three: 0.7028, eval/one: 0.8288, eval/two: 0.7986, eval/three: 0.8782 \n",
      "Epoch: 475, train/loss: 0.1525, eval/loss: 0.4078, train/r2: 0.7920, eval/r2: 0.8338, train/one: 0.9728, train/two: 0.6418, train/three: 0.7614, eval/one: 0.8280, eval/two: 0.7954, eval/three: 0.8781 \n",
      "Epoch: 480, train/loss: 0.1868, eval/loss: 0.4106, train/r2: 0.7968, eval/r2: 0.8324, train/one: 0.9605, train/two: 0.6862, train/three: 0.7437, eval/one: 0.8269, eval/two: 0.7925, eval/three: 0.8778 \n",
      "Epoch: 485, train/loss: 0.1876, eval/loss: 0.4118, train/r2: 0.7619, eval/r2: 0.8319, train/one: 0.9658, train/two: 0.6964, train/three: 0.6235, eval/one: 0.8264, eval/two: 0.7915, eval/three: 0.8778 \n",
      "Epoch: 490, train/loss: 0.1594, eval/loss: 0.4123, train/r2: 0.8316, eval/r2: 0.8316, train/one: 0.9654, train/two: 0.7218, train/three: 0.8077, eval/one: 0.8261, eval/two: 0.7908, eval/three: 0.8777 \n",
      "Epoch: 495, train/loss: 0.1490, eval/loss: 0.4129, train/r2: 0.8195, eval/r2: 0.8313, train/one: 0.9709, train/two: 0.7204, train/three: 0.7673, eval/one: 0.8259, eval/two: 0.7902, eval/three: 0.8777 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 72 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 72 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-124/metadata\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "mus = [0.5431315108096543,\n",
    " 0.5373212199292232,\n",
    " 0.5625938858971476,\n",
    " 0.5611233230776563,\n",
    " 0.5471325694126035]\n",
    "\n",
    "sigmas = [249.05215723502948,\n",
    " 248.6726517269414,\n",
    " 247.84789229685813,\n",
    " 247.65868506084584,\n",
    " 248.16012425733976]\n",
    "\n",
    "mus, sigmas = [], []\n",
    "scores = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "splits = kfold.split(range(96))\n",
    "\n",
    "for fold, (train_idx, eval_idx) in enumerate(splits):\n",
    "    MODEL_NAME = f\"resnet.finetune.fold.{fold}\"\n",
    "    checkpoint_name = f\"finetune.fold.{fold}.pt\"\n",
    "    \n",
    "    train_inputs = inputs[train_idx]\n",
    "    train_targets = targets[train_idx]\n",
    "    eval_inputs = inputs[eval_idx]\n",
    "    eval_targets = targets[eval_idx]\n",
    "    \n",
    "    mu, sigma = get_stats(train_inputs, p=False, r=True)\n",
    "    train_inputs = zscore(train_inputs, mu, sigma)\n",
    "    eval_inputs = zscore(eval_inputs, mu, sigma)\n",
    "    mus.append(mu)\n",
    "    sigmas.append(sigma)\n",
    "\n",
    "    train_ds = TensorDataset(train_inputs.float(), train_targets.float())\n",
    "    eval_ds = TensorDataset(eval_inputs.float(), eval_targets.float())\n",
    "    \n",
    "    BATCH_SIZE = len(train_ds)\n",
    "    train_dl, eval_dl = return_dls(train_ds, eval_ds, BATCH_SIZE, len(eval_ds))\n",
    "    \n",
    "    model = ResNet(input_channels=2, dropout=DROPOUT).to(device)\n",
    "    if fold == 0: print(get_model_size(model))\n",
    "    \n",
    "    ckpt = torch.load(f\"pretrain.fold.{fold}.pt\", weights_only=False)\n",
    "    print(ckpt[\"score\"])\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    scheduler = get_scheduler(optimizer, train_dl, EPOCHS)\n",
    "    score = train(\n",
    "            model, \n",
    "            optimizer, \n",
    "            device,\n",
    "            scaler,\n",
    "            scheduler,\n",
    "            train_dl, \n",
    "            eval_dl,\n",
    "            EPOCHS,\n",
    "            checkpoint_name,\n",
    "            neptune_run=setup_neptune(),\n",
    "        )\n",
    "    \n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32e55b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune.fold.0.pt 0.8201662037121897\n",
      "finetune.fold.1.pt 0.882358145491022\n",
      "finetune.fold.2.pt 0.8594031252916086\n",
      "finetune.fold.3.pt 0.832977530496494\n",
      "finetune.fold.4.pt 0.8438998270503034\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "path = \"/kaggle/working\"\n",
    "files = sorted(os.listdir(path))\n",
    "for f in files:\n",
    "    if \"finetune\" in f:\n",
    "        ckpt_path = os.path.join(path, f)\n",
    "        ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "        print(f, ckpt[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68de9b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48822867684523436"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"/kaggle/working/pretrain.fold.1.pt\", weights_only=False)\n",
    "ckpt[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "176d5a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 3623.216623942057, Std: 6772.114021862655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3973e7bf1e7f4cef920abe298c13ebf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.02771512232720852, Std: 2.6981143951416016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([96, 2, 2048]), torch.float32, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = load_test_data()\n",
    "get_stats(test)\n",
    "test = get_spectra_features(test)\n",
    "test = torch.tensor(test)\n",
    "test = zscore(test, 0.5373212199292232, 248.6726517269414).float()\n",
    "test.shape, test.dtype, get_stats(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58b4472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 2.440112045396947, Std: 2.335613724637091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((96, 3), None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(input_channels=2, dropout=0.5).to(device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = model(test.cuda())\n",
    "\n",
    "preds = preds.cpu().detach().double().numpy()\n",
    "preds.shape, get_stats(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "106352ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.15105102956295013, 10.833093643188477)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.min(), preds.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dce6794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Sodium Acetate</th>\n",
       "      <th>Magnesium Sulfate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.798570</td>\n",
       "      <td>1.251581</td>\n",
       "      <td>1.060938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5.863643</td>\n",
       "      <td>1.152817</td>\n",
       "      <td>2.101619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6.059584</td>\n",
       "      <td>1.126695</td>\n",
       "      <td>0.958145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.802019</td>\n",
       "      <td>1.260450</td>\n",
       "      <td>0.665935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9.207160</td>\n",
       "      <td>1.057580</td>\n",
       "      <td>1.141668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>3.193847</td>\n",
       "      <td>1.244646</td>\n",
       "      <td>-0.068723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>4.436370</td>\n",
       "      <td>1.213420</td>\n",
       "      <td>1.034073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>3.710596</td>\n",
       "      <td>1.220895</td>\n",
       "      <td>0.775212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>4.581892</td>\n",
       "      <td>1.231528</td>\n",
       "      <td>1.095441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>3.892003</td>\n",
       "      <td>1.279501</td>\n",
       "      <td>1.109248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID   Glucose  Sodium Acetate  Magnesium Sulfate\n",
       "0    1  3.798570        1.251581           1.060938\n",
       "1    2  5.863643        1.152817           2.101619\n",
       "2    3  6.059584        1.126695           0.958145\n",
       "3    4  4.802019        1.260450           0.665935\n",
       "4    5  9.207160        1.057580           1.141668\n",
       "..  ..       ...             ...                ...\n",
       "91  92  3.193847        1.244646          -0.068723\n",
       "92  93  4.436370        1.213420           1.034073\n",
       "93  94  3.710596        1.220895           0.775212\n",
       "94  95  4.581892        1.231528           1.095441\n",
       "95  96  3.892003        1.279501           1.109248\n",
       "\n",
       "[96 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']\n",
    "preds_df = pd.DataFrame(preds, columns=column_names)\n",
    "preds_df.insert(0, 'ID', [i+1 for i in range(len(preds_df))])\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dad1cd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('resnet.finetune.fold.4', 0.48822867684523436)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME, ckpt[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71b8e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Sodium Acetate</th>\n",
       "      <th>Magnesium Sulfate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.798570</td>\n",
       "      <td>1.251581</td>\n",
       "      <td>1.060938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5.863643</td>\n",
       "      <td>1.152817</td>\n",
       "      <td>2.101619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6.059584</td>\n",
       "      <td>1.126695</td>\n",
       "      <td>0.958145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.802019</td>\n",
       "      <td>1.260450</td>\n",
       "      <td>0.665935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9.207160</td>\n",
       "      <td>1.057580</td>\n",
       "      <td>1.141668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>3.193847</td>\n",
       "      <td>1.244646</td>\n",
       "      <td>-0.068723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>4.436370</td>\n",
       "      <td>1.213420</td>\n",
       "      <td>1.034073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>3.710596</td>\n",
       "      <td>1.220895</td>\n",
       "      <td>0.775212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>4.581892</td>\n",
       "      <td>1.231528</td>\n",
       "      <td>1.095441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>3.892003</td>\n",
       "      <td>1.279501</td>\n",
       "      <td>1.109248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID   Glucose  Sodium Acetate  Magnesium Sulfate\n",
       "0    1  3.798570        1.251581           1.060938\n",
       "1    2  5.863643        1.152817           2.101619\n",
       "2    3  6.059584        1.126695           0.958145\n",
       "3    4  4.802019        1.260450           0.665935\n",
       "4    5  9.207160        1.057580           1.141668\n",
       "..  ..       ...             ...                ...\n",
       "91  92  3.193847        1.244646          -0.068723\n",
       "92  93  4.436370        1.213420           1.034073\n",
       "93  94  3.710596        1.220895           0.775212\n",
       "94  95  4.581892        1.231528           1.095441\n",
       "95  96  3.892003        1.279501           1.109248\n",
       "\n",
       "[96 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"resnet.pretrain.4882.fold1.csv\"\n",
    "preds_df.to_csv(name, index=False)\n",
    "f = pd.read_csv(f\"/kaggle/working/{name}\")\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "235a2294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.79856992,  1.25158083,  1.06093752],\n",
       "       [ 5.86364269,  1.15281737,  2.10161853],\n",
       "       [ 6.05958366,  1.12669456,  0.9581452 ],\n",
       "       [ 4.80201864,  1.26045012,  0.66593462],\n",
       "       [ 9.20716   ,  1.05757987,  1.1416676 ],\n",
       "       [ 7.77510023,  1.1327076 ,  1.44393945],\n",
       "       [ 7.23803663,  1.15470648,  0.76583844],\n",
       "       [ 6.0335536 ,  1.18760645,  1.69648898],\n",
       "       [ 6.02058363,  1.21945333,  0.63327855],\n",
       "       [ 5.6684618 ,  1.25679851,  0.86719656],\n",
       "       [ 6.16663599,  1.23595548,  0.24023819],\n",
       "       [ 3.28187561,  1.24793601, -0.02351693],\n",
       "       [ 3.71338058,  1.23581684,  0.14943853],\n",
       "       [ 4.8654952 ,  1.18839943,  0.22163081],\n",
       "       [ 5.1359477 ,  1.19414449, -0.02370556],\n",
       "       [ 4.9015398 ,  1.24607182,  0.1454611 ],\n",
       "       [ 5.19880581,  1.25317705,  0.18482471],\n",
       "       [ 5.8379159 ,  1.21845615,  0.4257805 ],\n",
       "       [ 4.65041494,  1.27293575,  0.07994768],\n",
       "       [ 4.33357668,  1.22052288,  0.35409403],\n",
       "       [ 4.66607475,  1.26752329,  0.1781573 ],\n",
       "       [ 7.83023548,  1.16748261,  0.35211864],\n",
       "       [ 4.74812365,  1.21661675,  0.72776592],\n",
       "       [ 8.11464787,  1.1664362 ,  0.22832197],\n",
       "       [ 3.55558705,  1.22404885,  0.31689996],\n",
       "       [ 4.18159342,  1.21243811,  0.52532345],\n",
       "       [ 6.50832701,  1.24440145,  0.27133462],\n",
       "       [ 7.37761259,  1.17695463,  0.26100925],\n",
       "       [ 7.26346302,  1.21550643,  0.27223083],\n",
       "       [ 5.205338  ,  1.22508037,  1.18745863],\n",
       "       [ 3.71814489,  1.27552629,  0.5525251 ],\n",
       "       [ 8.1463089 ,  1.14761913,  1.33228767],\n",
       "       [ 3.3214891 ,  1.25086737,  0.54730219],\n",
       "       [ 3.96211648,  1.2674495 ,  0.29876035],\n",
       "       [ 5.93509388,  1.2192657 ,  1.03518927],\n",
       "       [ 4.37828779,  1.27028537,  1.02291715],\n",
       "       [ 8.02021313,  1.15509725,  0.64731538],\n",
       "       [ 4.33916998,  1.26628029, -0.01728654],\n",
       "       [ 3.69040298,  1.23726356,  0.14348479],\n",
       "       [ 6.90260935,  1.22040367,  0.6552214 ],\n",
       "       [ 4.75535345,  1.24524188, -0.15105103],\n",
       "       [ 5.3084054 ,  1.21771145,  0.56564987],\n",
       "       [ 4.13229656,  1.20193315,  1.67476654],\n",
       "       [ 3.82180023,  1.21004903,  0.93061978],\n",
       "       [ 2.7215488 ,  1.23601401,  1.05259454],\n",
       "       [ 4.98432589,  1.20714498,  1.31340659],\n",
       "       [ 4.54350185,  1.22970319,  1.58633363],\n",
       "       [ 3.28904247,  1.18576038,  0.17756084],\n",
       "       [ 4.53610849,  1.23411572,  0.50636321],\n",
       "       [ 5.49809599,  1.21257842,  1.65405989],\n",
       "       [ 3.58657432,  1.18841004,  2.12787557],\n",
       "       [ 6.05027819,  1.1866008 ,  0.25627694],\n",
       "       [ 7.64171505,  1.17754388, -0.05695644],\n",
       "       [ 3.45386648,  1.19007766,  1.54468274],\n",
       "       [ 6.6634655 ,  1.24246037, -0.04924702],\n",
       "       [ 6.25989151,  1.18999481,  1.56818616],\n",
       "       [ 5.13280821,  1.20118022,  2.03842473],\n",
       "       [ 5.18075418,  1.14967942,  1.85733235],\n",
       "       [ 2.80016661,  1.25972855,  1.1937418 ],\n",
       "       [ 4.99631548,  1.26607776,  0.32830128],\n",
       "       [ 3.06755614,  1.23631001,  0.80110222],\n",
       "       [ 2.9382937 ,  1.24227071,  0.68779981],\n",
       "       [ 7.66379833,  1.19075167,  0.32464269],\n",
       "       [ 6.64087343,  1.1714555 ,  0.82506329],\n",
       "       [ 7.09960938,  1.24346209,  0.70936102],\n",
       "       [ 6.52937126,  1.23077846,  0.55604577],\n",
       "       [ 5.33961535,  1.25067997,  0.52981395],\n",
       "       [ 7.47858906,  1.19035387,  0.46561947],\n",
       "       [ 7.78424263,  1.15704668,  0.2232703 ],\n",
       "       [ 7.82908106,  1.19068313,  0.42980671],\n",
       "       [ 8.19663048,  1.18962765,  0.13943015],\n",
       "       [ 7.39442492,  1.17062604,  0.71907884],\n",
       "       [10.83309364,  1.13365853,  0.60140365],\n",
       "       [ 6.93127489,  1.18552113,  1.37695527],\n",
       "       [ 7.75822115,  1.18183434,  0.30353692],\n",
       "       [ 8.13636208,  1.13723743,  1.45611417],\n",
       "       [ 5.16512966,  1.26258266,  0.84331501],\n",
       "       [ 6.84170341,  1.22062612,  0.79277134],\n",
       "       [ 4.75489855,  1.15412319,  1.02458024],\n",
       "       [ 4.39598036,  1.19579232,  1.5587554 ],\n",
       "       [ 5.91782808,  1.15780008,  0.85616499],\n",
       "       [ 2.63780856,  1.1816932 ,  1.65132904],\n",
       "       [ 3.87340569,  1.25407112,  1.16817474],\n",
       "       [ 3.36577892,  1.27802753,  1.82101583],\n",
       "       [ 4.43199682,  1.22154486,  1.79488921],\n",
       "       [ 6.08080244,  1.19906437,  0.19935051],\n",
       "       [ 3.46651888,  1.22607481,  0.06615098],\n",
       "       [ 3.94015408,  1.24089932,  0.320467  ],\n",
       "       [ 4.69106722,  1.29131949,  0.24693733],\n",
       "       [ 1.11632991,  1.19574869,  0.14013278],\n",
       "       [ 6.06467772,  1.20996892,  0.12017679],\n",
       "       [ 3.1938467 ,  1.2446456 , -0.06872346],\n",
       "       [ 4.43637037,  1.21342003,  1.03407311],\n",
       "       [ 3.71059561,  1.22089469,  0.77521211],\n",
       "       [ 4.58189249,  1.23152792,  1.09544063],\n",
       "       [ 3.89200258,  1.27950084,  1.1092484 ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd61bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
