{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4998a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup_reproducibility(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "SEED = 1000\n",
    "setup_reproducibility(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185d0c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def get_stats(tensor, p=True, r=False):\n",
    "    mean, std = tensor.mean(), tensor.std()\n",
    "    min, max =  tensor.min(), tensor.max()\n",
    "    \n",
    "    if p: print(f\"Min: {min}, Max: {max}, Mean: {mean}, Std: {std}\")\n",
    "    if r: return min, max, mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-8)\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "    \n",
    "\n",
    "def get_index(iterable):\n",
    "    return random.randint(0, len(iterable) - 1)\n",
    "\n",
    "\n",
    "def split(inputs, targets, seed):\n",
    "    return train_test_split(\n",
    "        inputs,\n",
    "        targets, \n",
    "        test_size=0.2,\n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    ) \n",
    "\n",
    "\n",
    "def show_waves(waves, dpi=100):\n",
    "    \"\"\"\n",
    "    waves: numpy array of shape (3, N)\n",
    "    Creates three separate figures that stretch wide.\n",
    "    \"\"\"\n",
    "\n",
    "    N = waves.shape[1]\n",
    "    t = np.arange(N)\n",
    "\n",
    "    # Wide aspect ratio; height modest so each window fills width\n",
    "    for i in range(waves.shape[0]):\n",
    "        fig = plt.figure(figsize=(14, 4), dpi=dpi)  # wide figure\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(t, waves[i], linewidth=1)\n",
    "        ax.set_title(f\"Wave {i+1}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()  # reduce margins to use width\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def hf_ds_download(hf_token, repo_id):\n",
    "    login(hf_token[1:])\n",
    "    return snapshot_download(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "def get_advanced_spectra_features(X):\n",
    "    \"\"\"Create multi-channel features from spectra: raw, 1st derivative, 2nd derivative.\"\"\"\n",
    "    X_processed = np.zeros_like(X)\n",
    "    # Baseline correction and SNV\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        poly = np.polyfit(np.arange(X.shape[1]), X[i], 3)\n",
    "        baseline = np.polyval(poly, np.arange(X.shape[1]))\n",
    "        corrected_spec = X[i] - baseline\n",
    "        X_processed[i] = (corrected_spec - corrected_spec.mean()) / (corrected_spec.std() + 1e-8)\n",
    "\n",
    "    # Calculate derivatives\n",
    "    deriv1 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=1, axis=1)\n",
    "    deriv2 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=2, axis=1)\n",
    "\n",
    "    # Stack as channels\n",
    "    return np.stack([X_processed, deriv1, deriv2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfbf156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58096ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_all_datasets():\n",
    "    train_inputs = []\n",
    "    train_targets = []\n",
    "    \n",
    "    timegate = pd.read_csv(os.path.join(path, files[1]))\n",
    "\n",
    "    timegate.drop(columns=\"fold_idx\", inplace=True)\n",
    "    timegate.drop(columns=\"MSM_present\", inplace=True)\n",
    "    timegate_inputs = timegate[timegate.columns[:-3]].to_numpy()\n",
    "    timegate_targets = timegate[timegate.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(timegate_inputs)\n",
    "    train_targets.append(timegate_targets)\n",
    "    \n",
    "    mettler_toledo = pd.read_csv(os.path.join(path, files[2]))\n",
    "\n",
    "    mettler_toledo.drop(columns=\"fold_idx\", inplace=True)\n",
    "    mettler_toledo.drop(columns=\"MSM_present\", inplace=True)\n",
    "    mettler_toledo_inputs = mettler_toledo[mettler_toledo.columns[:-3]].to_numpy()\n",
    "    mettler_toledo_targets = mettler_toledo[mettler_toledo.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(mettler_toledo_inputs)\n",
    "    train_targets.append(mettler_toledo_targets)\n",
    "    \n",
    "    kaiser = pd.read_csv(os.path.join(path, files[3]))\n",
    "\n",
    "    kaiser.drop(columns=\"fold_idx\", inplace=True)\n",
    "    kaiser.drop(columns=\"MSM_present\", inplace=True)\n",
    "    kaiser_inputs = kaiser[kaiser.columns[:-3]].to_numpy()\n",
    "    kaiser_targets = kaiser[kaiser.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(kaiser_inputs)\n",
    "    train_targets.append(kaiser_targets)\n",
    "    \n",
    "    anton = pd.read_csv(os.path.join(path, files[4]))\n",
    "\n",
    "    anton.drop(columns=\"fold_idx\", inplace=True)\n",
    "    anton.drop(columns=\"MSM_present\", inplace=True)\n",
    "    anton_inputs = anton[anton.columns[:-3]].to_numpy()\n",
    "    anton_targets = anton[anton.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(anton_inputs)\n",
    "    train_targets.append(anton_targets)\n",
    "    \n",
    "    tornado = pd.read_csv(os.path.join(path, files[7]))\n",
    "\n",
    "    tornado.drop(columns=\"fold_idx\", inplace=True)\n",
    "    tornado.drop(columns=\"MSM_present\", inplace=True)\n",
    "    tornado_inputs = tornado[tornado.columns[:-3]].to_numpy()\n",
    "    tornado_targets = tornado[tornado.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(tornado_inputs)\n",
    "    train_targets.append(tornado_targets)\n",
    "    \n",
    "    csv_path = os.path.join(path, files[8])\n",
    "    tec5 = pd.read_csv(csv_path)\n",
    "\n",
    "    tec5.drop(columns=\"fold_idx\", inplace=True)\n",
    "    tec5.drop(columns=\"MSM_present\", inplace=True)\n",
    "    tec5_inputs = tec5[tec5.columns[:-3]].to_numpy()\n",
    "    tec5_targets = tec5[tec5.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(tec5_inputs)\n",
    "    train_targets.append(tec5_targets)\n",
    "    \n",
    "    csv_path = os.path.join(path, files[9])\n",
    "    metrohm = pd.read_csv(csv_path)\n",
    "\n",
    "    metrohm.drop(columns=\"fold_idx\", inplace=True)\n",
    "    metrohm.drop(columns=\"MSM_present\", inplace=True)\n",
    "    metrohm_inputs = metrohm[metrohm.columns[:-3]].to_numpy()\n",
    "    metrohm_targets = metrohm[metrohm.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(metrohm_inputs)\n",
    "    train_targets.append(metrohm_targets)\n",
    "    \n",
    "    csv_path = os.path.join(path, files[10])\n",
    "    anton785 = pd.read_csv(csv_path)\n",
    "\n",
    "    anton785.drop(columns=\"fold_idx\", inplace=True)\n",
    "    anton785.drop(columns=\"MSM_present\", inplace=True)\n",
    "    anton785_inputs = anton785[anton785.columns[:-3]].to_numpy()\n",
    "    anton785_targets = anton785[anton785.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(anton785_inputs)\n",
    "    train_targets.append(anton785_targets)\n",
    "    \n",
    "    return train_inputs, train_targets\n",
    "\n",
    "inputs, targets = load_all_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
