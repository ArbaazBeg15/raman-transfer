{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75ce4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -q  neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4998a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "SEED = 1000\n",
    "setup_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185d0c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, train_dl, epochs):\n",
    "    total_training_steps = len(train_dl) * epochs\n",
    "    warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "\n",
    "\n",
    "def get_stats(tensor, p=True, r=False):\n",
    "    mean, std = tensor.mean(), tensor.std()\n",
    "    if p: print(f\"Mean: {mean}, Std: {std}\")\n",
    "    if r: return mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-8)\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "    \n",
    "\n",
    "def get_index(iterable):\n",
    "    return random.randint(0, len(iterable) - 1)\n",
    "\n",
    "\n",
    "def get_indices(iterable, n):\n",
    "    return random.sample(range(len(iterable)), n)\n",
    "\n",
    "\n",
    "def split(inputs, targets, seed):\n",
    "    return train_test_split(\n",
    "        inputs,\n",
    "        targets, \n",
    "        test_size=0.2,\n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    ) \n",
    "\n",
    "\n",
    "def show_waves(waves, dpi=100):\n",
    "    \"\"\"\n",
    "    waves: numpy array of shape (3, N)\n",
    "    Creates three separate figures that stretch wide.\n",
    "    \"\"\"\n",
    "    N = waves.shape[1]\n",
    "    t = np.arange(N)\n",
    "\n",
    "    # Wide aspect ratio; height modest so each window fills width\n",
    "    for i in range(waves.shape[0]):\n",
    "        fig = plt.figure(figsize=(14, 4), dpi=dpi)  # wide figure\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(t, waves[i], linewidth=1)\n",
    "        ax.set_title(f\"Wave {i+1}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()  # reduce margins to use width\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def hf_ds_download(hf_token, repo_id):\n",
    "    login(hf_token[1:])\n",
    "    return snapshot_download(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "def get_spectra_features(X, b=False):\n",
    "    \"\"\"Create multi-channel features from spectra: raw, 1st derivative, 2nd derivative.\"\"\"\n",
    "    X_processed = np.zeros_like(X)\n",
    "    # Baseline correction and SNV\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        poly = np.polyfit(np.arange(X.shape[1]), X[i], 3)\n",
    "        baseline = np.polyval(poly, np.arange(X.shape[1]))\n",
    "        corrected_spec = X[i] - baseline\n",
    "        #X_processed[i] = (corrected_spec - corrected_spec.mean()) / (corrected_spec.std() + 1e-8)\n",
    "        X_processed[i] = corrected_spec\n",
    "        \n",
    "    # Calculate derivatives\n",
    "    deriv1 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=1, axis=1)\n",
    "    deriv2 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=2, axis=1)\n",
    "\n",
    "    if b: return np.stack([X_processed, deriv1, deriv2], axis=1)\n",
    "    return np.stack([deriv1, deriv2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bfbf156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'sample_submission.csv'),\n",
       " (1, 'timegate.csv'),\n",
       " (2, 'mettler_toledo.csv'),\n",
       " (3, 'kaiser.csv'),\n",
       " (4, 'anton_532.csv'),\n",
       " (5, 'transfer_plate.csv'),\n",
       " (6, '96_samples.csv'),\n",
       " (7, 'tornado.csv'),\n",
       " (8, 'tec5.csv'),\n",
       " (9, 'metrohm.csv'),\n",
       " (10, 'anton_785.csv')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58096ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_transfer_data():\n",
    "    csv_path = os.path.join(path, files[5])\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    input_cols = df.columns[1:2049]\n",
    "    target_cols = df.columns[2050:]\n",
    "\n",
    "    targets  = df[target_cols].dropna().to_numpy()\n",
    "\n",
    "    df = df[input_cols]\n",
    "    df['Unnamed: 1'] = df['Unnamed: 1'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "    df['Unnamed: 2048'] = df['Unnamed: 2048'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "\n",
    "    inputs = df.to_numpy().reshape(-1, 2, 2048)\n",
    "    inputs = inputs.mean(axis=1)\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    test = pd.read_csv(os.path.join(path, files[6]))\n",
    "\n",
    "    row1 = test.columns[1:].to_numpy().copy()\n",
    "    row1[-1] = \"5611\"\n",
    "    row1 = row1.astype(np.float64)\n",
    "\n",
    "\n",
    "    cols = test.columns[1:]\n",
    "    test = test[cols]\n",
    "    test[\" 5611]\"] = test[\" 5611]\"].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "    test = test.to_numpy()\n",
    "\n",
    "    test = np.insert(test, 0, row1, axis=0)\n",
    "    return test.reshape(-1, 2, 2048).mean(axis=1)\n",
    "\n",
    "\n",
    "def load_all_datasets():\n",
    "    train_inputs = []\n",
    "    train_targets = []\n",
    "    \n",
    "    timegate = pd.read_csv(os.path.join(path, files[1]))\n",
    "\n",
    "    timegate.drop(columns=\"fold_idx\", inplace=True)\n",
    "    timegate.drop(columns=\"MSM_present\", inplace=True)\n",
    "    timegate_inputs = timegate[timegate.columns[:-3]].to_numpy()\n",
    "    timegate_targets = timegate[timegate.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(timegate_inputs)\n",
    "    train_targets.append(timegate_targets)\n",
    "    \n",
    "    mettler_toledo = pd.read_csv(os.path.join(path, files[2]))\n",
    "\n",
    "    mettler_toledo.drop(columns=\"fold_idx\", inplace=True)\n",
    "    mettler_toledo.drop(columns=\"MSM_present\", inplace=True)\n",
    "    mettler_toledo_inputs = mettler_toledo[mettler_toledo.columns[:-3]].to_numpy()\n",
    "    mettler_toledo_targets = mettler_toledo[mettler_toledo.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(mettler_toledo_inputs)\n",
    "    train_targets.append(mettler_toledo_targets)\n",
    "    \n",
    "    kaiser = pd.read_csv(os.path.join(path, files[3]))\n",
    "\n",
    "    kaiser.drop(columns=\"fold_idx\", inplace=True)\n",
    "    kaiser.drop(columns=\"MSM_present\", inplace=True)\n",
    "    kaiser_inputs = kaiser[kaiser.columns[:-3]].to_numpy()\n",
    "    kaiser_targets = kaiser[kaiser.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(kaiser_inputs)\n",
    "    train_targets.append(kaiser_targets)\n",
    "    \n",
    "    anton = pd.read_csv(os.path.join(path, files[4]))\n",
    "\n",
    "    anton.drop(columns=\"fold_idx\", inplace=True)\n",
    "    anton.drop(columns=\"MSM_present\", inplace=True)\n",
    "    anton_inputs = anton[anton.columns[:-3]].to_numpy()\n",
    "    anton_targets = anton[anton.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(anton_inputs)\n",
    "    train_targets.append(anton_targets)\n",
    "    \n",
    "    tornado = pd.read_csv(os.path.join(path, files[7]))\n",
    "\n",
    "    tornado.drop(columns=\"fold_idx\", inplace=True)\n",
    "    tornado.drop(columns=\"MSM_present\", inplace=True)\n",
    "    tornado_inputs = tornado[tornado.columns[:-3]].to_numpy()\n",
    "    tornado_targets = tornado[tornado.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(tornado_inputs)\n",
    "    train_targets.append(tornado_targets)\n",
    "    \n",
    "    csv_path = os.path.join(path, files[8])\n",
    "    tec5 = pd.read_csv(csv_path)\n",
    "\n",
    "    tec5.drop(columns=\"fold_idx\", inplace=True)\n",
    "    tec5.drop(columns=\"MSM_present\", inplace=True)\n",
    "    tec5_inputs = tec5[tec5.columns[:-3]].to_numpy()\n",
    "    tec5_targets = tec5[tec5.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(tec5_inputs)\n",
    "    train_targets.append(tec5_targets)\n",
    "    \n",
    "    csv_path = os.path.join(path, files[9])\n",
    "    metrohm = pd.read_csv(csv_path)\n",
    "\n",
    "    metrohm.drop(columns=\"fold_idx\", inplace=True)\n",
    "    metrohm.drop(columns=\"MSM_present\", inplace=True)\n",
    "    metrohm_inputs = metrohm[metrohm.columns[:-3]].to_numpy()\n",
    "    metrohm_targets = metrohm[metrohm.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(metrohm_inputs)\n",
    "    train_targets.append(metrohm_targets)\n",
    "    \n",
    "    csv_path = os.path.join(path, files[10])\n",
    "    anton785 = pd.read_csv(csv_path)\n",
    "\n",
    "    anton785.drop(columns=\"fold_idx\", inplace=True)\n",
    "    anton785.drop(columns=\"MSM_present\", inplace=True)\n",
    "    anton785_inputs = anton785[anton785.columns[:-3]].to_numpy()\n",
    "    anton785_targets = anton785[anton785.columns[-3:]].to_numpy()\n",
    "\n",
    "    train_inputs.append(anton785_inputs)\n",
    "    train_targets.append(anton785_targets)\n",
    "    \n",
    "    return train_inputs, train_targets\n",
    "\n",
    "inputs_list, targets_list = load_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40aa78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for i in range(len(inputs_list)):\n",
    "        indices = get_indices(inputs_list[i], 96)\n",
    "        inputs_list[i] = inputs_list[i][indices]\n",
    "        targets_list[i] = targets_list[i][indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33584f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e659fcd20fcc457587d2e3676a27701d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea2bf447c1c4646b01c3aa5e32aa93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd1da12864343d98f3f44d5db5cb6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ffe05badd849b08d4c5e22e101e2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0fc6209b1445be83771e5015a51fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d39d4e0003e4da7a45313edcb060329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada5e4933c2e46ef8701ef79917f1574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1179af746ec41fb8bfa3fbe254b14f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(133, 2, 511),\n",
       " (275, 2, 2901),\n",
       " (134, 2, 6593),\n",
       " (270, 2, 1651),\n",
       " (385, 2, 3001),\n",
       " (395, 2, 3126),\n",
       " (399, 2, 1917),\n",
       " (270, 2, 1101)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_list = [get_spectra_features(inputs_list[i]) for i in range(len(inputs_list))]\n",
    "[i.shape for i in inputs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04b147b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([133, 2, 2048]),\n",
       "  torch.Size([275, 2, 2048]),\n",
       "  torch.Size([134, 2, 2048]),\n",
       "  torch.Size([270, 2, 2048]),\n",
       "  torch.Size([385, 2, 2048]),\n",
       "  torch.Size([395, 2, 2048]),\n",
       "  torch.Size([399, 2, 2048]),\n",
       "  torch.Size([270, 2, 2048])],\n",
       " [torch.Size([133, 3]),\n",
       "  torch.Size([275, 3]),\n",
       "  torch.Size([134, 3]),\n",
       "  torch.Size([270, 3]),\n",
       "  torch.Size([385, 3]),\n",
       "  torch.Size([395, 3]),\n",
       "  torch.Size([399, 3]),\n",
       "  torch.Size([270, 3])])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def foo(inputs, targets):\n",
    "    for i in range(len(inputs)):\n",
    "        x = inputs[i]\n",
    "        x = torch.tensor(x)\n",
    "        inputs[i] = F.interpolate(x, size=2048, mode=\"nearest-exact\")\n",
    "\n",
    "    #inputs = torch.cat(inputs)\n",
    "    targets = [torch.tensor(t) for t in targets]\n",
    "    #targets = torch.cat(targets)\n",
    "    return inputs, targets\n",
    "    \n",
    "inputs_list, targets_list = foo(inputs_list, targets_list)\n",
    "[i.shape for i in inputs_list], [i.shape for i in targets_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b16a1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "if False:\n",
    "    for i in range(len(inputs_list)):\n",
    "        inputs = inputs_list[i]\n",
    "        targets = targets_list[i]\n",
    "        train_inputs, eval_inputs, train_targets, eval_targets = split(inputs, targets, SEED)\n",
    "        \n",
    "        inputs_list[i] = (train_inputs, eval_inputs)\n",
    "        targets_list[i] = (train_targets, eval_targets)\n",
    "        \n",
    "    def foo(inputs, targets):\n",
    "        for i in range(len(inputs)):\n",
    "            train_inputs, eval_inputs = inputs[i]\n",
    "            train_inputs = torch.tensor(train_inputs)\n",
    "            eval_inputs = torch.tensor(eval_inputs)\n",
    "            train_inputs = F.interpolate(train_inputs, size=2048, mode=\"nearest-exact\")\n",
    "            eval_inputs = F.interpolate(eval_inputs, size=2048, mode=\"nearest-exact\")   \n",
    "            inputs[i] = (train_inputs, eval_inputs)\n",
    "\n",
    "        train_inputs = [i[0] for i in inputs]\n",
    "        eval_inputs = [i[1] for i in inputs]\n",
    "        train_inputs = torch.cat(train_inputs)\n",
    "        eval_inputs = torch.cat(eval_inputs)\n",
    "        \n",
    "        train_targets = [torch.tensor(t[0]) for t in targets]\n",
    "        eval_targets = [torch.tensor(t[1]) for t in targets]\n",
    "        train_targets = torch.cat(train_targets)\n",
    "        eval_targets = torch.cat(eval_targets)\n",
    "\n",
    "        return train_inputs, eval_inputs, train_targets, eval_targets\n",
    "        \n",
    "    train_inputs, eval_inputs, train_targets, eval_targets = foo(inputs_list, targets_list)\n",
    "    train_inputs.shape, eval_inputs.shape, train_targets.shape, eval_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74ef9524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1de66aea044411082022a7ee3756d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transfer_inputs, transfer_targets = load_transfer_data()\n",
    "transfer_inputs = get_spectra_features(transfer_inputs)\n",
    "#train_transfer_inputs, eval_transfer_inputs, train_transfer_targets, eval_transfer_targets = split(transfer_inputs, transfer_targets, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac83271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_list.append(torch.tensor(transfer_inputs))\n",
    "targets_list.append(torch.tensor(transfer_targets))\n",
    "len(targets_list), len(inputs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f970b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([133, 2, 2048]),\n",
       "  torch.Size([275, 2, 2048]),\n",
       "  torch.Size([134, 2, 2048]),\n",
       "  torch.Size([270, 2, 2048]),\n",
       "  torch.Size([385, 2, 2048]),\n",
       "  torch.Size([395, 2, 2048]),\n",
       "  torch.Size([399, 2, 2048]),\n",
       "  torch.Size([270, 2, 2048]),\n",
       "  torch.Size([96, 2, 2048])],\n",
       " [torch.Size([133, 3]),\n",
       "  torch.Size([275, 3]),\n",
       "  torch.Size([134, 3]),\n",
       "  torch.Size([270, 3]),\n",
       "  torch.Size([385, 3]),\n",
       "  torch.Size([395, 3]),\n",
       "  torch.Size([399, 3]),\n",
       "  torch.Size([270, 3]),\n",
       "  torch.Size([96, 3])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in inputs_list], [i.shape for i in targets_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2304b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    train_inputs = torch.cat([train_inputs, torch.tensor(train_transfer_inputs)])\n",
    "    eval_inputs = torch.cat([eval_inputs, torch.tensor(eval_transfer_inputs)])\n",
    "    train_targets = torch.cat([train_targets, torch.tensor(train_transfer_targets)])\n",
    "    eval_targets = torch.cat([eval_targets, torch.tensor(eval_transfer_targets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2788ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    min, max, mu, sigma = get_stats(train_inputs, r=True)\n",
    "    train_inputs = zscore(train_inputs)\n",
    "    eval_inputs = zscore(eval_inputs)\n",
    "    get_stats(train_inputs), get_stats(eval_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf4e0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "if False:\n",
    "    train_ds = TensorDataset(train_inputs.float(), train_targets.float())\n",
    "    eval_ds = TensorDataset(eval_inputs.float(), eval_targets.float())\n",
    "    len(train_ds), len(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "982956a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+5232)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds, train_batch_size, eval_batch_size):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ac2d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "        if DROPOUT: neptune_run[\"h_parameters\"] = {\"dropout\": DROPOUT}\n",
    "        if DROP_PATH_RATE: neptune_run[\"h_parameters\"] = {\"drop_path_rate\": DROP_PATH_RATE}\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "560637b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    \n",
    "    dim1 = r2_score(targets[:, 0], preds[:, 0])\n",
    "    dim2 = r2_score(targets[:, 1], preds[:, 1])\n",
    "    dim3 = r2_score(targets[:, 2], preds[:, 2])\n",
    "    \n",
    "    mean_r2 = (dim1 + dim2 + dim3) / 3\n",
    "    \n",
    "    return dim1, dim2, dim3, mean_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e91d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A residual block with two 1D convolutional layers.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.elu = nn.ELU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.elu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.elu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"A deeper ResNet-style 1D CNN for Raman spectra.\"\"\"\n",
    "    def __init__(self, dropout, input_channels=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.elu = nn.GELU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout), # Increased dropout for better regularization\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, stride=s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26cd0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    device,\n",
    "    scaler, \n",
    "    scheduler,\n",
    "    train_dl,\n",
    "    eval_dl,\n",
    "    epochs,\n",
    "    checkpoint_name,\n",
    "    score=-float(\"inf\"),\n",
    "    neptune_run=None,\n",
    "    p=True,\n",
    "):  \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for inputs, targets in train_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            if neptune_run is not None:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            total_loss += loss.detach().cpu()\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        all_logits = torch.cat(all_logits)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "        total_loss = total_loss / len(train_dl)\n",
    "        \n",
    "        model.eval()\n",
    "        eval_total_loss = 0.0\n",
    "        eval_all_logits = []\n",
    "        eval_all_targets = []\n",
    "\n",
    "        for inputs, targets in eval_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                #with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "\n",
    "            eval_total_loss += loss.detach().cpu()\n",
    "            eval_all_logits.append(logits.detach().cpu())\n",
    "            eval_all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        eval_all_logits = torch.cat(eval_all_logits)\n",
    "        eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "        eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "        eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "        \n",
    "        if eval_r2 > score:\n",
    "            score = eval_r2\n",
    "            data = {\"state_dict\": model.state_dict()}\n",
    "            data[\"epoch\"] = epoch \n",
    "            data[\"score\"] = score\n",
    "            torch.save(data, f\"/kaggle/working/{checkpoint_name}\")\n",
    "        \n",
    "        if neptune_run is not None:\n",
    "            neptune_run[\"train/loss\"].append(total_loss)\n",
    "            neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "            neptune_run[\"train/r2\"].append(r2)\n",
    "            neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "            neptune_run[\"train/one\"].append(one)\n",
    "            neptune_run[\"train/two\"].append(two)\n",
    "            neptune_run[\"train/three\"].append(three)\n",
    "            neptune_run[\"eval/one\"].append(eval_one)\n",
    "            neptune_run[\"eval/two\"].append(eval_two)\n",
    "            neptune_run[\"eval/three\"].append(eval_three)\n",
    "            \n",
    "        if p and epoch % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, \"\n",
    "                f\"train/loss: {total_loss:.4f}, \"\n",
    "                f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "                f\"train/r2: {r2:.4f}, \"\n",
    "                f\"eval/r2: {eval_r2:.4f}, \"\n",
    "                f\"train/one: {one:.4f}, \"\n",
    "                f\"train/two: {two:.4f}, \"\n",
    "                f\"train/three: {three:.4f}, \"\n",
    "                f\"eval/one: {eval_one:.4f}, \"\n",
    "                f\"eval/two: {eval_two:.4f}, \"\n",
    "                f\"eval/three: {eval_three:.4f} \"\n",
    "            )\n",
    "            \n",
    "    if neptune_run is not None: neptune_run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6678e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings#; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "EPOCHS = 500\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "\n",
    "DROPOUT = 0.5\n",
    "#DROP_PATH_RATE = 0.0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853355ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.981251\n",
      "None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c932d7eef3437cb5fb1c016e51cc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 11.2464, eval/loss: 9.5109, train/r2: -1.2558, eval/r2: -0.9679, train/one: -0.7138, train/two: -2.3070, train/three: -0.7467, eval/one: -0.6092, eval/two: -1.5417, eval/three: -0.7527 \n",
      "Epoch: 5, train/loss: 10.6874, eval/loss: 9.4922, train/r2: -1.1235, eval/r2: -0.9154, train/one: -0.6317, train/two: -2.1010, train/three: -0.6378, eval/one: -0.6120, eval/two: -1.3989, eval/three: -0.7352 \n",
      "Epoch: 10, train/loss: 9.4248, eval/loss: 9.4877, train/r2: -0.7439, eval/r2: -0.8537, train/one: -0.4481, train/two: -1.3500, train/three: -0.4337, eval/one: -0.6184, eval/two: -1.2222, eval/three: -0.7204 \n",
      "Epoch: 15, train/loss: 7.7384, eval/loss: 9.0460, train/r2: -0.3398, eval/r2: -0.6978, train/one: -0.1956, train/two: -0.6556, train/three: -0.1683, eval/one: -0.5481, eval/two: -0.8907, eval/three: -0.6544 \n",
      "Epoch: 20, train/loss: 6.3762, eval/loss: 6.9918, train/r2: -0.1368, eval/r2: -0.2394, train/one: 0.0227, train/two: -0.4020, train/three: -0.0312, eval/one: -0.2056, eval/two: -0.2559, eval/three: -0.2567 \n",
      "Epoch: 25, train/loss: 5.8953, eval/loss: 6.3131, train/r2: -0.1307, eval/r2: -0.0887, train/one: 0.1038, train/two: -0.5157, train/three: 0.0199, eval/one: -0.0867, eval/two: 0.0021, eval/three: -0.1814 \n",
      "Epoch: 30, train/loss: 6.1326, eval/loss: 7.3567, train/r2: -0.1415, eval/r2: -0.1693, train/one: 0.0610, train/two: -0.5189, train/three: 0.0334, eval/one: -0.3007, eval/two: -0.0778, eval/three: -0.1294 \n",
      "Epoch: 35, train/loss: 5.7177, eval/loss: 6.5706, train/r2: -0.0405, eval/r2: -0.0372, train/one: 0.1267, train/two: -0.3036, train/three: 0.0554, eval/one: -0.1621, eval/two: 0.0616, eval/three: -0.0112 \n",
      "Epoch: 40, train/loss: 5.5271, eval/loss: 5.7084, train/r2: -0.0136, eval/r2: 0.0579, train/one: 0.1559, train/two: -0.2881, train/three: 0.0913, eval/one: -0.0014, eval/two: 0.0974, eval/three: 0.0776 \n",
      "Epoch: 45, train/loss: 5.3754, eval/loss: 5.3324, train/r2: 0.0244, eval/r2: 0.0454, train/one: 0.1793, train/two: -0.2120, train/three: 0.1060, eval/one: 0.0811, eval/two: 0.0105, eval/three: 0.0445 \n",
      "Epoch: 50, train/loss: 5.0193, eval/loss: 6.6129, train/r2: 0.0279, eval/r2: -0.1957, train/one: 0.2438, train/two: -0.2516, train/three: 0.0914, eval/one: -0.1539, eval/two: -0.4003, eval/three: -0.0328 \n",
      "Epoch: 55, train/loss: 4.6425, eval/loss: 6.5759, train/r2: 0.0649, eval/r2: -0.2142, train/one: 0.3025, train/two: -0.2722, train/three: 0.1643, eval/one: -0.1459, eval/two: -0.4782, eval/three: -0.0185 \n",
      "Epoch: 60, train/loss: 4.4270, eval/loss: 4.7690, train/r2: 0.0578, eval/r2: 0.0632, train/one: 0.3429, train/two: -0.3158, train/three: 0.1463, eval/one: 0.1859, eval/two: -0.1426, eval/three: 0.1462 \n",
      "Epoch: 65, train/loss: 4.3334, eval/loss: 9.1764, train/r2: 0.0610, eval/r2: -0.2404, train/one: 0.3578, train/two: -0.3392, train/three: 0.1645, eval/one: -0.6823, eval/two: -0.0184, eval/three: -0.0206 \n",
      "Epoch: 70, train/loss: 3.7765, eval/loss: 4.1053, train/r2: 0.1490, eval/r2: 0.1845, train/one: 0.4465, train/two: -0.2228, train/three: 0.2233, eval/one: 0.3037, eval/two: 0.0206, eval/three: 0.2293 \n",
      "Epoch: 75, train/loss: 3.7048, eval/loss: 3.8722, train/r2: 0.1663, eval/r2: 0.2032, train/one: 0.4567, train/two: -0.1986, train/three: 0.2406, eval/one: 0.3493, eval/two: 0.0223, eval/three: 0.2381 \n",
      "Epoch: 80, train/loss: 3.4481, eval/loss: 3.9910, train/r2: 0.1808, eval/r2: 0.1872, train/one: 0.4972, train/two: -0.2475, train/three: 0.2926, eval/one: 0.3258, eval/two: -0.0059, eval/three: 0.2417 \n",
      "Epoch: 85, train/loss: 3.3458, eval/loss: 4.2177, train/r2: 0.1903, eval/r2: 0.2034, train/one: 0.5146, train/two: -0.2384, train/three: 0.2946, eval/one: 0.2690, eval/two: 0.0170, eval/three: 0.3241 \n",
      "Epoch: 90, train/loss: 3.1176, eval/loss: 6.6217, train/r2: 0.2270, eval/r2: 0.0156, train/one: 0.5513, train/two: -0.1847, train/three: 0.3144, eval/one: -0.1975, eval/two: 0.0597, eval/three: 0.1846 \n",
      "Epoch: 95, train/loss: 3.0138, eval/loss: 3.6862, train/r2: 0.2377, eval/r2: 0.2640, train/one: 0.5654, train/two: -0.2114, train/three: 0.3592, eval/one: 0.3684, eval/two: 0.0498, eval/three: 0.3737 \n",
      "Epoch: 100, train/loss: 2.9708, eval/loss: 3.8545, train/r2: 0.2466, eval/r2: 0.2700, train/one: 0.5709, train/two: -0.2089, train/three: 0.3778, eval/one: 0.3278, eval/two: 0.0576, eval/three: 0.4247 \n",
      "Epoch: 105, train/loss: 2.9118, eval/loss: 3.7860, train/r2: 0.2456, eval/r2: 0.2551, train/one: 0.5754, train/two: -0.2884, train/three: 0.4498, eval/one: 0.3499, eval/two: 0.0545, eval/three: 0.3610 \n",
      "Epoch: 110, train/loss: 2.8461, eval/loss: 3.5629, train/r2: 0.2858, eval/r2: 0.3114, train/one: 0.5820, train/two: -0.2049, train/three: 0.4805, eval/one: 0.3793, eval/two: 0.0798, eval/three: 0.4752 \n",
      "Epoch: 115, train/loss: 2.7146, eval/loss: 4.0690, train/r2: 0.3022, eval/r2: 0.2484, train/one: 0.6031, train/two: -0.1923, train/three: 0.4959, eval/one: 0.2886, eval/two: 0.0640, eval/three: 0.3926 \n",
      "Epoch: 120, train/loss: 2.6342, eval/loss: 3.6164, train/r2: 0.3085, eval/r2: 0.3251, train/one: 0.6150, train/two: -0.2095, train/three: 0.5201, eval/one: 0.3599, eval/two: 0.0716, eval/three: 0.5437 \n",
      "Epoch: 125, train/loss: 2.5962, eval/loss: 3.5773, train/r2: 0.3120, eval/r2: 0.3370, train/one: 0.6193, train/two: -0.2295, train/three: 0.5462, eval/one: 0.3643, eval/two: 0.0776, eval/three: 0.5692 \n",
      "Epoch: 130, train/loss: 2.5562, eval/loss: 4.0413, train/r2: 0.3260, eval/r2: 0.1930, train/one: 0.6268, train/two: -0.1806, train/three: 0.5318, eval/one: 0.3227, eval/two: 0.0919, eval/three: 0.1642 \n",
      "Epoch: 135, train/loss: 2.3974, eval/loss: 3.5735, train/r2: 0.3262, eval/r2: 0.3530, train/one: 0.6538, train/two: -0.2220, train/three: 0.5470, eval/one: 0.3615, eval/two: 0.1072, eval/three: 0.5903 \n",
      "Epoch: 140, train/loss: 2.3984, eval/loss: 3.4878, train/r2: 0.3594, eval/r2: 0.3453, train/one: 0.6507, train/two: -0.1289, train/three: 0.5565, eval/one: 0.3821, eval/two: 0.0845, eval/three: 0.5692 \n",
      "Epoch: 145, train/loss: 2.6071, eval/loss: 3.9828, train/r2: 0.3390, eval/r2: 0.3273, train/one: 0.6120, train/two: -0.1847, train/three: 0.5898, eval/one: 0.2775, eval/two: 0.0993, eval/three: 0.6052 \n",
      "Epoch: 150, train/loss: 2.4644, eval/loss: 3.3731, train/r2: 0.3351, eval/r2: 0.3578, train/one: 0.6370, train/two: -0.2297, train/three: 0.5979, eval/one: 0.4025, eval/two: 0.0800, eval/three: 0.5910 \n",
      "Epoch: 155, train/loss: 2.3117, eval/loss: 3.3442, train/r2: 0.3804, eval/r2: 0.3718, train/one: 0.6598, train/two: -0.1345, train/three: 0.6160, eval/one: 0.4029, eval/two: 0.0799, eval/three: 0.6326 \n",
      "Epoch: 160, train/loss: 2.2193, eval/loss: 3.3777, train/r2: 0.3820, eval/r2: 0.3506, train/one: 0.6757, train/two: -0.1517, train/three: 0.6220, eval/one: 0.4047, eval/two: 0.0804, eval/three: 0.5666 \n",
      "Epoch: 165, train/loss: 2.1762, eval/loss: 3.2223, train/r2: 0.3398, eval/r2: 0.3819, train/one: 0.6833, train/two: -0.3179, train/three: 0.6541, eval/one: 0.4266, eval/two: 0.0799, eval/three: 0.6392 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "mus, sigmas = [], []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "splits = kfold.split(range(96))\n",
    "\n",
    "for fold, (train_idx, eval_idx) in enumerate(splits):\n",
    "    MODEL_NAME = f\"resnet.pretrain.fold.{fold}\"\n",
    "    checkpoint_name = f\"pretrain.fold.{fold}.pt\"\n",
    "    \n",
    "    train_inputs = torch.cat([inputs_list[i][train_idx] for i in range(len(inputs_list))])\n",
    "    train_targets = torch.cat([targets_list[i][train_idx] for i in range(len(inputs_list))])\n",
    "    eval_inputs = torch.cat([inputs_list[i][eval_idx] for i in range(len(inputs_list))])\n",
    "    eval_targets = torch.cat([targets_list[i][eval_idx] for i in range(len(inputs_list))])\n",
    "\n",
    "    mu, sigma = get_stats(train_inputs, p=False, r=True)\n",
    "    train_inputs = zscore(train_inputs, mu, sigma)\n",
    "    eval_inputs = zscore(eval_inputs, mu, sigma)\n",
    "    mus.append(mu)\n",
    "    sigmas.append(sigma)\n",
    "    \n",
    "    train_ds = TensorDataset(train_inputs.float(), train_targets.float())\n",
    "    eval_ds = TensorDataset(eval_inputs.float(), eval_targets.float())\n",
    "    \n",
    "    BATCH_SIZE = len(train_ds)\n",
    "    train_dl, eval_dl = return_dls(train_ds, eval_ds, BATCH_SIZE, len(eval_ds))\n",
    "    \n",
    "    model = ResNet(input_channels=2, dropout=DROPOUT).to(device)\n",
    "    if fold == 0: print(get_model_size(model))\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    scheduler = get_scheduler(optimizer, train_dl, EPOCHS)\n",
    "    \n",
    "    train(\n",
    "        model, \n",
    "        optimizer, \n",
    "        device,\n",
    "        scaler,\n",
    "        scheduler,\n",
    "        train_dl, \n",
    "        eval_dl,\n",
    "        EPOCHS,\n",
    "        checkpoint_name,\n",
    "        neptune_run=None#setup_neptune(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = f\"/kaggle/working/{checkpoint_name}\"\n",
    "ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "ckpt[\"epoch\"], ckpt[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(443, 0.4866819899481086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b99917",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_test_data()\n",
    "get_stats(test)\n",
    "test = get_spectra_features(test)\n",
    "test = torch.tensor(test)\n",
    "test = zscore(test, mu, sigma).float()\n",
    "test.shape, test.dtype, get_stats(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(input_channels=2).to(device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = model(test.cuda())\n",
    "\n",
    "preds = preds.cpu().detach().double().numpy()\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3273410",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']\n",
    "preds_df = pd.DataFrame(preds, columns=column_names)\n",
    "preds_df.insert(0, 'ID', [i+1 for i in range(len(preds_df))])\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = MODEL_NAME+\".finetune.transfer.in.pretrain.csv\"\n",
    "preds_df.to_csv(name, index=False)\n",
    "f = pd.read_csv(f\"/kaggle/working/{name}\")\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35f1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
