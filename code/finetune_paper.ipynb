{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ca4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "SEED = 1000\n",
    "setup_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb8c02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def generate_csv(preds, name):\n",
    "    column_names = ['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']\n",
    "    preds_df = pd.DataFrame(preds, columns=column_names)\n",
    "    preds_df.insert(0, 'ID', [i+1 for i in range(len(preds_df))])\n",
    "    preds_df.to_csv(name, index=False)\n",
    "    \n",
    "    \n",
    "def get_ckpt(path):\n",
    "    return torch.load(path, weights_only=False)\n",
    "\n",
    "\n",
    "def cuda_to_np(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, train_dl, epochs):\n",
    "    total_training_steps = len(train_dl) * epochs\n",
    "    warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "\n",
    "\n",
    "def get_stats(tensor, p=True, r=False, minmax=False):\n",
    "    if minmax:\n",
    "        min, max = tensor.min(), tensor.max()\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Min: {min}, Max: {max} ,Mean: {mean}, Std: {std}\")\n",
    "        if r: return min, max, mean, std\n",
    "    else:\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        if p: print(f\"Mean: {mean}, Std: {std}\")\n",
    "        if r: return mean, std\n",
    "    \n",
    "    \n",
    "def zscore(tensor, mean=None, std=None):\n",
    "    if mean is None: mean = tensor.mean()\n",
    "    if std is None: std = tensor.std()\n",
    "    return (tensor - mean) / (std + 1e-8)\n",
    "\n",
    "\n",
    "def reverse_zscore(tensor, mu, sigma):\n",
    "    return (tensor * sigma) + mu\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "    \n",
    "\n",
    "def get_index(iterable):\n",
    "    return random.randint(0, len(iterable) - 1)\n",
    "\n",
    "\n",
    "def get_indices(iterable, n):\n",
    "    return random.sample(range(len(iterable)), n)\n",
    "\n",
    "\n",
    "def split(inputs, targets, seed):\n",
    "    return train_test_split(\n",
    "        inputs,\n",
    "        targets, \n",
    "        test_size=0.2,\n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    ) \n",
    "\n",
    "\n",
    "def show_waves(waves, dpi=100):\n",
    "    \"\"\"\n",
    "    waves: numpy array of shape (3, N)\n",
    "    Creates three separate figures that stretch wide.\n",
    "    \"\"\"\n",
    "    N = waves.shape[1]\n",
    "    t = np.arange(N)\n",
    "\n",
    "    # Wide aspect ratio; height modest so each window fills width\n",
    "    for i in range(waves.shape[0]):\n",
    "        fig = plt.figure(figsize=(14, 4), dpi=dpi)  # wide figure\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(t, waves[i], linewidth=1)\n",
    "        ax.set_title(f\"Wave {i+1}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()  # reduce margins to use width\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def hf_ds_download(hf_token, repo_id):\n",
    "    login(hf_token[1:])\n",
    "    return snapshot_download(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "def get_spectra_features(X, b=False):\n",
    "    \"\"\"Create multi-channel features from spectra: raw, 1st derivative, 2nd derivative.\"\"\"\n",
    "    X_processed = np.zeros_like(X)\n",
    "    # Baseline correction and SNV\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        poly = np.polyfit(np.arange(X.shape[1]), X[i], 3)\n",
    "        baseline = np.polyval(poly, np.arange(X.shape[1]))\n",
    "        corrected_spec = X[i] - baseline\n",
    "        #X_processed[i] = (corrected_spec - corrected_spec.mean()) / (corrected_spec.std() + 1e-8)\n",
    "        X_processed[i] = corrected_spec\n",
    "        \n",
    "    # Calculate derivatives\n",
    "    deriv1 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=1, axis=1)\n",
    "    deriv2 = signal.savgol_filter(X_processed, window_length=11, polyorder=3, deriv=2, axis=1)\n",
    "\n",
    "    if b: return np.stack([X_processed, deriv1, deriv2], axis=1)\n",
    "    return np.stack([deriv1, deriv2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfbbe45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'sample_submission.csv'),\n",
       " (1, 'timegate.csv'),\n",
       " (2, 'mettler_toledo.csv'),\n",
       " (3, 'kaiser.csv'),\n",
       " (4, 'anton_532.csv'),\n",
       " (5, 'transfer_plate.csv'),\n",
       " (6, '96_samples.csv'),\n",
       " (7, 'tornado.csv'),\n",
       " (8, 'tec5.csv'),\n",
       " (9, 'metrohm.csv'),\n",
       " (10, 'anton_785.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/input/dig-4-bio-raman-transfer-learning-challenge\"\n",
    "files = os.listdir(path)\n",
    "[(i, files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62074de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    test = pd.read_csv(os.path.join(path, files[6]))\n",
    "\n",
    "    row1 = test.columns[1:].to_numpy().copy()\n",
    "    row1[-1] = \"5611\"\n",
    "    row1 = row1.astype(np.float64)\n",
    "\n",
    "\n",
    "    cols = test.columns[1:]\n",
    "    test = test[cols]\n",
    "    test[\" 5611]\"] = test[\" 5611]\"].str.replace('[\\[\\]]', '', regex=True).astype('int64')\n",
    "    test = test.to_numpy()\n",
    "\n",
    "    test = np.insert(test, 0, row1, axis=0)\n",
    "    return test.reshape(-1, 2, 2048).mean(axis=1)\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "    inputs = load_test_data()\n",
    "    \n",
    "    spectra_selection = np.logical_and(\n",
    "        300 <= np.array([float(one) for one in range(2048)]),\n",
    "        np.array([float(one) for one in range(2048)]) <= 1942,\n",
    "    )\n",
    "    \n",
    "    inputs = inputs[:, spectra_selection]\n",
    "\n",
    "    wns = np.array([\n",
    "        float(one) for one in range(2048)\n",
    "    ])[spectra_selection]\n",
    "    wavenumbers = np.arange(300, 1943)\n",
    "\n",
    "    interpolated_data = np.array(\n",
    "        [np.interp(wavenumbers, xp=wns, fp=i) for i in inputs]\n",
    "    )\n",
    "\n",
    "    normed_spectra = interpolated_data / np.max(interpolated_data)\n",
    "    return normed_spectra\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1f5e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 1643), (96, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_transfer_data():\n",
    "    csv_path = os.path.join(path, files[5])\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    input_cols = df.columns[1:2049]\n",
    "    target_cols = df.columns[2050:]\n",
    "\n",
    "    targets  = df[target_cols].dropna().to_numpy()\n",
    "\n",
    "    df = df[input_cols]\n",
    "    df['Unnamed: 1'] = df['Unnamed: 1'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "    df['Unnamed: 2048'] = df['Unnamed: 2048'].str.replace(\"[\\[\\]]\", \"\", regex=True).astype('int64')\n",
    "\n",
    "    inputs = df.to_numpy().reshape(-1, 2, 2048)\n",
    "    inputs = inputs.mean(axis=1)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def preprocess_transfer_data():\n",
    "    inputs, targets = load_transfer_data()\n",
    "    \n",
    "    spectra_selection = np.logical_and(\n",
    "        300 <= np.array([float(one) for one in range(2048)]),\n",
    "        np.array([float(one) for one in range(2048)]) <= 1942,\n",
    "    )\n",
    "    \n",
    "    inputs = inputs[:, spectra_selection]\n",
    "    \n",
    "    wns = np.array([\n",
    "        float(one) for one in range(2048)\n",
    "    ])[spectra_selection]\n",
    "    wavenumbers = np.arange(300, 1943)\n",
    "    \n",
    "    interpolated_data = np.array(\n",
    "        [np.interp(wavenumbers, xp=wns, fp=i) for i in inputs]\n",
    "    )\n",
    "    \n",
    "    normed_spectra = interpolated_data / np.max(interpolated_data)\n",
    "    return normed_spectra, targets\n",
    "\n",
    "inputs, targets = preprocess_transfer_data()\n",
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba2c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "np_dtype_from_torch = {\n",
    "    torch.float32: np.float32,\n",
    "    torch.float64: np.float64,\n",
    "}\n",
    "\n",
    "class SpectralDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra,\n",
    "        concentrations,\n",
    "        dtype=None,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=0.0,\n",
    "        baseline_factor_bound=0.0,\n",
    "        baseline_period_lower_bound=100.0,\n",
    "        baseline_period_upper_bound=200.0,\n",
    "        augment_slope_std=0.0,\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=0,\n",
    "        spectrum_rolling_sigma=0.0,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    ):\n",
    "        self.dtype = dtype or torch.float32\n",
    "        self.combine_spectra_range = combine_spectra_range\n",
    "        self.baseline_factor_bound = baseline_factor_bound\n",
    "        self.augment_slope_std = augment_slope_std\n",
    "        self.augment_intercept_std = augment_intersept_std\n",
    "        self.baseline_period_lower_bound = baseline_period_lower_bound\n",
    "        self.baseline_period_upper_bound = baseline_period_upper_bound\n",
    "        self.rolling_bound = rolling_bound\n",
    "        self.spectrum_rolling_sigma = spectrum_rolling_sigma\n",
    "        self.augmentation_weight = torch.tensor(augmentation_weight, dtype=dtype)\n",
    "        self.original_dp_weight = original_datapoint_weight\n",
    "\n",
    "        # normalize spectra\n",
    "        spectra = torch.tensor(spectra, dtype=dtype)\n",
    "\n",
    "        if spectra_mean_std is None:\n",
    "            self.s_mean = torch.mean(spectra)\n",
    "            self.s_std = torch.std(spectra)\n",
    "        else:\n",
    "            self.s_mean, self.s_std = spectra_mean_std\n",
    "\n",
    "        self.spectra = torch.divide(\n",
    "            torch.subtract(spectra, self.s_mean),\n",
    "            self.s_std,\n",
    "        )\n",
    "\n",
    "        self.dummy_wns = np.tile(\n",
    "            np.arange(\n",
    "                0., 1., 1. / self.spectra.shape[2],\n",
    "                dtype=np_dtype_from_torch[self.dtype]\n",
    "            )[None, :self.spectra.shape[2]],\n",
    "            (self.spectra.shape[1], 1),\n",
    "        )\n",
    "\n",
    "        # normalize concentrations\n",
    "        concentrations = torch.tensor(concentrations, dtype=dtype)\n",
    "        if concentration_mean_std is None:\n",
    "            self.concentration_means = torch.nanmean(concentrations, dim=0)\n",
    "\n",
    "            self.concentration_stds = torch.maximum(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        torch.std(col[torch.logical_not(torch.isnan(col))])\n",
    "                        for col in concentrations.T\n",
    "                    ]\n",
    "                ),\n",
    "                torch.tensor([1e-3] * concentrations.shape[1]),\n",
    "            )\n",
    "        else:\n",
    "            self.concentration_means = concentration_mean_std[0]\n",
    "            self.concentration_stds = concentration_mean_std[1]\n",
    "\n",
    "        self.concentrations = torch.divide(\n",
    "            torch.subtract(\n",
    "                concentrations,\n",
    "                self.concentration_means,\n",
    "            ),\n",
    "            self.concentration_stds,\n",
    "        )\n",
    "\n",
    "    def pick_two(self, max_idx=None):\n",
    "        max_idx = max_idx or len(self)\n",
    "        return random.choices(range(max_idx), k=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.concentrations)\n",
    "\n",
    "    def augment_spectra(self, spectra):\n",
    "        if self.augment_slope_std > 0.0:\n",
    "\n",
    "            def spectrum_approximation(x, slope, intercept):\n",
    "                return (slope * x + intercept).reshape(-1, 1)[:, 0]\n",
    "\n",
    "            slope, inter = scipy.optimize.curve_fit(\n",
    "                spectrum_approximation,\n",
    "                self.dummy_wns,\n",
    "                spectra.reshape(-1, 1)[:, 0],\n",
    "                p0=np.random.rand(2),\n",
    "            )[0]\n",
    "\n",
    "            new_slope = slope * (\n",
    "                    np.random.gamma(\n",
    "                        shape=1. / self.augment_slope_std,\n",
    "                        scale=self.augment_slope_std,\n",
    "                        size=1,\n",
    "                    )\n",
    "            )[0]\n",
    "            new_intercept = inter * (\n",
    "                1.0 + np.random.randn(1) * self.augment_intercept_std\n",
    "            )[0]\n",
    "            spectra += torch.tensor(\n",
    "                (new_slope - slope)\n",
    "            ) * self.dummy_wns + new_intercept - inter\n",
    "\n",
    "        factor = self.baseline_factor_bound * torch.rand(size=(1,))\n",
    "        offset = torch.rand(size=(1,)) * 2.0 * torch.pi\n",
    "        period = self.baseline_period_lower_bound + (\n",
    "            self.baseline_period_upper_bound - self.baseline_period_lower_bound\n",
    "        ) * torch.rand(size=(1,))\n",
    "        permutations = factor * torch.cos(\n",
    "            2.0 * torch.pi / period * self.dummy_wns + offset\n",
    "        )\n",
    "        return self.roll_spectrum(\n",
    "            spectra + permutations * spectra,\n",
    "            delta=random.randint(-self.rolling_bound, self.rolling_bound),\n",
    "        )\n",
    "\n",
    "    def roll_spectrum(self, spectra, delta):\n",
    "        num_spectra = spectra.shape[0]\n",
    "        rolled_spectra = np.roll(spectra, delta, axis=1)\n",
    "        if delta > 0:\n",
    "            rolled_spectra[:, :delta] = (\n",
    "                np.random.rand(num_spectra, delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta:(delta + 1)]\n",
    "        elif delta < 0:\n",
    "            rolled_spectra[:, delta:] = (\n",
    "                np.random.rand(num_spectra, -delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta - 1:delta]\n",
    "        return rolled_spectra\n",
    "\n",
    "    def combine_k_items(self, indices, weights):\n",
    "        return (\n",
    "            # spectra\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None, None], self.spectra[indices, :, :]),\n",
    "                dim=0,\n",
    "            ),\n",
    "            # concentrations\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None], self.concentrations[indices, :]),\n",
    "                dim=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.combine_spectra_range < 1e-12:\n",
    "            spectrum = self.spectra[idx]\n",
    "            spectrum = self.augment_spectra(spectrum)\n",
    "            return {\n",
    "                \"spectra\": spectrum,\n",
    "                \"concentrations\": self.concentrations[idx],\n",
    "                \"label_weight\": torch.tensor(1.0, dtype=self.dtype),\n",
    "            }\n",
    "        else:\n",
    "            if random.random() < self.original_dp_weight:\n",
    "                one_weight = 1.\n",
    "                label_weight = torch.tensor(1.0, dtype=self.dtype)\n",
    "            else:\n",
    "                one_weight = random.uniform(0.0, self.combine_spectra_range)\n",
    "                label_weight = self.augmentation_weight\n",
    "            weights = torch.tensor([one_weight, (1 - one_weight)])\n",
    "            # just pick two random indices\n",
    "            indices = random.choices(range(len(self)), k=2)\n",
    "\n",
    "            mixed_spectra, mixed_concentrations = self.combine_k_items(\n",
    "                indices=indices,\n",
    "                weights=weights,\n",
    "            )\n",
    "            mixed_spectra = self.augment_spectra(mixed_spectra)\n",
    "            return mixed_spectra, mixed_concentrations, label_weight\n",
    "\n",
    "\n",
    "config = {\n",
    "    'initial_cnn_channels': 32,\n",
    "    'cnn_channel_factor': 1.279574024454846,\n",
    "    'num_cnn_layers': 8,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 2,\n",
    "    'activation_function': 'ELU',\n",
    "    'fc_dropout': 0.10361700399831791,\n",
    "    'lr': 0.001,\n",
    "    'gamma': 0.9649606352621118,\n",
    "    'baseline_factor_bound': 0.748262317340447,\n",
    "    'baseline_period_lower_bound': 0.9703081695287203,\n",
    "    'baseline_period_span': 19.79744237606427,\n",
    "    'original_datapoint_weight': 0.4335003268130408,\n",
    "    'augment_slope_std': 0.08171025264382692,\n",
    "    'batch_size': 32,\n",
    "    'fc_dims': 226,\n",
    "    'rolling_bound': 2,\n",
    "    'num_blocks': 2,\n",
    "}\n",
    "\n",
    "def get_dataset(inputs, targets, config, inputs_mean_std=None, targets_mean_std=None):\n",
    "    return SpectralDataset(\n",
    "        spectra=inputs[:, None, :],\n",
    "        concentrations=targets,\n",
    "        dtype=torch.float32,\n",
    "        spectra_mean_std=inputs_mean_std,\n",
    "        concentration_mean_std=targets_mean_std,\n",
    "        combine_spectra_range=1.0,\n",
    "        baseline_factor_bound=config[\"baseline_factor_bound\"],\n",
    "        baseline_period_lower_bound=config[\"baseline_period_lower_bound\"],\n",
    "        baseline_period_upper_bound=(config[\"baseline_period_lower_bound\"] + config[\"baseline_period_span\"]),\n",
    "        augment_slope_std=config[\"augment_slope_std\"],\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=config[\"rolling_bound\"],\n",
    "        spectrum_rolling_sigma=0.01,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c11034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+5232)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds, train_batch_size, eval_batch_size):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e343398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/kaggle-spect\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"nadam\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "        if DROPOUT: neptune_run[\"h_parameters\"] = {\"dropout\": DROPOUT}\n",
    "        if DROP_PATH_RATE: neptune_run[\"h_parameters\"] = {\"drop_path_rate\": DROP_PATH_RATE}\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4227b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.mse_loss(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    \n",
    "    dim1 = r2_score(targets[:, 0], preds[:, 0])\n",
    "    dim2 = r2_score(targets[:, 1], preds[:, 1])\n",
    "    dim3 = r2_score(targets[:, 2], preds[:, 2])\n",
    "    \n",
    "    return dim1, dim2, dim3, r2_score(targets, logits)\n",
    "\n",
    "\n",
    "class MSEIgnoreNans(_Loss):\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        weights: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        mask = torch.isfinite(target)\n",
    "        mse = torch.mean(\n",
    "            torch.mul(\n",
    "                torch.square(input[mask] - target[mask]),\n",
    "                torch.tile(weights[:, None], dims=(1, target.shape[1]))[mask],\n",
    "            )\n",
    "        )\n",
    "        return torch.where(\n",
    "            torch.isfinite(mse),\n",
    "            mse,\n",
    "            torch.tensor(0.).to(target.device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ccc0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Identity(torch.torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "# this is not a resnet yet\n",
    "class ReZeroBlock(torch.torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        activation_function,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dtype,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(ReZeroBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = torch.torch.nn.BatchNorm1d\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = divmod(kernel_size, 2)[0] if stride == 1 else 0\n",
    "\n",
    "        # does not change spatial dimension\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.bn1 = norm_layer(out_channels, dtype=dtype)\n",
    "        # Both self.conv2 and self.downsample layers\n",
    "        # downsample the input when stride != 1\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            groups=out_channels,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        if stride > 1:\n",
    "            down_conv = torch.nn.Conv1d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "                dtype=dtype,\n",
    "                # groups=out_channels,\n",
    "            )\n",
    "        else:\n",
    "            down_conv = Identity()\n",
    "\n",
    "        self.down_sample = torch.nn.Sequential(\n",
    "            down_conv,\n",
    "            norm_layer(out_channels),\n",
    "        )\n",
    "        self.bn2 = norm_layer(out_channels, dtype=dtype)\n",
    "        # does not change the spatial dimension\n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.bn3 = norm_layer(out_channels, dtype=dtype)\n",
    "        self.activation = activation_function(inplace=True)\n",
    "        self.factor = torch.torch.nn.parameter.Parameter(torch.tensor(0.0, dtype=dtype))\n",
    "\n",
    "    def next_spatial_dim(self, last_spatial_dim):\n",
    "        return math.floor(\n",
    "            (last_spatial_dim + 2 * self.padding - self.kernel_size)\n",
    "            / self.stride + 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        # not really the identity, but kind of\n",
    "        identity = self.down_sample(x)\n",
    "\n",
    "        return self.activation(out * self.factor + identity)\n",
    "\n",
    "\n",
    "class ResNetEncoder(torch.torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectrum_size,\n",
    "        cnn_encoder_channel_dims,\n",
    "        activation_function,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dtype,\n",
    "        num_blocks,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "\n",
    "        self.spatial_dims = [spectrum_size]\n",
    "        layers = []\n",
    "        for in_channels, out_channels in zip(\n",
    "            cnn_encoder_channel_dims[:-1],\n",
    "            cnn_encoder_channel_dims[1:],\n",
    "        ):\n",
    "            block = ReZeroBlock(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                activation_function=activation_function,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            layers.append(block)\n",
    "            self.spatial_dims.append(block.next_spatial_dim(self.spatial_dims[-1]))\n",
    "            for _ in range(num_blocks - 1):\n",
    "                block = ReZeroBlock(\n",
    "                    in_channels=out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    activation_function=activation_function,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    dtype=dtype,\n",
    "                )\n",
    "                layers.append(block)\n",
    "                self.spatial_dims.append(block.next_spatial_dim(self.spatial_dims[-1]))\n",
    "\n",
    "        self.resnet_layers = torch.torch.nn.Sequential(*layers)\n",
    "        if verbose:\n",
    "            print(\"CNN Encoder Channel Dims: %s\" % (cnn_encoder_channel_dims))\n",
    "            print(\"CNN Encoder Spatial Dims: %s\" % (self.spatial_dims))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet_layers(x)\n",
    "\n",
    "\n",
    "class ReZeroNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra_channels,\n",
    "        spectra_size,\n",
    "        initial_cnn_channels,\n",
    "        cnn_channel_factor,\n",
    "        num_cnn_layers,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        activation_function,\n",
    "        fc_dims,\n",
    "        fc_dropout=0.0,\n",
    "        dtype=None,\n",
    "        verbose=False,\n",
    "        fc_output_channels=1,\n",
    "        num_blocks=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc_output_channels = fc_output_channels\n",
    "        self.dtype = dtype or torch.float32\n",
    "\n",
    "        activation_function = getattr(torch.nn, activation_function)\n",
    "\n",
    "        # Setup CNN Encoder\n",
    "        cnn_encoder_channel_dims = [spectra_channels] + [\n",
    "            int(initial_cnn_channels * (cnn_channel_factor**idx))\n",
    "            for idx in range(num_cnn_layers)\n",
    "        ]\n",
    "        self.cnn_encoder = ResNetEncoder(\n",
    "            spectrum_size=spectra_size,\n",
    "            cnn_encoder_channel_dims=cnn_encoder_channel_dims,\n",
    "            activation_function=activation_function,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            num_blocks=num_blocks,\n",
    "            dtype=dtype,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.fc_dims = [\n",
    "            int(\n",
    "                self.cnn_encoder.spatial_dims[-1]\n",
    "            ) * int(cnn_encoder_channel_dims[-1])\n",
    "        ] + fc_dims\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Fc Dims: %s\" % self.fc_dims)\n",
    "        fc_layers = []\n",
    "        for idx, (in_dim, out_dim) in enumerate(\n",
    "                zip(self.fc_dims[:-2], self.fc_dims[1:-1])\n",
    "        ):\n",
    "            fc_layers.append(torch.nn.Linear(in_dim, out_dim))\n",
    "            fc_layers.append(torch.nn.ELU())\n",
    "            fc_layers.append(torch.nn.Dropout(fc_dropout / (2 ** idx)))\n",
    "        fc_layers.append(\n",
    "            torch.nn.Linear(\n",
    "                self.fc_dims[-2],\n",
    "                self.fc_dims[-1] * self.fc_output_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.fc_net = torch.nn.Sequential(*fc_layers)\n",
    "        if verbose:\n",
    "            num_params = sum(p.numel() for p in self.parameters())\n",
    "            print(\"Number of Parameters: %s\" % num_params)\n",
    "\n",
    "    def forward(self, spectra):\n",
    "        embeddings = self.cnn_encoder(spectra)\n",
    "        forecast = self.fc_net(embeddings.view(-1, self.fc_dims[0]))\n",
    "        if self.fc_output_channels > 1:\n",
    "            forecast = forecast.reshape(\n",
    "                -1, self.fc_output_channels, self.fc_dims[-1]\n",
    "            )\n",
    "        return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035dd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "mse_loss_function = MSEIgnoreNans()\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    device,\n",
    "    scaler, \n",
    "    scheduler,\n",
    "    train_dl,\n",
    "    eval_dl,\n",
    "    loss_fn,\n",
    "    epochs,\n",
    "    checkpoint_name,\n",
    "    score=-float(\"inf\"),\n",
    "    neptune_run=None,\n",
    "    p=True,\n",
    "):  \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for inputs, targets, weights in train_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets, weights)\n",
    "                            \n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            if neptune_run is not None:  neptune_run[\"lr_step\"].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            total_loss += loss.detach().cpu()\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        all_logits = torch.cat(all_logits)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        one, two, three, r2 = metric_fn(all_logits, all_targets)\n",
    "        total_loss = total_loss / len(train_dl)\n",
    "        \n",
    "        model.eval()\n",
    "        eval_total_loss = 0.0\n",
    "        eval_all_logits = []\n",
    "        eval_all_targets = []\n",
    "\n",
    "        for inputs, targets, weights in eval_dl:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                #with torch.amp.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets, weights)\n",
    "\n",
    "            eval_total_loss += loss.detach().cpu()\n",
    "            eval_all_logits.append(logits.detach().cpu())\n",
    "            eval_all_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        eval_all_logits = torch.cat(eval_all_logits)\n",
    "        eval_all_targets = torch.cat(eval_all_targets)\n",
    "\n",
    "        eval_one, eval_two, eval_three, eval_r2 = metric_fn(eval_all_logits, eval_all_targets)\n",
    "        eval_total_loss = eval_total_loss / len(eval_dl)\n",
    "        \n",
    "        if eval_r2 > score:\n",
    "            score = eval_r2\n",
    "            data = {\"state_dict\": model.state_dict()}\n",
    "            data[\"epoch\"] = epoch \n",
    "            data[\"score\"] = score\n",
    "            torch.save(data, f\"/kaggle/working/{checkpoint_name}\")\n",
    "        \n",
    "        if neptune_run is not None:\n",
    "            neptune_run[\"train/loss\"].append(total_loss)\n",
    "            neptune_run[\"eval/loss\"].append(eval_total_loss)\n",
    "            neptune_run[\"train/r2\"].append(r2)\n",
    "            neptune_run[\"eval/r2\"].append(eval_r2)\n",
    "            neptune_run[\"train/one\"].append(one)\n",
    "            neptune_run[\"train/two\"].append(two)\n",
    "            neptune_run[\"train/three\"].append(three)\n",
    "            neptune_run[\"eval/one\"].append(eval_one)\n",
    "            neptune_run[\"eval/two\"].append(eval_two)\n",
    "            neptune_run[\"eval/three\"].append(eval_three)\n",
    "            \n",
    "        if p and epoch % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, \"\n",
    "                f\"train/loss: {total_loss:.4f}, \"\n",
    "                f\"eval/loss: {eval_total_loss:.4f}, \"\n",
    "                f\"train/r2: {r2:.4f}, \"\n",
    "                f\"eval/r2: {eval_r2:.4f}, \"\n",
    "                f\"train/one: {one:.4f}, \"\n",
    "                f\"train/two: {two:.4f}, \"\n",
    "                f\"train/three: {three:.4f}, \"\n",
    "                f\"eval/one: {eval_one:.4f}, \"\n",
    "                f\"eval/two: {eval_two:.4f}, \"\n",
    "                f\"eval/three: {eval_three:.4f} \"\n",
    "            )\n",
    "            \n",
    "    if neptune_run is not None: neptune_run.stop()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd5d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings#; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "\n",
    "DROPOUT = 0.5\n",
    "DROP_PATH_RATE = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESUME = False\n",
    "\n",
    "config[\"dtype\"] = torch.float32\n",
    "config[\"spectra_size\"] = 1643\n",
    "config[\"spectra_channels\"] = 1\n",
    "config[\"fc_dims\"] = [\n",
    "    config[\"fc_dims\"],\n",
    "    int(config[\"fc_dims\"] / 2),\n",
    "    3,\n",
    "]\n",
    "\n",
    "#mse_loss_function = MSEIgnoreNans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54dd678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734309\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-152\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb01a5ce9af450e856162772e5e5511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.6501, eval/loss: 2.4915, train/r2: -0.6854, eval/r2: -2.1255, train/one: -0.6954, train/two: -0.8915, train/three: -0.4692, eval/one: -2.2867, eval/two: -2.4923, eval/three: -1.5975 \n",
      "Epoch: 5, train/loss: 0.9358, eval/loss: 1.1058, train/r2: -0.0709, eval/r2: -0.1981, train/one: 0.0721, train/two: -0.0826, train/three: -0.2023, eval/one: 0.0665, eval/two: -0.4455, eval/three: -0.2154 \n",
      "Epoch: 10, train/loss: 0.8517, eval/loss: 0.7014, train/r2: 0.0978, eval/r2: 0.2019, train/one: 0.2840, train/two: -0.0476, train/three: 0.0571, eval/one: 0.3024, eval/two: 0.1556, eval/three: 0.1476 \n",
      "Epoch: 15, train/loss: 0.7116, eval/loss: 0.7622, train/r2: 0.2439, eval/r2: 0.2116, train/one: 0.5951, train/two: 0.0612, train/three: 0.0753, eval/one: 0.4767, eval/two: -0.0101, eval/three: 0.1682 \n",
      "Epoch: 20, train/loss: 0.5359, eval/loss: 0.7852, train/r2: 0.4100, eval/r2: 0.0056, train/one: 0.6344, train/two: 0.3014, train/three: 0.2942, eval/one: 0.4900, eval/two: 0.1001, eval/three: -0.5733 \n",
      "Epoch: 25, train/loss: 0.4572, eval/loss: 0.7120, train/r2: 0.4971, eval/r2: 0.2825, train/one: 0.6661, train/two: 0.3106, train/three: 0.5145, eval/one: 0.5090, eval/two: -0.2331, eval/three: 0.5716 \n",
      "Epoch: 30, train/loss: 0.3952, eval/loss: 0.3329, train/r2: 0.5641, eval/r2: 0.4784, train/one: 0.6592, train/two: 0.3816, train/three: 0.6514, eval/one: 0.4111, eval/two: 0.4362, eval/three: 0.5880 \n",
      "Epoch: 35, train/loss: 0.3638, eval/loss: 0.3742, train/r2: 0.6419, eval/r2: 0.3653, train/one: 0.7739, train/two: 0.3362, train/three: 0.8157, eval/one: 0.4302, eval/two: -0.1757, eval/three: 0.8413 \n",
      "Epoch: 40, train/loss: 0.2592, eval/loss: 0.2400, train/r2: 0.7453, eval/r2: 0.6675, train/one: 0.8299, train/two: 0.5293, train/three: 0.8767, eval/one: 0.8618, eval/two: 0.2650, eval/three: 0.8757 \n",
      "Epoch: 45, train/loss: 0.2826, eval/loss: 0.4389, train/r2: 0.7578, eval/r2: 0.5277, train/one: 0.8261, train/two: 0.5886, train/three: 0.8588, eval/one: 0.6232, eval/two: 0.1965, eval/three: 0.7633 \n",
      "Epoch: 50, train/loss: 0.1957, eval/loss: 0.3158, train/r2: 0.8073, eval/r2: 0.5839, train/one: 0.8687, train/two: 0.6916, train/three: 0.8617, eval/one: 0.7010, eval/two: 0.1489, eval/three: 0.9019 \n",
      "Epoch: 55, train/loss: 0.1578, eval/loss: 0.3746, train/r2: 0.8524, eval/r2: 0.4234, train/one: 0.9056, train/two: 0.7907, train/three: 0.8609, eval/one: 0.8219, eval/two: 0.2744, eval/three: 0.1739 \n",
      "Epoch: 60, train/loss: 0.2139, eval/loss: 0.2377, train/r2: 0.7573, eval/r2: 0.7239, train/one: 0.8377, train/two: 0.5886, train/three: 0.8457, eval/one: 0.8050, eval/two: 0.5696, eval/three: 0.7970 \n",
      "Epoch: 65, train/loss: 0.1393, eval/loss: 0.2390, train/r2: 0.8624, eval/r2: 0.5721, train/one: 0.9133, train/two: 0.7971, train/three: 0.8769, eval/one: 0.7877, eval/two: 0.3301, eval/three: 0.5985 \n",
      "Epoch: 70, train/loss: 0.1860, eval/loss: 0.2256, train/r2: 0.8117, eval/r2: 0.6853, train/one: 0.8321, train/two: 0.6965, train/three: 0.9066, eval/one: 0.7262, eval/two: 0.4976, eval/three: 0.8321 \n",
      "Epoch: 75, train/loss: 0.1187, eval/loss: 0.3118, train/r2: 0.8840, eval/r2: 0.6408, train/one: 0.9013, train/two: 0.8222, train/three: 0.9284, eval/one: 0.5533, eval/two: 0.5072, eval/three: 0.8617 \n",
      "Epoch: 80, train/loss: 0.1508, eval/loss: 0.3055, train/r2: 0.8539, eval/r2: 0.7106, train/one: 0.8797, train/two: 0.7566, train/three: 0.9253, eval/one: 0.6944, eval/two: 0.5287, eval/three: 0.9086 \n",
      "Epoch: 85, train/loss: 0.1456, eval/loss: 0.2191, train/r2: 0.8424, eval/r2: 0.6593, train/one: 0.8657, train/two: 0.7385, train/three: 0.9230, eval/one: 0.8235, eval/two: 0.3792, eval/three: 0.7752 \n",
      "Epoch: 90, train/loss: 0.1427, eval/loss: 0.2937, train/r2: 0.8399, eval/r2: 0.6746, train/one: 0.8575, train/two: 0.7591, train/three: 0.9031, eval/one: 0.6568, eval/two: 0.4704, eval/three: 0.8966 \n",
      "Epoch: 95, train/loss: 0.1353, eval/loss: 0.2149, train/r2: 0.8916, eval/r2: 0.7138, train/one: 0.9182, train/two: 0.8332, train/three: 0.9234, eval/one: 0.7456, eval/two: 0.5539, eval/three: 0.8420 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 179 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 179 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-152/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b20d37ad5ab4a3f85da07969d929cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 2.0101, eval/loss: 1.8450, train/r2: -0.9347, eval/r2: -1.2201, train/one: -0.9843, train/two: -1.1973, train/three: -0.6224, eval/one: -2.0356, eval/two: -0.3446, eval/three: -1.2801 \n",
      "Epoch: 5, train/loss: 1.1097, eval/loss: 0.8675, train/r2: -0.1045, eval/r2: -0.1129, train/one: -0.2234, train/two: -0.0246, train/three: -0.0656, eval/one: -0.0508, eval/two: -0.0752, eval/three: -0.2128 \n",
      "Epoch: 10, train/loss: 0.9187, eval/loss: 0.7793, train/r2: 0.1063, eval/r2: -0.0357, train/one: 0.2867, train/two: -0.1017, train/three: 0.1338, eval/one: 0.3357, eval/two: 0.0358, eval/three: -0.4785 \n",
      "Epoch: 15, train/loss: 0.7423, eval/loss: 0.5261, train/r2: 0.2979, eval/r2: 0.3559, train/one: 0.5152, train/two: 0.1327, train/three: 0.2458, eval/one: 0.5179, eval/two: 0.2604, eval/three: 0.2895 \n",
      "Epoch: 20, train/loss: 0.5965, eval/loss: 0.8103, train/r2: 0.3991, eval/r2: 0.1750, train/one: 0.5472, train/two: 0.1660, train/three: 0.4841, eval/one: 0.5494, eval/two: -0.0554, eval/three: 0.0311 \n",
      "Epoch: 25, train/loss: 0.5356, eval/loss: 0.6570, train/r2: 0.4296, eval/r2: 0.0935, train/one: 0.5984, train/two: 0.2126, train/three: 0.4777, eval/one: 0.3348, eval/two: -0.0236, eval/three: -0.0307 \n",
      "Epoch: 30, train/loss: 0.4837, eval/loss: 0.5102, train/r2: 0.5441, eval/r2: 0.3768, train/one: 0.7119, train/two: 0.2466, train/three: 0.6738, eval/one: 0.4822, eval/two: 0.0057, eval/three: 0.6425 \n",
      "Epoch: 35, train/loss: 0.3629, eval/loss: 0.4862, train/r2: 0.6406, eval/r2: 0.4551, train/one: 0.7375, train/two: 0.4136, train/three: 0.7707, eval/one: 0.6410, eval/two: 0.4331, eval/three: 0.2913 \n",
      "Epoch: 40, train/loss: 0.2686, eval/loss: 0.2820, train/r2: 0.7179, eval/r2: 0.6626, train/one: 0.8069, train/two: 0.5210, train/three: 0.8258, eval/one: 0.7920, eval/two: 0.5841, eval/three: 0.6118 \n",
      "Epoch: 45, train/loss: 0.2467, eval/loss: 0.3030, train/r2: 0.7445, eval/r2: 0.6553, train/one: 0.8142, train/two: 0.6041, train/three: 0.8153, eval/one: 0.7782, eval/two: 0.4531, eval/three: 0.7344 \n",
      "Epoch: 50, train/loss: 0.2392, eval/loss: 0.3596, train/r2: 0.7783, eval/r2: 0.6334, train/one: 0.7483, train/two: 0.7211, train/three: 0.8655, eval/one: 0.7214, eval/two: 0.4922, eval/three: 0.6866 \n",
      "Epoch: 55, train/loss: 0.1717, eval/loss: 0.2761, train/r2: 0.8286, eval/r2: 0.6093, train/one: 0.8540, train/two: 0.7037, train/three: 0.9280, eval/one: 0.8692, eval/two: 0.2761, eval/three: 0.6825 \n",
      "Epoch: 60, train/loss: 0.1954, eval/loss: 0.1530, train/r2: 0.8196, eval/r2: 0.8222, train/one: 0.8419, train/two: 0.7548, train/three: 0.8619, eval/one: 0.8762, eval/two: 0.7000, eval/three: 0.8903 \n",
      "Epoch: 65, train/loss: 0.1635, eval/loss: 0.2352, train/r2: 0.8347, eval/r2: 0.6458, train/one: 0.8426, train/two: 0.7881, train/three: 0.8733, eval/one: 0.5806, eval/two: 0.5419, eval/three: 0.8149 \n",
      "Epoch: 70, train/loss: 0.1652, eval/loss: 0.1749, train/r2: 0.8412, eval/r2: 0.8172, train/one: 0.8728, train/two: 0.7550, train/three: 0.8957, eval/one: 0.9267, eval/two: 0.6454, eval/three: 0.8793 \n",
      "Epoch: 75, train/loss: 0.1556, eval/loss: 0.2720, train/r2: 0.8632, eval/r2: 0.7097, train/one: 0.8989, train/two: 0.8198, train/three: 0.8709, eval/one: 0.7368, eval/two: 0.5671, eval/three: 0.8252 \n",
      "Epoch: 80, train/loss: 0.1619, eval/loss: 0.2099, train/r2: 0.8307, eval/r2: 0.7682, train/one: 0.8850, train/two: 0.7082, train/three: 0.8990, eval/one: 0.7089, eval/two: 0.6957, eval/three: 0.8999 \n",
      "Epoch: 85, train/loss: 0.1740, eval/loss: 0.1982, train/r2: 0.8103, eval/r2: 0.7501, train/one: 0.8586, train/two: 0.6755, train/three: 0.8968, eval/one: 0.8872, eval/two: 0.5352, eval/three: 0.8279 \n",
      "Epoch: 90, train/loss: 0.1686, eval/loss: 0.2135, train/r2: 0.8068, eval/r2: 0.7508, train/one: 0.7804, train/two: 0.7382, train/three: 0.9019, eval/one: 0.9063, eval/two: 0.5425, eval/three: 0.8037 \n",
      "Epoch: 95, train/loss: 0.1388, eval/loss: 0.1241, train/r2: 0.8457, eval/r2: 0.8289, train/one: 0.8959, train/two: 0.7106, train/three: 0.9307, eval/one: 0.8784, eval/two: 0.7849, eval/three: 0.8234 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 114 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 114 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-153/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-154\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c172bf1632f44829b24ad5a330a8b5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.6166, eval/loss: 2.7312, train/r2: -0.7100, eval/r2: -1.1672, train/one: -1.3112, train/two: -0.6603, train/three: -0.1584, eval/one: -1.7373, eval/two: -1.6121, eval/three: -0.1521 \n",
      "Epoch: 5, train/loss: 1.1911, eval/loss: 1.0018, train/r2: -0.2977, eval/r2: -0.3153, train/one: -0.3390, train/two: -0.3675, train/three: -0.1866, eval/one: -0.2426, eval/two: -0.3656, eval/three: -0.3379 \n",
      "Epoch: 10, train/loss: 0.8818, eval/loss: 1.1166, train/r2: 0.0339, eval/r2: -0.0181, train/one: 0.1655, train/two: -0.0277, train/three: -0.0362, eval/one: -0.0006, eval/two: -0.1388, eval/three: 0.0849 \n",
      "Epoch: 15, train/loss: 0.7674, eval/loss: 0.7094, train/r2: 0.1907, eval/r2: 0.1261, train/one: 0.3067, train/two: 0.0750, train/three: 0.1903, eval/one: 0.4780, eval/two: -0.2447, eval/three: 0.1451 \n",
      "Epoch: 20, train/loss: 0.6215, eval/loss: 0.6426, train/r2: 0.3968, eval/r2: 0.3026, train/one: 0.5644, train/two: 0.2072, train/three: 0.4189, eval/one: 0.6419, eval/two: -0.0308, eval/three: 0.2966 \n",
      "Epoch: 25, train/loss: 0.5222, eval/loss: 0.7378, train/r2: 0.4359, eval/r2: 0.3243, train/one: 0.5890, train/two: 0.2055, train/three: 0.5133, eval/one: 0.3131, eval/two: 0.1523, eval/three: 0.5074 \n",
      "Epoch: 30, train/loss: 0.4936, eval/loss: 0.6099, train/r2: 0.5247, eval/r2: 0.1852, train/one: 0.7479, train/two: 0.2333, train/three: 0.5930, eval/one: -0.0657, eval/two: -0.1083, eval/three: 0.7297 \n",
      "Epoch: 35, train/loss: 0.3430, eval/loss: 0.5198, train/r2: 0.6306, eval/r2: 0.3275, train/one: 0.7537, train/two: 0.3935, train/three: 0.7447, eval/one: 0.1928, eval/two: 0.2908, eval/three: 0.4989 \n",
      "Epoch: 40, train/loss: 0.3472, eval/loss: 0.3838, train/r2: 0.7052, eval/r2: 0.5632, train/one: 0.6878, train/two: 0.6660, train/three: 0.7619, eval/one: 0.7004, eval/two: 0.2025, eval/three: 0.7867 \n",
      "Epoch: 45, train/loss: 0.2268, eval/loss: 0.3559, train/r2: 0.7422, eval/r2: 0.4739, train/one: 0.7528, train/two: 0.6139, train/three: 0.8601, eval/one: 0.7992, eval/two: -0.1745, eval/three: 0.7971 \n",
      "Epoch: 50, train/loss: 0.1960, eval/loss: 0.3131, train/r2: 0.8264, eval/r2: 0.6414, train/one: 0.8132, train/two: 0.7569, train/three: 0.9090, eval/one: 0.8527, eval/two: 0.2636, eval/three: 0.8077 \n",
      "Epoch: 55, train/loss: 0.2099, eval/loss: 0.3639, train/r2: 0.7754, eval/r2: 0.5930, train/one: 0.8270, train/two: 0.6185, train/three: 0.8807, eval/one: 0.5714, eval/two: 0.4163, eval/three: 0.7914 \n",
      "Epoch: 60, train/loss: 0.1715, eval/loss: 0.4505, train/r2: 0.8133, eval/r2: 0.4276, train/one: 0.8152, train/two: 0.7138, train/three: 0.9110, eval/one: 0.1186, eval/two: 0.4476, eval/three: 0.7167 \n",
      "Epoch: 65, train/loss: 0.1374, eval/loss: 0.3340, train/r2: 0.8552, eval/r2: 0.6409, train/one: 0.8759, train/two: 0.8179, train/three: 0.8718, eval/one: 0.6305, eval/two: 0.4529, eval/three: 0.8393 \n",
      "Epoch: 70, train/loss: 0.1536, eval/loss: 0.2175, train/r2: 0.8478, eval/r2: 0.7288, train/one: 0.8310, train/two: 0.8092, train/three: 0.9032, eval/one: 0.7428, eval/two: 0.5776, eval/three: 0.8660 \n",
      "Epoch: 75, train/loss: 0.1431, eval/loss: 0.1945, train/r2: 0.8556, eval/r2: 0.7875, train/one: 0.9067, train/two: 0.7494, train/three: 0.9107, eval/one: 0.8270, eval/two: 0.6645, eval/three: 0.8711 \n",
      "Epoch: 80, train/loss: 0.1496, eval/loss: 0.2976, train/r2: 0.8611, eval/r2: 0.6544, train/one: 0.8579, train/two: 0.8203, train/three: 0.9052, eval/one: 0.7503, eval/two: 0.4109, eval/three: 0.8020 \n",
      "Epoch: 85, train/loss: 0.1495, eval/loss: 0.2406, train/r2: 0.8516, eval/r2: 0.7584, train/one: 0.8471, train/two: 0.8203, train/three: 0.8874, eval/one: 0.8054, eval/two: 0.6567, eval/three: 0.8131 \n",
      "Epoch: 90, train/loss: 0.1786, eval/loss: 0.3960, train/r2: 0.8479, eval/r2: 0.2992, train/one: 0.8762, train/two: 0.7805, train/three: 0.8870, eval/one: 0.6120, eval/two: -0.5481, eval/three: 0.8338 \n",
      "Epoch: 95, train/loss: 0.1611, eval/loss: 0.2457, train/r2: 0.8474, eval/r2: 0.6490, train/one: 0.8640, train/two: 0.7982, train/three: 0.8801, eval/one: 0.7662, eval/two: 0.2524, eval/three: 0.9285 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 116 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 116 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-154/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a461c95c00a478f9f22f58e7bdf2b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.7116, eval/loss: 2.4823, train/r2: -0.7114, eval/r2: -0.8075, train/one: -1.1410, train/two: -0.4731, train/three: -0.5203, eval/one: -1.3673, eval/two: -0.7357, eval/three: -0.3195 \n",
      "Epoch: 5, train/loss: 1.2253, eval/loss: 1.1354, train/r2: -0.4465, eval/r2: -0.3381, train/one: -0.3850, train/two: -0.6549, train/three: -0.2996, eval/one: -0.4696, eval/two: -0.0052, eval/three: -0.5396 \n",
      "Epoch: 10, train/loss: 1.0409, eval/loss: 1.4123, train/r2: 0.0157, eval/r2: -0.3966, train/one: 0.0870, train/two: -0.0340, train/three: -0.0058, eval/one: -0.9267, eval/two: -0.2191, eval/three: -0.0440 \n",
      "Epoch: 15, train/loss: 0.8691, eval/loss: 1.1447, train/r2: 0.1497, eval/r2: 0.1025, train/one: 0.3005, train/two: 0.0092, train/three: 0.1393, eval/one: 0.2375, eval/two: 0.1453, eval/three: -0.0751 \n",
      "Epoch: 20, train/loss: 0.6191, eval/loss: 0.6932, train/r2: 0.3121, eval/r2: 0.2927, train/one: 0.4217, train/two: 0.1747, train/three: 0.3400, eval/one: 0.5513, eval/two: -0.0534, eval/three: 0.3802 \n",
      "Epoch: 25, train/loss: 0.5283, eval/loss: 0.6188, train/r2: 0.4257, eval/r2: 0.3063, train/one: 0.6830, train/two: 0.2045, train/three: 0.3896, eval/one: 0.3913, eval/two: 0.4707, eval/three: 0.0569 \n",
      "Epoch: 30, train/loss: 0.4153, eval/loss: 0.7941, train/r2: 0.5716, eval/r2: 0.5183, train/one: 0.7673, train/two: 0.3324, train/three: 0.6149, eval/one: 0.5820, eval/two: 0.2612, eval/three: 0.7115 \n",
      "Epoch: 35, train/loss: 0.3273, eval/loss: 0.5175, train/r2: 0.6716, eval/r2: 0.6079, train/one: 0.8018, train/two: 0.4378, train/three: 0.7751, eval/one: 0.7820, eval/two: 0.3720, eval/three: 0.6698 \n",
      "Epoch: 40, train/loss: 0.3768, eval/loss: 0.6998, train/r2: 0.6603, eval/r2: 0.4372, train/one: 0.7794, train/two: 0.4606, train/three: 0.7408, eval/one: 0.6914, eval/two: 0.3087, eval/three: 0.3116 \n",
      "Epoch: 45, train/loss: 0.3072, eval/loss: 0.5962, train/r2: 0.7232, eval/r2: 0.4650, train/one: 0.7822, train/two: 0.5093, train/three: 0.8781, eval/one: 0.5554, eval/two: 0.0874, eval/three: 0.7522 \n",
      "Epoch: 50, train/loss: 0.2706, eval/loss: 0.5758, train/r2: 0.7343, eval/r2: 0.5162, train/one: 0.6993, train/two: 0.6149, train/three: 0.8888, eval/one: 0.6708, eval/two: 0.2599, eval/three: 0.6179 \n",
      "Epoch: 55, train/loss: 0.2324, eval/loss: 0.4097, train/r2: 0.7466, eval/r2: 0.6878, train/one: 0.7933, train/two: 0.5767, train/three: 0.8697, eval/one: 0.7494, eval/two: 0.6623, eval/three: 0.6516 \n",
      "Epoch: 60, train/loss: 0.2241, eval/loss: 0.4071, train/r2: 0.7822, eval/r2: 0.5617, train/one: 0.8565, train/two: 0.6147, train/three: 0.8755, eval/one: 0.5246, eval/two: 0.6246, eval/three: 0.5359 \n",
      "Epoch: 65, train/loss: 0.1900, eval/loss: 0.3107, train/r2: 0.8213, eval/r2: 0.7048, train/one: 0.8276, train/two: 0.7411, train/three: 0.8953, eval/one: 0.7524, eval/two: 0.7564, eval/three: 0.6056 \n",
      "Epoch: 70, train/loss: 0.1808, eval/loss: 0.3109, train/r2: 0.8205, eval/r2: 0.7686, train/one: 0.8397, train/two: 0.7311, train/three: 0.8905, eval/one: 0.8015, eval/two: 0.6681, eval/three: 0.8360 \n",
      "Epoch: 75, train/loss: 0.1700, eval/loss: 0.3673, train/r2: 0.8474, eval/r2: 0.6873, train/one: 0.8649, train/two: 0.7822, train/three: 0.8951, eval/one: 0.7049, eval/two: 0.5647, eval/three: 0.7922 \n",
      "Epoch: 80, train/loss: 0.1885, eval/loss: 0.4048, train/r2: 0.8104, eval/r2: 0.6969, train/one: 0.8449, train/two: 0.6983, train/three: 0.8882, eval/one: 0.8276, eval/two: 0.4217, eval/three: 0.8414 \n",
      "Epoch: 85, train/loss: 0.1712, eval/loss: 0.2822, train/r2: 0.8365, eval/r2: 0.6805, train/one: 0.9038, train/two: 0.6958, train/three: 0.9099, eval/one: 0.8227, eval/two: 0.6537, eval/three: 0.5651 \n",
      "Epoch: 90, train/loss: 0.1753, eval/loss: 0.3740, train/r2: 0.8436, eval/r2: 0.5811, train/one: 0.8826, train/two: 0.7672, train/three: 0.8810, eval/one: 0.7476, eval/two: 0.5390, eval/three: 0.4567 \n",
      "Epoch: 95, train/loss: 0.1802, eval/loss: 0.5003, train/r2: 0.8167, eval/r2: 0.6382, train/one: 0.8598, train/two: 0.6842, train/three: 0.9062, eval/one: 0.8257, eval/two: 0.4373, eval/three: 0.6514 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 103 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 103 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-155/metadata\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a520febd1d493eb48e7cbea9116665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 1.5197, eval/loss: 2.7203, train/r2: -0.5134, eval/r2: -1.8786, train/one: -0.5964, train/two: -0.6400, train/three: -0.3037, eval/one: -3.2710, eval/two: -1.4573, eval/three: -0.9077 \n",
      "Epoch: 5, train/loss: 1.1335, eval/loss: 1.3603, train/r2: -0.1080, eval/r2: -1.0471, train/one: -0.1086, train/two: -0.2678, train/three: 0.0524, eval/one: 0.0711, eval/two: -2.8742, eval/three: -0.3382 \n",
      "Epoch: 10, train/loss: 0.8474, eval/loss: 0.8829, train/r2: 0.1488, eval/r2: -0.0601, train/one: 0.3171, train/two: -0.0098, train/three: 0.1391, eval/one: 0.3687, eval/two: -0.4021, eval/three: -0.1467 \n",
      "Epoch: 15, train/loss: 0.7282, eval/loss: 0.6914, train/r2: 0.3304, eval/r2: 0.0237, train/one: 0.5480, train/two: 0.1453, train/three: 0.2978, eval/one: 0.1664, eval/two: -0.3632, eval/three: 0.2680 \n",
      "Epoch: 20, train/loss: 0.6023, eval/loss: 0.6669, train/r2: 0.3882, eval/r2: 0.2021, train/one: 0.5293, train/two: 0.1908, train/three: 0.4446, eval/one: 0.4305, eval/two: -0.1112, eval/three: 0.2869 \n",
      "Epoch: 25, train/loss: 0.5228, eval/loss: 0.7244, train/r2: 0.5070, eval/r2: 0.1386, train/one: 0.6101, train/two: 0.3739, train/three: 0.5370, eval/one: 0.3968, eval/two: -0.2838, eval/three: 0.3028 \n",
      "Epoch: 30, train/loss: 0.4035, eval/loss: 0.4361, train/r2: 0.5706, eval/r2: 0.3470, train/one: 0.7174, train/two: 0.3099, train/three: 0.6846, eval/one: 0.7134, eval/two: -0.2135, eval/three: 0.5412 \n",
      "Epoch: 35, train/loss: 0.2867, eval/loss: 0.3855, train/r2: 0.6531, eval/r2: 0.3727, train/one: 0.8111, train/two: 0.4282, train/three: 0.7198, eval/one: 0.7466, eval/two: -0.0818, eval/three: 0.4534 \n",
      "Epoch: 40, train/loss: 0.3220, eval/loss: 0.2538, train/r2: 0.6771, eval/r2: 0.6547, train/one: 0.7811, train/two: 0.5092, train/three: 0.7410, eval/one: 0.8134, eval/two: 0.4238, eval/three: 0.7269 \n",
      "Epoch: 45, train/loss: 0.2639, eval/loss: 0.3061, train/r2: 0.7520, eval/r2: 0.4890, train/one: 0.8091, train/two: 0.5901, train/three: 0.8567, eval/one: 0.6388, eval/two: 0.0389, eval/three: 0.7892 \n",
      "Epoch: 50, train/loss: 0.2261, eval/loss: 0.2540, train/r2: 0.7958, eval/r2: 0.6562, train/one: 0.8225, train/two: 0.6923, train/three: 0.8727, eval/one: 0.5899, eval/two: 0.5604, eval/three: 0.8182 \n",
      "Epoch: 55, train/loss: 0.1947, eval/loss: 0.2960, train/r2: 0.7974, eval/r2: 0.6664, train/one: 0.8411, train/two: 0.6292, train/three: 0.9219, eval/one: 0.7575, eval/two: 0.3599, eval/three: 0.8817 \n",
      "Epoch: 60, train/loss: 0.1874, eval/loss: 0.2694, train/r2: 0.8054, eval/r2: 0.5988, train/one: 0.8631, train/two: 0.6716, train/three: 0.8814, eval/one: 0.6239, eval/two: 0.3770, eval/three: 0.7955 \n",
      "Epoch: 65, train/loss: 0.1488, eval/loss: 0.2119, train/r2: 0.8531, eval/r2: 0.7539, train/one: 0.8898, train/two: 0.7732, train/three: 0.8964, eval/one: 0.8059, eval/two: 0.7056, eval/three: 0.7502 \n",
      "Epoch: 70, train/loss: 0.1495, eval/loss: 0.2270, train/r2: 0.8460, eval/r2: 0.7634, train/one: 0.8502, train/two: 0.7716, train/three: 0.9163, eval/one: 0.6950, eval/two: 0.7468, eval/three: 0.8483 \n",
      "Epoch: 75, train/loss: 0.1643, eval/loss: 0.2800, train/r2: 0.8359, eval/r2: 0.6833, train/one: 0.8833, train/two: 0.6904, train/three: 0.9341, eval/one: 0.7981, eval/two: 0.4626, eval/three: 0.7892 \n",
      "Epoch: 80, train/loss: 0.1651, eval/loss: 0.2537, train/r2: 0.8452, eval/r2: 0.6019, train/one: 0.8986, train/two: 0.7660, train/three: 0.8711, eval/one: 0.6976, eval/two: 0.2772, eval/three: 0.8308 \n",
      "Epoch: 85, train/loss: 0.1466, eval/loss: 0.2858, train/r2: 0.8408, eval/r2: 0.6474, train/one: 0.8696, train/two: 0.7883, train/three: 0.8644, eval/one: 0.5554, eval/two: 0.4877, eval/three: 0.8991 \n",
      "Epoch: 90, train/loss: 0.1297, eval/loss: 0.2735, train/r2: 0.8679, eval/r2: 0.4807, train/one: 0.9007, train/two: 0.8025, train/three: 0.9005, eval/one: 0.7613, eval/two: -0.1193, eval/three: 0.8001 \n",
      "Epoch: 95, train/loss: 0.1468, eval/loss: 0.1731, train/r2: 0.8417, eval/r2: 0.7273, train/one: 0.8835, train/two: 0.7476, train/three: 0.8942, eval/one: 0.8216, eval/two: 0.5475, eval/three: 0.8129 \n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 102 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 102 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/arbaaz/kaggle-spect/e/KAG-156/metadata\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "inputs_mean_std = []\n",
    "targets_mean_std = []\n",
    "scores = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "splits = kfold.split(inputs)\n",
    "\n",
    "for fold, (train_idx, eval_idx) in enumerate(splits):\n",
    "    MODEL_NAME = f\"resnet.paper.finetune1.fold.{fold}\"\n",
    "    checkpoint_name = f\"paper.direct.finetune1.fold.{fold}.pt\"\n",
    "    \n",
    "    train_inputs = inputs[train_idx]\n",
    "    train_targets = targets[train_idx]\n",
    "    eval_inputs = inputs[eval_idx]\n",
    "    eval_targets = targets[eval_idx]\n",
    "\n",
    "    train_ds = get_dataset(train_inputs, train_targets, config)\n",
    "    \n",
    "    inputs_mean_std.append((fold, train_ds.s_mean, train_ds.s_std))\n",
    "    targets_mean_std.append((fold, train_ds.concentration_means, train_ds.concentration_stds))\n",
    "    \n",
    "    eval_ds = get_dataset(eval_inputs, eval_targets, config, (train_ds.s_mean, train_ds.s_std), (train_ds.concentration_means, train_ds.concentration_stds))\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    train_dl, eval_dl = return_dls(train_ds, eval_ds, BATCH_SIZE, len(eval_ds))\n",
    "    \n",
    "    #model = ResNet(input_channels=1, dropout=DROPOUT).to(device)\n",
    "    model = ReZeroNet(**config).to(device)\n",
    "    if fold == 0: print(get_model_size(model))\n",
    "    \n",
    "    ckpt = get_ckpt(\"/kaggle/working/paper.pretrain.fold.3.pt\")\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, foreach=True)\n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    scheduler = get_scheduler(optimizer, train_dl, EPOCHS)\n",
    "    \n",
    "    score = train(\n",
    "            model, \n",
    "            optimizer, \n",
    "            device,\n",
    "            scaler,\n",
    "            scheduler,\n",
    "            train_dl, \n",
    "            eval_dl,\n",
    "            mse_loss_function,\n",
    "            EPOCHS,\n",
    "            checkpoint_name,\n",
    "            neptune_run=setup_neptune(),\n",
    "        )\n",
    "    \n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4925ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralTestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectra,\n",
    "        concentrations,\n",
    "        dtype=None,\n",
    "        spectra_mean_std=None,\n",
    "        concentration_mean_std=None,\n",
    "        combine_spectra_range=0.0,\n",
    "        baseline_factor_bound=0.0,\n",
    "        baseline_period_lower_bound=100.0,\n",
    "        baseline_period_upper_bound=200.0,\n",
    "        augment_slope_std=0.0,\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=0,\n",
    "        spectrum_rolling_sigma=0.0,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    ):\n",
    "        self.dtype = dtype or torch.float32\n",
    "        self.combine_spectra_range = combine_spectra_range\n",
    "        self.baseline_factor_bound = baseline_factor_bound\n",
    "        self.augment_slope_std = augment_slope_std\n",
    "        self.augment_intercept_std = augment_intersept_std\n",
    "        self.baseline_period_lower_bound = baseline_period_lower_bound\n",
    "        self.baseline_period_upper_bound = baseline_period_upper_bound\n",
    "        self.rolling_bound = rolling_bound\n",
    "        self.spectrum_rolling_sigma = spectrum_rolling_sigma\n",
    "        self.augmentation_weight = torch.tensor(augmentation_weight, dtype=dtype)\n",
    "        self.original_dp_weight = original_datapoint_weight\n",
    "\n",
    "        # normalize spectra\n",
    "        spectra = torch.tensor(spectra, dtype=dtype)\n",
    "\n",
    "        if spectra_mean_std is None:\n",
    "            self.s_mean = torch.mean(spectra)\n",
    "            self.s_std = torch.std(spectra)\n",
    "        else:\n",
    "            self.s_mean, self.s_std = spectra_mean_std\n",
    "\n",
    "        self.spectra = torch.divide(\n",
    "            torch.subtract(spectra, self.s_mean),\n",
    "            self.s_std,\n",
    "        )\n",
    "\n",
    "        self.dummy_wns = np.tile(\n",
    "            np.arange(\n",
    "                0., 1., 1. / self.spectra.shape[2],\n",
    "                dtype=np_dtype_from_torch[self.dtype]\n",
    "            )[None, :self.spectra.shape[2]],\n",
    "            (self.spectra.shape[1], 1),\n",
    "        )\n",
    "\n",
    "        if False:\n",
    "            # normalize concentrations\n",
    "            concentrations = torch.tensor(concentrations, dtype=dtype)\n",
    "            if concentration_mean_std is None:\n",
    "                self.concentration_means = torch.nanmean(concentrations, dim=0)\n",
    "\n",
    "                self.concentration_stds = torch.maximum(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            torch.std(col[torch.logical_not(torch.isnan(col))])\n",
    "                            for col in concentrations.T\n",
    "                        ]\n",
    "                    ),\n",
    "                    torch.tensor([1e-3] * concentrations.shape[1]),\n",
    "                )\n",
    "            else:\n",
    "                self.concentration_means = concentration_mean_std[0]\n",
    "                self.concentration_stds = concentration_mean_std[1]\n",
    "\n",
    "            self.concentrations = torch.divide(\n",
    "                torch.subtract(\n",
    "                    concentrations,\n",
    "                    self.concentration_means,\n",
    "                ),\n",
    "                self.concentration_stds,\n",
    "            )\n",
    "\n",
    "    def pick_two(self, max_idx=None):\n",
    "        max_idx = max_idx or len(self)\n",
    "        return random.choices(range(max_idx), k=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 96\n",
    "\n",
    "    def augment_spectra(self, spectra):\n",
    "        if self.augment_slope_std > 0.0:\n",
    "\n",
    "            def spectrum_approximation(x, slope, intercept):\n",
    "                return (slope * x + intercept).reshape(-1, 1)[:, 0]\n",
    "\n",
    "            slope, inter = scipy.optimize.curve_fit(\n",
    "                spectrum_approximation,\n",
    "                self.dummy_wns,\n",
    "                spectra.reshape(-1, 1)[:, 0],\n",
    "                p0=np.random.rand(2),\n",
    "            )[0]\n",
    "\n",
    "            new_slope = slope * (\n",
    "                    np.random.gamma(\n",
    "                        shape=1. / self.augment_slope_std,\n",
    "                        scale=self.augment_slope_std,\n",
    "                        size=1,\n",
    "                    )\n",
    "            )[0]\n",
    "            new_intercept = inter * (\n",
    "                1.0 + np.random.randn(1) * self.augment_intercept_std\n",
    "            )[0]\n",
    "            spectra += torch.tensor(\n",
    "                (new_slope - slope)\n",
    "            ) * self.dummy_wns + new_intercept - inter\n",
    "\n",
    "        factor = self.baseline_factor_bound * torch.rand(size=(1,))\n",
    "        offset = torch.rand(size=(1,)) * 2.0 * torch.pi\n",
    "        period = self.baseline_period_lower_bound + (\n",
    "            self.baseline_period_upper_bound - self.baseline_period_lower_bound\n",
    "        ) * torch.rand(size=(1,))\n",
    "        permutations = factor * torch.cos(\n",
    "            2.0 * torch.pi / period * self.dummy_wns + offset\n",
    "        )\n",
    "        return self.roll_spectrum(\n",
    "            spectra + permutations * spectra,\n",
    "            delta=random.randint(-self.rolling_bound, self.rolling_bound),\n",
    "        )\n",
    "\n",
    "    def roll_spectrum(self, spectra, delta):\n",
    "        num_spectra = spectra.shape[0]\n",
    "        rolled_spectra = np.roll(spectra, delta, axis=1)\n",
    "        if delta > 0:\n",
    "            rolled_spectra[:, :delta] = (\n",
    "                np.random.rand(num_spectra, delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta:(delta + 1)]\n",
    "        elif delta < 0:\n",
    "            rolled_spectra[:, delta:] = (\n",
    "                np.random.rand(num_spectra, -delta) * self.spectrum_rolling_sigma + 1\n",
    "            ) * rolled_spectra[:, delta - 1:delta]\n",
    "        return rolled_spectra\n",
    "\n",
    "    def combine_k_items(self, indices, weights):\n",
    "        return (\n",
    "            # spectra\n",
    "            torch.sum(\n",
    "                torch.mul(weights[:, None, None], self.spectra[indices, :, :]),\n",
    "                dim=0,\n",
    "            ),\n",
    "            # concentrations\n",
    "            #torch.sum(\n",
    "            #    torch.mul(weights[:, None], self.concentrations[indices, :]),\n",
    "            #    dim=0,\n",
    "            #)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if True:#self.combine_spectra_range < 1e-12:\n",
    "            spectrum = self.spectra[idx]\n",
    "            #spectrum = self.augment_spectra(spectrum)\n",
    "            return spectrum\n",
    "        else:\n",
    "            if random.random() < self.original_dp_weight:\n",
    "                one_weight = 1.\n",
    "                label_weight = torch.tensor(1.0, dtype=self.dtype)\n",
    "            else:\n",
    "                one_weight = random.uniform(0.0, self.combine_spectra_range)\n",
    "                label_weight = self.augmentation_weight\n",
    "            weights = torch.tensor([one_weight, (1 - one_weight)])\n",
    "            # just pick two random indices\n",
    "            indices = random.choices(range(len(self)), k=2)\n",
    "\n",
    "            mixed_spectra = self.combine_k_items(\n",
    "                indices=indices,\n",
    "                weights=weights,\n",
    "            )\n",
    "            mixed_spectra = self.augment_spectra(mixed_spectra[0])\n",
    "            return mixed_spectra\n",
    "        \n",
    "  \n",
    "def get_test_dataset(inputs, inputs_mean_std, targets_mean_std):\n",
    "    return SpectralTestDataset(\n",
    "        spectra=inputs[:, None, :],\n",
    "        concentrations=None,\n",
    "        dtype=torch.float32,\n",
    "        spectra_mean_std=inputs_mean_std,\n",
    "        concentration_mean_std=targets_mean_std,\n",
    "        combine_spectra_range=1.0,\n",
    "        baseline_factor_bound=config[\"baseline_factor_bound\"],\n",
    "        baseline_period_lower_bound=config[\"baseline_period_lower_bound\"],\n",
    "        baseline_period_upper_bound=(config[\"baseline_period_lower_bound\"] + config[\"baseline_period_span\"]),\n",
    "        augment_slope_std=config[\"augment_slope_std\"],\n",
    "        augment_intersept_std=0.0,\n",
    "        rolling_bound=config[\"rolling_bound\"],\n",
    "        spectrum_rolling_sigma=0.01,\n",
    "        augmentation_weight=0.1,\n",
    "        original_datapoint_weight=1.,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7516c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/paper.direct.finetune1.fold.0.pt 92 0.8142614922087263\n",
      "/kaggle/working/paper.direct.finetune1.fold.1.pt 99 0.8473047146848552\n",
      "/kaggle/working/paper.direct.finetune1.fold.2.pt 98 0.8289041368117737\n",
      "/kaggle/working/paper.direct.finetune1.fold.3.pt 99 0.7971901048240233\n",
      "/kaggle/working/paper.direct.finetune1.fold.4.pt 96 0.812437645407583\n"
     ]
    }
   ],
   "source": [
    "def get_ckpt_paths():\n",
    "    output_dir = \"/kaggle/working\"\n",
    "    output_files = sorted(os.listdir(output_dir))\n",
    "\n",
    "    ckpt_paths = []\n",
    "    for f in output_files:\n",
    "        if \"paper.direct.finetune1\" in f and \"csv\" not in f:\n",
    "            ckpt_path = os.path.join(output_dir, f)\n",
    "            ckpt_paths.append(ckpt_path)\n",
    "            ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "            print(ckpt_path, ckpt[\"epoch\"], ckpt[\"score\"])\n",
    "            \n",
    "    return ckpt_paths\n",
    "\n",
    "ckpt_paths = get_ckpt_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(ckpt_name, i):\n",
    "    ckpt = get_ckpt(ckpt_name)\n",
    "    \n",
    "    test_inputs = get_test_data()\n",
    "    test_ds = get_test_dataset(test_inputs, inputs_mean_std[i][1:], targets_mean_std[i][1:])\n",
    "    test_dl = DataLoader(test_ds, batch_size=32)\n",
    "    \n",
    "    model = ReZeroNet(**config).to(device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    for inputs in test_dl:\n",
    "        with torch.inference_mode():\n",
    "            preds = model(inputs.cuda())\n",
    "            preds = preds.double() \n",
    "            all_preds.append(cuda_to_np(preds))\n",
    "            \n",
    "    preds = np.concatenate(all_preds)\n",
    "    \n",
    "    mus = targets_mean_std[i][1:][0]\n",
    "    sigmas = targets_mean_std[i][1:][1]\n",
    "\n",
    "    for i in range(3):\n",
    "        preds[:, i] = reverse_zscore(preds[:, i], mus[i].numpy(), sigmas[i].numpy())\n",
    "    \n",
    "    return preds\n",
    "\n",
    "preds = inference()\n",
    "generate_csv(preds, \"foo\")\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b84e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.4059156 ,  1.11855214,  0.50671941],\n",
       "       [ 6.08617511,  1.95102328,  1.36496889],\n",
       "       [ 5.59815486,  0.28770998,  0.96444254],\n",
       "       [ 3.78061458,  0.48947609,  0.56943667],\n",
       "       [ 9.49626273,  1.06738254,  0.89053112],\n",
       "       [ 9.51238127,  1.71623778,  1.13761533],\n",
       "       [ 4.23734011,  0.7913762 ,  0.65157604],\n",
       "       [ 7.24411709,  1.88871845,  1.03811711],\n",
       "       [ 5.23462576,  1.59747915,  1.66877327],\n",
       "       [ 9.69133049,  0.54634542,  0.50025093],\n",
       "       [ 8.11715343,  1.20651393,  1.49084356],\n",
       "       [ 2.15665524,  1.4428628 ,  1.349757  ],\n",
       "       [ 2.95588415,  1.64431348,  1.45508458],\n",
       "       [ 4.19447924,  1.5130457 ,  1.83401597],\n",
       "       [ 3.60263336,  1.21913352,  1.42976138],\n",
       "       [ 6.58194007,  1.18379839,  1.04568582],\n",
       "       [ 3.19547955,  1.56208269,  1.36346606],\n",
       "       [ 6.39453465,  0.98706539,  1.77678796],\n",
       "       [ 6.46681182,  1.70580973,  1.30314127],\n",
       "       [ 2.54879328,  1.25286288,  1.44192536],\n",
       "       [ 5.168391  ,  1.37080225,  1.10651656],\n",
       "       [ 4.15478479,  1.20706654,  1.55279765],\n",
       "       [ 2.0386584 ,  1.46911145,  0.66729004],\n",
       "       [ 4.38287729,  1.42928372,  1.71876782],\n",
       "       [ 2.82272234,  1.56883359,  1.35325874],\n",
       "       [ 2.8808076 ,  1.26300045,  1.70994617],\n",
       "       [ 5.63627336,  1.26107758,  1.10565989],\n",
       "       [ 3.36531401,  1.29823944,  1.83627741],\n",
       "       [ 4.01073146,  1.50858331,  1.35222272],\n",
       "       [ 4.84810542,  0.48069739,  0.50247552],\n",
       "       [ 5.77123873,  1.30741874,  2.3313527 ],\n",
       "       [ 6.13106455,  0.9968421 ,  0.9724898 ],\n",
       "       [ 3.46598702,  1.38760086,  1.62663457],\n",
       "       [ 9.23772889,  1.29271212,  1.13627373],\n",
       "       [ 7.49168007,  1.06805176,  1.0163469 ],\n",
       "       [ 7.35146194,  0.39415094,  1.062289  ],\n",
       "       [ 6.86027271,  1.23457216,  0.69077077],\n",
       "       [ 5.97657384,  1.59822026,  0.58242711],\n",
       "       [ 1.91746516,  1.05594744,  0.99015929],\n",
       "       [ 8.30621982,  1.194315  ,  1.54201755],\n",
       "       [ 5.83250868,  1.19783609,  1.62058544],\n",
       "       [ 3.87319778,  1.11239619,  0.45946346],\n",
       "       [ 8.81374199,  0.43696831,  1.62982236],\n",
       "       [ 1.89420846,  0.92187998,  1.18847925],\n",
       "       [ 2.99429979,  1.10416841,  0.73307057],\n",
       "       [ 5.35764087,  0.91688624,  1.77970085],\n",
       "       [ 1.63246741,  0.61340638,  1.65288491],\n",
       "       [ 2.92669468,  1.06920199,  1.38236999],\n",
       "       [ 3.60291847,  1.47824002,  1.8693885 ],\n",
       "       [ 7.23872557,  0.60439145,  1.54452419],\n",
       "       [ 0.82644463,  1.43868454,  1.75159461],\n",
       "       [ 1.72349138,  1.15184979,  1.96142891],\n",
       "       [ 2.46596382,  0.8593584 ,  1.76629587],\n",
       "       [ 2.76862313,  1.85436621,  0.95178006],\n",
       "       [ 3.18812795,  1.30194897,  1.71161457],\n",
       "       [ 4.10748314,  1.54300008,  1.56615489],\n",
       "       [ 4.87283914,  1.75617827,  1.63964413],\n",
       "       [ 3.57877846,  1.3585655 ,  1.76113496],\n",
       "       [ 5.357941  ,  0.52955236,  1.65968467],\n",
       "       [ 4.55055812,  0.77153057,  1.29201539],\n",
       "       [ 3.06185439,  0.57735092,  1.44449321],\n",
       "       [ 3.86434999,  1.6351642 ,  1.60170051],\n",
       "       [ 3.38728977,  1.58826653,  1.26187625],\n",
       "       [ 8.18814773,  1.38173538,  2.05468546],\n",
       "       [ 7.81911682,  1.6844883 ,  0.67320896],\n",
       "       [ 9.61710038,  0.93802046,  0.42809632],\n",
       "       [10.93696159,  1.29557198,  1.16319622],\n",
       "       [ 3.5394405 ,  1.33332621,  0.82237312],\n",
       "       [ 6.36432963,  1.48477595,  1.07374604],\n",
       "       [ 7.69466571,  1.60388829,  1.18711556],\n",
       "       [ 8.3941986 ,  1.2400654 ,  1.44484218],\n",
       "       [ 7.17524221,  1.62813273,  1.59463953],\n",
       "       [ 5.31358776,  1.69796893,  1.95664415],\n",
       "       [ 7.40780208,  0.82708046,  0.96166624],\n",
       "       [ 5.26794492,  1.4076154 ,  2.0517365 ],\n",
       "       [ 9.67493517,  0.52474141,  1.12280123],\n",
       "       [ 4.37201309,  0.84378356,  0.60875906],\n",
       "       [ 4.76593009,  1.18500046,  1.91186854],\n",
       "       [ 3.7859254 ,  1.66155909,  0.79859573],\n",
       "       [ 5.22061185,  0.93515397,  1.78672806],\n",
       "       [ 8.45686222,  0.69559162,  0.68544194],\n",
       "       [ 1.58028935,  1.24154283,  1.77850257],\n",
       "       [ 6.40029811,  0.73103767,  1.79596241],\n",
       "       [ 3.98776236,  1.24327262,  1.29708418],\n",
       "       [ 4.01029194,  0.82794479,  1.60910431],\n",
       "       [ 2.34485302,  1.33572682,  1.75213526],\n",
       "       [ 4.71323485,  1.46161154,  1.05496158],\n",
       "       [ 2.32583681,  1.35928197,  1.60723228],\n",
       "       [ 5.41069699,  1.66174269,  1.53057978],\n",
       "       [ 3.46890896,  1.37480547,  1.74419241],\n",
       "       [ 4.86160839,  1.19612878,  1.57366012],\n",
       "       [ 2.98665591,  1.2937562 ,  1.74563213],\n",
       "       [ 6.55684522,  0.28678039,  1.27119992],\n",
       "       [ 3.62686178,  0.72808533,  0.9240369 ],\n",
       "       [ 1.3067466 ,  1.51089903,  1.08265857],\n",
       "       [ 3.82035007,  1.20228895,  0.90076904]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensemble_inference():\n",
    "    test_inputs = get_test_data()\n",
    "    all_preds = []\n",
    "\n",
    "    for i, ckpt_path in enumerate(ckpt_paths):\n",
    "        ckpt = get_ckpt(ckpt_path)\n",
    "        \n",
    "        model = ReZeroNet(**config).to(device)\n",
    "        model.load_state_dict(ckpt[\"state_dict\"])\n",
    "        model.eval()\n",
    "\n",
    "        test_ds = get_test_dataset(test_inputs, inputs_mean_std[i][1:], targets_mean_std[i][1:])\n",
    "        test_dl = DataLoader(test_ds, batch_size=32)\n",
    "        \n",
    "        fold_preds = []\n",
    "        for inputs in test_dl:\n",
    "            with torch.inference_mode():\n",
    "                preds = model(inputs.cuda())\n",
    "                preds = cuda_to_np(preds.double())\n",
    "                fold_preds.append(preds)\n",
    "                \n",
    "        fold_preds = np.concatenate(fold_preds)\n",
    "        \n",
    "        means = targets_mean_std[i][1:][0]\n",
    "        stds = targets_mean_std[i][1:][1]\n",
    "        for i in range(3):\n",
    "            fold_preds[:, i] = reverse_zscore(fold_preds[:, i], means[i].numpy(), stds[i].numpy())\n",
    "            \n",
    "        all_preds.append(fold_preds)\n",
    "\n",
    "    return np.mean(all_preds, axis=0)\n",
    "\n",
    "preds = ensemble_inference()\n",
    "generate_csv(preds, \"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba72f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(4000):\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
